[{"content":"In this post, we will explore the numerics of Hermite polynomials, as part of a numerical solver project for Kramers hyperbolic system. In particular, we try to compute eigenvectors numerically for the Hermite generating matrix, which is a tridiagonal matrix given by the recurrence relation\n$$ x \\mathrm{H}_{n}(x) = \\sqrt{n+1} \\mathrm{H}_{n+1}(x) - \\sqrt{n} \\mathrm{H}_{n-1}(x) $$for \\( n \\geq 1 \\) with \\(\\mathrm{H}_{n}(x)\\) being the \\(n\\)-th normalized Hermite polynomial. The generating matrix is given by\n$$ \\mathrm{T}_n = \\begin{pmatrix} 0 \u0026 \\sqrt{1} \u0026 0 \u0026 \\cdots \u0026 \\cdots \\\\ \\sqrt{1} \u0026 0 \u0026 \\sqrt{2} \u0026 \\cdots \u0026 \\cdots \\\\ 0 \u0026 \\sqrt{2} \u0026 \\ddots \u0026 \\cdots \u0026 \\cdots \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 0 \u0026 \\sqrt{n-1} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\sqrt{n-1} \u0026 0 \\end{pmatrix}. $$All the codes below are implemented in Julia.\nTheoretical eigenvalues and eigenvectors Recall that the Hermite polynomials are defined by\n$$ \\mathrm{He}_n(x) = (-1)^n \\exp(x^2/2) \\frac{\\mathrm{d}^n}{\\mathrm{d}x^n} \\left( \\exp(-x^2/2) \\right) $$for $n \\geq 0$. The first few Hermite polynomials are given by\n$$ \\mathrm{He}_0(x) = 1, \\\\ \\mathrm{He}_1(x) = x, \\\\ \\mathrm{He}_2(x) = x^2 - 1, \\\\ \\mathrm{He}_3(x) = x^3 - 3x. $$The Hermite polynomials can be computed using the recurrence relation\n$$ \\mathrm{He}_{n+1}(x) = x \\mathrm{He}_{n}(x) - n \\mathrm{He}_{n-1}(x) $$for $n \\geq 2$ with $\\mathrm{He}_0(x) = 1$ and $\\mathrm{He}_1(x) = x$.\nThe Hermite polynomials are orthogonal with respect to the (normalized) weight function $w(x) = \\exp(-x^2/2)/\\sqrt{2\\pi}$ on the interval $(-\\infty, \\infty)$. The $\\mathrm{L}^2$ norm of $\\mathrm{He}_n(x)$ with respect to this weight function is given by\n$$ ||\\mathrm{He}_n||_{\\mathrm{L}^2} = \\sqrt{n!} $$ for $n \\geq 0$. Let\u0026rsquo;s denote the normalized Hermite polynomials by\n$$ \\mathrm{H}_n = \\frac{\\mathrm{He}_n}{||\\mathrm{He}_n||_{\\mathrm{L}^2}} = \\frac{\\mathrm{He}_n}{\\sqrt{n!}}. $$It is well-known that the eigenvalues of the Hermite generating matrix $\\mathrm{T}_n$ of size $n$ are given by the roots of the Hermite polynomial $\\mathrm{H}_n(x)$, and the eigenvectors are given by the values of the Hermite polynomials at the roots, namely\n$$ \\mathrm{T}_n \\mathbf{v}_{n,k} = \\lambda_{n,k} \\mathbf{v}_{n,k} $$ where $\\lambda_{n,k}$ is the $k$-th root of $\\mathrm{He}_n(x)$, and\n$$ \\mathbf{v}_{n,k} = \\begin{pmatrix} \\mathrm{H}_0(\\lambda_{n,k}) \\\\ \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\cdots \\\\ \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\end{pmatrix} $$ is the corresponding eigenvector.\nTo see this, we do a direct computation\n$$ \\mathrm{T}_n \\mathbf{v}_{n,k} = \\begin{pmatrix} \\sqrt{1} \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\sqrt{1} \\mathrm{H}_0(\\lambda_{n,k}) + \\sqrt{2} \\mathrm{H}_2(\\lambda_{n,k}) \\\\ \\cdots\\\\ \\sqrt{n-2} \\mathrm{H}_{n-3}(\\lambda_{n,k}) + \\sqrt{n-1} \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\\\ \\sqrt{n-1} \\mathrm{H}_{n-2}(\\lambda_{n,k}) + \\sqrt{n} \\underbrace{\\mathrm{H}_{n}(\\lambda_{n,k})}_{=0} \\end{pmatrix} = \\begin{pmatrix} \\lambda_{n,k} \\mathrm{H}_0(\\lambda_{n,k}) \\\\ \\lambda_{n,k} \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\cdots \\\\ \\lambda_{n,k} \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\end{pmatrix} = \\lambda_{n,k} \\mathbf{v}_{n,k}. $$Let\u0026rsquo;s define $\\mathbf{w}_{n,k} = {\\mathbf{v}_{n,k}}/{||\\mathbf{v}_{n,k}||_2}$, which is the normalized eigenvector, and\n$$ \\mathrm{P}_n := \\begin{pmatrix} \\mathbf{w}_{n,1} \u0026 \\mathbf{w}_{n,2} \u0026 \\cdots \u0026 \\mathbf{w}_{n,n} \\end{pmatrix} $$is the change of basis matrix. It follows that\n$$ \\mathrm{P}_n^T \\mathrm{T}_n \\mathrm{P}_n = \\mathrm{D}_n:= \\mathrm{diag}(\\lambda_{n,1}, \\lambda_{n,1}, \\cdots, \\lambda_{n,n}). $$Thus, in theory, we can compute the eigenvalues and eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ by computing the roots of the normalized Hermite polynomial $\\mathrm{H}_n(x)$ and evaluating the normalized Hermite polynomials at these roots.\nNumerical properties of the Hermite polynomials Numerical instability when evaluating the Hermite polynomials The first thing we need to note is that the Hermite polynomials\u0026rsquo; coefficients grow very quickly as $n$ increases. Very large and very small coefficients exist in the polynomial, which can lead to numerical instability when evaluating the polynomial. For example,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 using Polynomials function He_symbolic(n::Int) He_0 = Polynomial([1.0]) if n == 0 return He_0 end He_1 = Polynomial([0.0, 1.0]) if n == 1 return He_1 end for k in 2:n He_2 = Polynomial([0.0, 1.0]) * He_1 - (k - 1) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end 1 He_symbolic(15) -2.027025e6∙x + 4.729725e6∙x3 - 2.837835e6∙x5 + 675675.0∙x7 - 75075.0∙x9 + 4095.0∙x11 - 105.0∙x13 + 1.0∙x15\nFor $n=20$, the largest coefficient is around $10^{6}$, while the smallest is always $1.0$, this could lead to numerical instability when evaluating the polynomial directly. To solve this problem, we can use the recurrence relation to evaluate the polynomial, which should give us a more stable result for small $n$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function He(n::Int, x::Float64) He_0 = 1.0 if n == 0 return He_0 end He_1 = x for k in 2:n He_2 = x * He_1 - (k - 1) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end 1 2 3 4 5 6 x = 1.1 n = 51 val_direct = He_symbolic(n)(x) val_recursive = He(n, x) val_direct, val_recursive (-5.624972601518976e32, -5.624972601518795e32) We can see relatively small but noticeable discrepancies between the two methods, since Polynomials.jl already uses numerically stable methods to evaluate a polynomial. For our purposes, we use the recursive evaluation.\nNumerical instability when computing the roots of the Hermite polynomials The second thing we need to note is that the roots of the Hermite polynomials are very close to each other, especially for large $n$. This can lead to numerical instability when computing the roots of the polynomial. For example, for $n=15$, the roots are\n1 println(sort(roots(He_symbolic(15)))) [-6.36394788882981, -5.190093591304892, -4.196207711268877, -3.2890824243988406, -2.4324368270097385, -1.6067100690287315, -0.7991290683245481, 0.0, 0.7991290683245479, 1.6067100690287317, 2.4324368270097474, 3.2890824243988055, 4.196207711268944, 5.190093591304826, 6.36394788882981] As we can see, roots are clustered around $x=0$, and does not become more spaced out as $n$ increases. To see this, we can plot the roots of the Hermite polynomials for $n=1, \\dots, 20$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 using Plots # Collect roots for n=1:20 root_data = [] for n in 1:20 push!(root_data, roots(He_symbolic(n))) end # Flatten the root data and associate each root with its corresponding n x_vals = vcat(root_data...) y_vals = vcat([fill(n, length(root_data[n])) for n in 1:20]...) # Plot the roots distribution scatter(x_vals, y_vals, xlabel=\u0026#34;Roots\u0026#34;, ylabel=\u0026#34;n\u0026#34;, title=\u0026#34;Roots Distribution of Hermite polynomials for n=1:20\u0026#34;, legend=false) To efficiently compute the roots of the Hermite polynomials, we can use the generating matrix $\\mathrm{T}_n$ and compute the eigenvalues of the matrix. This is a numerically stable method as the matrix is tridiagonal, symmetric, and there exists numerically stable algorithms to compute the eigenvalues and eigenvectors of such matrices.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 function T(n::Int) d = zeros(n) e = sqrt.(1:n-1) T = SymTridiagonal(d, e) return T end using LinearAlgebra function get_eigen(n::Int) Tn = T(n) F = eigen(Tn) # Eigenvalues and eigenvectors idx = sortperm(F.values) # Sort eigenvalues and eigenvectors eigenvals = F.values[idx] # Sorted eigenvalues (roots) eigenvecs = F.vectors[:, idx] # Sorted eigenvectors # One step of Newton\u0026#39;s method to refine the roots He_nx = He.(n, eigenvals) # Hermite polynomial at the roots He_nm1x = He.(n-1, eigenvals) # Candidate for the derivative eigenvals .-= He_nx ./ (n .* He_nm1x) return eigenvals, eigenvecs end get_eigen (generic function with 1 method) 1 2 3 eigenvals, _ = get_eigen(5) println(\u0026#34;Eigenvalues for n=5: \u0026#34;, eigenvals) println(sort(roots(He_symbolic(5)))) Eigenvalues for n=5: [-2.8569700138728056, -1.355626179974266, 0.0, 1.355626179974266, 2.8569700138728056] [-2.856970013872804, -1.3556261799742675, 0.0, 1.3556261799742657, 2.856970013872808] From the above comparison, we can see that the eigenvalues computed from the generating matrix preserve the symmetry of the roots better. As $n$ increases, this algorithm becomes more efficient than the root-finding algorithm, as the roots become more clustered together.\nNormalized eigenvectors of the Hermite generating matrix Now we are in a position to compute the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$. Note that we already computed the eigenvalues and eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ in the previous section using a numerically robust method. In this section, we will construct the normalized eigenvectors of the Hermite generating matrix by evaluating the normalized Hermite polynomials at the roots of the Hermite polynomial, i.e. $\\mathbf{v}_{n,k}$ for $k=1, \\dots, n$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 using SpecialFunctions: logfactorial # An auxiliary function to compute the normalization constant function He_l2norm(n::Int) return sqrt(exp(logfactorial(n))) end function analytic_eigenvecs(n::Int) Pmat = zeros(Float64, n, n) # initializing the matrix rts, _ = get_eigen(n) # Get the roots for i in 1:n root_i = rts[i] for j in 1:n Pmat[j, i] = He(j-1, root_i) / He_l2norm(j-1) end # Normalize each vector (column) Pmat[:, i] /= norm(Pmat[:, i]) end return Pmat end Numerical properties of the eigenvectors Now, we have two ways to compute the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$: one is theoretically precise, the other is numerical. We will compare the two methods and see how well they agree with each other.\nThere are two important properties of the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ that we need to note:\nThe column matrix $\\mathrm{P}_n$ of eigenvectors is orthonormal, i.e. $\\mathrm{P}_n^T \\mathrm{P}_n = \\mathrm{I}_n$. By conjugating the eigenvectors, we can obtain the eigenvalues of the Hermite generating matrix $\\mathrm{T}_n$, which is a diagonal matrix $\\mathrm{D}_n$ with the roots of the Hermite polynomial on the diagonal. $$ \\mathrm{P}_n^T \\mathrm{T}_n \\mathrm{P}_n = \\mathrm{D}_n. $$ We will check how well these properties hold for both methods of computing the eigenvectors. The following codes compute the $\\infty$ norm of the above two properties.\n1 2 3 4 5 6 7 function orthonormality_error(P::Matrix{Float64}) return norm(P\u0026#39; * P - I, Inf) end function diagonal_error(P::Matrix{Float64}, eigenvals::Vector{Float64}, n::Int) return norm(P\u0026#39; * T(n) * P - Diagonal(eigenvals), Inf) end Show Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Define the range of n n_values = 2:200 # Initialize error arrays for both methods analytic_orth_errors = Float64[] analytic_diag_errors = Float64[] numerical_orth_errors = Float64[] numerical_diag_errors = Float64[] # Compute errors for both methods for n in n_values P_analytic = analytic_eigenvecs(n) rts, P_numerical = get_eigen(n) push!(analytic_orth_errors, orthonormality_error(P_analytic)) push!(analytic_diag_errors, diagonal_error(P_analytic, rts, n)) push!(numerical_orth_errors, orthonormality_error(P_numerical)) push!(numerical_diag_errors, diagonal_error(P_numerical, rts, n)) end # Create 2x2 subplots p1 = plot(n_values, log10.(analytic_orth_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Orthonormality Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p1, n_values, log10.(numerical_orth_errors), label=\u0026#34;Numerical\u0026#34;) p2 = plot(n_values, log10.(analytic_diag_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Diagonalization Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p2, n_values, log10.(numerical_diag_errors), label=\u0026#34;Numerical\u0026#34;) plot(p1, p2, layout=(2, 1)) From the plot we can see that the analytical method is marginally better than the numerical method before $n\\approx 170$, but both errors for analytical method blow up after $n\\approx 170$, while the numerical method remains stable. The reason for this is that in the analytical method, we need to compute the normalization factor of the Hermite polynomial, which is given by $||\\mathrm{He}_n||_{\\mathrm{L}^2} = \\sqrt{n!}$, and the factorial grows very quickly, eventually blows up at $n\\approx 170$.\n1 2 3 for n in 169:172 print(He_l2norm(n), \u0026#34;, \u0026#34;) end 2.0661723086434517e152, 2.6939590968143674e153, Inf, Inf, To solve this problem, we can use the recurrence relation to compute the normalized Hermite polynomials as before, which avoids the need to compute the normalization factor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 function Henorm(n::Int, x::Float64) He_0 = 1.0 if n == 0 return He_0 end He_1 = x for k in 2:n He_2 = x * He_1 / sqrt(k) - sqrt((k - 1) / k) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end function analytic_eigenvecs_norm(n::Int) Pmat = zeros(Float64, n, n) # initializing the matrix rts, _ = get_eigen(n) # Get the roots for i in 1:n root_i = rts[i] for j in 1:n Pmat[j, i] = Henorm(j-1, root_i) end # Normalize each vector (column) Pmat[:, i] /= norm(Pmat[:, i]) end return Pmat end Show Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Define the range of n n_values = 2:200 # Initialize error arrays for both methods analytic_orth_errors = Float64[] analytic_diag_errors = Float64[] numerical_orth_errors = Float64[] numerical_diag_errors = Float64[] # Compute errors for both methods for n in n_values P_analytic = analytic_eigenvecs_norm(n) rts, P_numerical = get_eigen(n) push!(analytic_orth_errors, orthonormality_error(P_analytic)) push!(analytic_diag_errors, diagonal_error(P_analytic, rts, n)) push!(numerical_orth_errors, orthonormality_error(P_numerical)) push!(numerical_diag_errors, diagonal_error(P_numerical, rts, n)) end # Create 2x2 subplots p1 = plot(n_values, log10.(analytic_orth_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Orthonormality Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p1, n_values, log10.(numerical_orth_errors), label=\u0026#34;Numerical\u0026#34;) p2 = plot(n_values, log10.(analytic_diag_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Diagonalization Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p2, n_values, log10.(numerical_diag_errors), label=\u0026#34;Numerical\u0026#34;) plot(p1, p2, layout=(2, 1)) After the above changes, we can see that the analytical method is now more stable than the numerical method, and the errors are much smaller. However, in practice, using large $n$ for the Hermite polynomials is not recommended, as there are already numerical instabilities when evaluating the Hermite polynomials.\nDownload the notebook Download the notebook\n","permalink":"http://localhost:1313/posts/25-05-02_hermite-numerics/","summary":"\u003cp\u003eIn this post, we will explore the numerics of Hermite polynomials, as part of a numerical solver project for Kramers hyperbolic system. In particular, we try to compute eigenvectors numerically for the Hermite generating matrix, which is a tridiagonal matrix given by the recurrence relation\u003c/p\u003e\n$$\nx \\mathrm{H}_{n}(x) = \\sqrt{n+1} \\mathrm{H}_{n+1}(x) - \\sqrt{n} \\mathrm{H}_{n-1}(x)\n$$\u003cp\u003efor \\( n \\geq 1 \\) with \\(\\mathrm{H}_{n}(x)\\) being the \\(n\\)-th normalized Hermite polynomial.\nThe generating matrix is given by\u003c/p\u003e","title":"Numerical Methods for Hermite Polynomials"},{"content":"I am a PhD student in Mathematics at Uppsala University. My research interests lie in the intersection of probability theory, partial differential equations, and machine learning. I am particularly interested in the mathematical analysis of stochastic processes and their applications in machine learning.\nResearch Publications Preprints M. Hou. Variational Formulation and Capacity Estimates for Non-Self-Adjoint Fokker-Planck Operators in Divergence Form. arXiv preprint, 2025. M. Hou. Boundedness of Weak Solutions to Degenerate Kolmogorov Equations of Hypoelliptic Type in Bounded Domains. arXiv preprint, 2024. B. Avelin, and M. Hou. Weak and Perron\u0026rsquo;s Solutions for Stationary Kramers-Fokker-Planck Equations in Bounded Domains. arXiv preprint, 2024. Published B. Avelin, M. Hou, and K. Nyström. A Galerkin Type Method for Kinetic Fokker-Planck Equations Based on Hermite Expansions. Kinet. Relat. Models, 2024. Thesis M. Hou. Behind the Training Dynamics of Neural Networks: Analysis of Fokker-Planck Equations and the Path to Metastability. PhD dissertation, Uppsala University, 2025. Contact Department of Mathematics\nUppsala University\n751 06 Uppsala\nSweden\nmingyi.hou@math.uu.se LinkedIn GitHub ","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003eI am a PhD student in Mathematics at Uppsala University. My research interests lie in the intersection of probability theory, partial differential equations, and machine learning. I am particularly interested in the mathematical analysis of stochastic processes and their applications in machine learning.\u003c/p\u003e\n\u003ch2 id=\"research-publications\"\u003eResearch Publications\u003c/h2\u003e\n\u003ch3 id=\"preprints\"\u003ePreprints\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eM. Hou. Variational Formulation and Capacity Estimates for Non-Self-Adjoint Fokker-Planck Operators in Divergence Form. \u003ca href=\"https://arxiv.org/abs/2502.12036\"\u003earXiv preprint, 2025\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eM. Hou. Boundedness of Weak Solutions to Degenerate Kolmogorov Equations of Hypoelliptic Type in Bounded Domains. \u003ca href=\"https://arxiv.org/abs/2407.00800\"\u003earXiv preprint, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eB. Avelin, and M. Hou. Weak and Perron\u0026rsquo;s Solutions for Stationary Kramers-Fokker-Planck Equations in Bounded Domains. \u003ca href=\"https://arxiv.org/abs/2405.04070\"\u003earXiv preprint, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"published\"\u003ePublished\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eB. Avelin, M. Hou, and K. Nyström. A Galerkin Type Method for Kinetic Fokker-Planck Equations Based on Hermite Expansions. \u003ca href=\"https://doi.org/10.3934/krm.2023035\"\u003eKinet. Relat. Models, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"thesis\"\u003eThesis\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eM. Hou. Behind the Training Dynamics of Neural Networks: Analysis of Fokker-Planck Equations and the Path to Metastability. \u003ca href=\"https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-553381\"\u003ePhD dissertation, Uppsala University, 2025.\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"contact\"\u003eContact\u003c/h2\u003e\n\u003cdiv style=\"display: flex; justify-content: space-between; flex-wrap: wrap;\"\u003e\n  \u003cdiv style=\"flex: 1; min-width: 200px;\"\u003e\n    \u003cp\u003eDepartment of Mathematics\u003cbr\u003e\n    Uppsala University\u003cbr\u003e\n    751 06 Uppsala\u003cbr\u003e\n    Sweden\u003c/p\u003e","title":"About Me"},{"content":"This is my Kaggle notebook for the House Prices - Advanced Regression Techniques competition on Kaggle. The goal of this competition is to predict the sale price of homes in Ames, Iowa, based on various features of the homes.\nI will be using various techniques learned in the course to predict the sale price of homes. The techniques include: preprocessing the data, feature engineering, and using different machine learning models to make predictions. I will also be using cross-validation to evaluate the performance of the models and to avoid overfitting.\nExploratory Data Analysis (EDA) Load Libraries \u0026amp; Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from pathlib import Path import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # Set plot styles sns.set(style=\u0026#34;whitegrid\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (12, 6) # Load data data_dir = Path(\u0026#34;./house-prices-advanced-regression-techniques\u0026#34;) df_train = pd.read_csv(data_dir / \u0026#34;train.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) df_test = pd.read_csv(data_dir / \u0026#34;test.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) print(f\u0026#34;Train shape: {df_train.shape}\u0026#34;) print(f\u0026#34;Test shape: {df_test.shape}\u0026#34;) Train shape: (1460, 80) Test shape: (1459, 79) Understand the Target Variable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Distribution of target sns.histplot(df_train[\u0026#34;SalePrice\u0026#34;], kde=True) plt.title(\u0026#34;SalePrice Distribution\u0026#34;) plt.xlabel(\u0026#34;SalePrice\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.savefig(\u0026#34;saleprice_distribution.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() # Log-transform target to check skew sns.histplot(np.log1p(df_train[\u0026#34;SalePrice\u0026#34;]), kde=True) plt.title(\u0026#34;Log-Transformed SalePrice\u0026#34;) plt.xlabel(\u0026#34;Log(SalePrice + 1)\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.savefig(\u0026#34;log_saleprice_distribution.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() # Summary stats df_train[\u0026#34;SalePrice\u0026#34;].describe() count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000 Name: SalePrice, dtype: float64 Overview of the Dataset 1 2 3 4 5 6 7 df = pd.concat([df_train, df_test], axis=0) # df.info() # df.describe() # Missing value heatmap import missingno as msno msno.matrix(df) \u0026lt;Axes: \u0026gt; Data Cleaning 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def clean_data(df): # Clean to_category = [\u0026#39;MSSubClass\u0026#39;, \u0026#39;MoSold\u0026#39;, \u0026#39;YrSold\u0026#39;, \u0026#39;GarageYrBlt\u0026#39;, \u0026#39;YearBuilt\u0026#39;, \u0026#39;YearRemodAdd\u0026#39;] for col in to_category: df[col] = df[col].astype(str) df[\u0026#39;Functional\u0026#39;] = df[\u0026#39;Functional\u0026#39;].fillna(\u0026#39;Typ\u0026#39;) df[\u0026#34;Electrical\u0026#34;] = df[\u0026#34;Electrical\u0026#34;].fillna(\u0026#39;SBrkr\u0026#39;) df[\u0026#34;KitchenQual\u0026#34;] = df[\u0026#34;KitchenQual\u0026#34;].fillna(\u0026#39;TA\u0026#39;) df[\u0026#34;Exterior1st\u0026#34;] = df[\u0026#34;Exterior1st\u0026#34;].fillna(df[\u0026#34;Exterior1st\u0026#34;].mode()[0]) df[\u0026#34;Exterior2nd\u0026#34;] = df[\u0026#34;Exterior2nd\u0026#34;].fillna(df[\u0026#34;Exterior2nd\u0026#34;].mode()[0]) df[\u0026#34;SaleType\u0026#34;] = df[\u0026#34;SaleType\u0026#34;].fillna(df[\u0026#34;SaleType\u0026#34;].mode()[0]) # Impute # Fill missing values in object columns with \u0026#34;None\u0026#34; objects = [] for i in df.columns: if df[i].dtype == object: objects.append(i) df.update(df[objects].fillna(\u0026#39;None\u0026#39;)) # Fill missing values in numeric columns with 0 numerics = [] for i in df.columns: if df[i].dtype != object: numerics.append(i) df.update(df[numerics].fillna(0)) return df 1 2 3 4 5 6 7 8 9 10 11 12 13 def load_data(): data_dir = Path(\u0026#34;./house-prices-advanced-regression-techniques\u0026#34;) df_train = pd.read_csv(data_dir / \u0026#34;train.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) df_test = pd.read_csv(data_dir / \u0026#34;test.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) # Clean data df = pd.concat([df_train, df_test], axis=0) df = clean_data(df) # Split back into train and test df_train = df.loc[df_train.index, :] df_test = df.loc[df_test.index, :].drop(columns=[\u0026#34;SalePrice\u0026#34;]) return df_train, df_test 1 2 3 4 5 6 7 df_train, df_test = load_data() # Check the cleaned data # train_missing = df_train.isnull().sum() # print(train_missing[train_missing \u0026gt; 0]) # test_missing = df_test.isnull().sum() # print(test_missing[test_missing \u0026gt; 0]) # df_train.info() Correlation with Target (Numerical Features) 1 2 3 4 5 6 7 8 9 10 11 # Compute correlation matrix corr_matrix = df_train.corr(numeric_only=True) # Get top 15 features correlated with SalePrice top_corr = corr_matrix[\u0026#34;SalePrice\u0026#34;].abs().sort_values(ascending=False).head(15) # Visualize sns.heatmap(df_train[top_corr.index].corr(), annot=True, cmap=\u0026#34;coolwarm\u0026#34;, fmt=\u0026#34;.2f\u0026#34;) plt.title(\u0026#34;Top Correlated Features with SalePrice\u0026#34;) plt.savefig(\u0026#34;top_correlated_features.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() Categorical Features Preview 1 2 3 4 5 6 7 8 9 10 11 12 13 categoricals = df_train.select_dtypes(include=\u0026#34;object\u0026#34;).columns print(f\u0026#34;Categorical features: {len(categoricals)}\u0026#34;) # print(categoricals.tolist()) # Example: Visualize average SalePrice by a few important categorical features important_cats = [\u0026#34;Neighborhood\u0026#34;, \u0026#34;ExterQual\u0026#34;, \u0026#34;GarageFinish\u0026#34;, \u0026#34;KitchenQual\u0026#34;] for col in important_cats: sns.boxplot(data=df_train, x=col, y=\u0026#34;SalePrice\u0026#34;) plt.title(f\u0026#34;SalePrice by {col}\u0026#34;) plt.xticks(rotation=45) plt.savefig(f\u0026#34;saleprice_by_{col}.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() Categorical features: 49 Time-Related Patterns 1 2 3 4 5 6 7 sns.boxplot(x=\u0026#34;YrSold\u0026#34;, y=\u0026#34;SalePrice\u0026#34;, data=df_train) plt.title(\u0026#34;SalePrice by Year Sold\u0026#34;) plt.show() sns.scatterplot(x=\u0026#34;YearBuilt\u0026#34;, y=\u0026#34;SalePrice\u0026#34;, data=df_train) plt.title(\u0026#34;SalePrice vs Year Built\u0026#34;) plt.show() It does not look like there is any time related feature.\nFeature Engineering 1 2 3 X = df_train.copy() y = X.pop(\u0026#34;SalePrice\u0026#34;) # X.info() Separate Categorical and Numerical Features 1 2 3 4 # cat_cols = X.select_dtypes(include=[\u0026#39;object\u0026#39;]).columns.tolist() # num_cols = X.select_dtypes(exclude=[\u0026#39;object\u0026#39;]).columns.tolist() # print(f\u0026#34;Categorical columns: {len(cat_cols)}\u0026#34;) # print(f\u0026#34;Numerical columns: {len(num_cols)}\u0026#34;) Encoding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from sklearn.base import BaseEstimator, TransformerMixin from category_encoders import TargetEncoder class Encoder(BaseEstimator, TransformerMixin): def __init__(self, ordered_levels): self.ordered_levels = ordered_levels self.low_cardinality = [] self.high_cardinality = [] self.te = None def fit(self, X, y): X = X.copy() self.ordered_levels = {k: [\u0026#34;None\u0026#34;] + v for k, v in self.ordered_levels.items()} for col, order in self.ordered_levels.items(): if col in X.columns: X[col] = pd.Categorical(X[col], categories=order, ordered=True).codes ordinal_cols = list(self.ordered_levels.keys()) nominal_cols = [col for col in X.select_dtypes(include=\u0026#39;object\u0026#39;).columns if col not in ordinal_cols] self.low_cardinality = [col for col in nominal_cols if X[col].nunique() \u0026lt;= 10] self.high_cardinality = [col for col in nominal_cols if X[col].nunique() \u0026gt; 10] for col in self.low_cardinality: X[col] = X[col].astype(\u0026#34;category\u0026#34;).cat.codes self.te = TargetEncoder() self.te.fit(X[self.high_cardinality], y) return self def transform(self, X): X = X.copy() for col, order in self.ordered_levels.items(): if col in X.columns: X[col] = pd.Categorical(X[col], categories=order, ordered=True).codes for col in self.low_cardinality: if col in X.columns: X[col] = X[col].astype(\u0026#34;category\u0026#34;).cat.codes if self.te: X[self.high_cardinality] = self.te.transform(X[self.high_cardinality]) return X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 five_levels = [\u0026#34;Po\u0026#34;, \u0026#34;Fa\u0026#34;, \u0026#34;TA\u0026#34;, \u0026#34;Gd\u0026#34;, \u0026#34;Ex\u0026#34;] ten_levels = list(range(1, 11)) # 1 - 10 is the correct range! ordered_levels = { \u0026#34;OverallQual\u0026#34;: ten_levels, \u0026#34;OverallCond\u0026#34;: ten_levels, \u0026#34;ExterQual\u0026#34;: five_levels, \u0026#34;ExterCond\u0026#34;: five_levels, \u0026#34;BsmtQual\u0026#34;: five_levels, \u0026#34;BsmtCond\u0026#34;: five_levels, \u0026#34;HeatingQC\u0026#34;: five_levels, \u0026#34;KitchenQual\u0026#34;: five_levels, \u0026#34;FireplaceQu\u0026#34;: five_levels, \u0026#34;GarageQual\u0026#34;: five_levels, \u0026#34;GarageCond\u0026#34;: five_levels, \u0026#34;PoolQC\u0026#34;: five_levels, \u0026#34;LotShape\u0026#34;: [\u0026#34;Reg\u0026#34;, \u0026#34;IR1\u0026#34;, \u0026#34;IR2\u0026#34;, \u0026#34;IR3\u0026#34;], \u0026#34;LandSlope\u0026#34;: [\u0026#34;Sev\u0026#34;, \u0026#34;Mod\u0026#34;, \u0026#34;Gtl\u0026#34;], \u0026#34;BsmtExposure\u0026#34;: [\u0026#34;No\u0026#34;, \u0026#34;Mn\u0026#34;, \u0026#34;Av\u0026#34;, \u0026#34;Gd\u0026#34;], \u0026#34;BsmtFinType1\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;LwQ\u0026#34;, \u0026#34;Rec\u0026#34;, \u0026#34;BLQ\u0026#34;, \u0026#34;ALQ\u0026#34;, \u0026#34;GLQ\u0026#34;], \u0026#34;BsmtFinType2\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;LwQ\u0026#34;, \u0026#34;Rec\u0026#34;, \u0026#34;BLQ\u0026#34;, \u0026#34;ALQ\u0026#34;, \u0026#34;GLQ\u0026#34;], \u0026#34;Functional\u0026#34;: [\u0026#34;Sal\u0026#34;, \u0026#34;Sev\u0026#34;, \u0026#34;Maj1\u0026#34;, \u0026#34;Maj2\u0026#34;, \u0026#34;Mod\u0026#34;, \u0026#34;Min2\u0026#34;, \u0026#34;Min1\u0026#34;, \u0026#34;Typ\u0026#34;], \u0026#34;GarageFinish\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;RFn\u0026#34;, \u0026#34;Fin\u0026#34;], \u0026#34;PavedDrive\u0026#34;: [\u0026#34;N\u0026#34;, \u0026#34;P\u0026#34;, \u0026#34;Y\u0026#34;], \u0026#34;Utilities\u0026#34;: [\u0026#34;NoSeWa\u0026#34;, \u0026#34;NoSewr\u0026#34;, \u0026#34;AllPub\u0026#34;], \u0026#34;CentralAir\u0026#34;: [\u0026#34;N\u0026#34;, \u0026#34;Y\u0026#34;], \u0026#34;Electrical\u0026#34;: [\u0026#34;Mix\u0026#34;, \u0026#34;FuseP\u0026#34;, \u0026#34;FuseF\u0026#34;, \u0026#34;FuseA\u0026#34;, \u0026#34;SBrkr\u0026#34;], \u0026#34;Fence\u0026#34;: [\u0026#34;MnWw\u0026#34;, \u0026#34;GdWo\u0026#34;, \u0026#34;MnPrv\u0026#34;, \u0026#34;GdPrv\u0026#34;], } 1 2 3 4 5 6 7 from sklearn.pipeline import Pipeline from xgboost import XGBRegressor basepipe = Pipeline([ (\u0026#34;encode\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;xgb\u0026#34;, XGBRegressor(random_state=42)) ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from sklearn.metrics import make_scorer, mean_squared_log_error from sklearn.model_selection import cross_val_score import numpy as np # Custom RMSLE scorer (greater_is_better=False because lower is better) rmsle_scorer = make_scorer( lambda y_true, y_pred: np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred))), greater_is_better=False ) y_log = np.log(y) # We train on the log transformed target # Then use it with cross_val_score score = cross_val_score(basepipe, X, y_log, cv=5, scoring=rmsle_scorer) print(f\u0026#34;RMSLE scores: {-score}\u0026#34;) # Negate to get actual RMSLE values print(f\u0026#34;Mean RMSLE: {-score.mean():.5f}\u0026#34;) RMSLE scores: [0.1397286 0.15149179 0.14633292 0.1268186 0.14509794] Mean RMSLE: 0.14189 Transform Skewed Numerical Features This transform is not used in the end.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class SkewedFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self, threshold=0.75): self.threshold = threshold self.skewed_cols = [] def fit(self, X, y=None): X = pd.DataFrame(X) skewness = X.skew().abs() self.skewed_cols = skewness[skewness \u0026gt; self.threshold].index.tolist() return self def transform(self, X): X = pd.DataFrame(X).copy() for col in self.skewed_cols: X[col] = np.log1p(X[col]) return X skew_transformer = SkewedFeatureTransformer() Add New Features 1 2 from sklearn.preprocessing import FunctionTransformer from sklearn.pipeline import Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def add_custom_features(df): df = df.copy() df[\u0026#39;TotalSF\u0026#39;] = df[\u0026#39;TotalBsmtSF\u0026#39;] + df[\u0026#39;1stFlrSF\u0026#39;] + df[\u0026#39;2ndFlrSF\u0026#39;] df[\u0026#39;Total_sqr_footage\u0026#39;] = df[\u0026#39;BsmtFinSF1\u0026#39;] + df[\u0026#39;BsmtFinSF2\u0026#39;] + df[\u0026#39;1stFlrSF\u0026#39;] + df[\u0026#39;2ndFlrSF\u0026#39;] df[\u0026#39;Total_Bathrooms\u0026#39;] = df[\u0026#39;FullBath\u0026#39;] + (0.5 * df[\u0026#39;HalfBath\u0026#39;]) + df[\u0026#39;BsmtFullBath\u0026#39;] + (0.5 * df[\u0026#39;BsmtHalfBath\u0026#39;]) df[\u0026#39;Total_porch_sf\u0026#39;] = df[\u0026#39;OpenPorchSF\u0026#39;] + df[\u0026#39;3SsnPorch\u0026#39;] + df[\u0026#39;EnclosedPorch\u0026#39;] + df[\u0026#39;ScreenPorch\u0026#39;] + df[\u0026#39;WoodDeckSF\u0026#39;] df[\u0026#39;haspool\u0026#39;] = (df[\u0026#39;PoolArea\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;has2ndfloor\u0026#39;] = (df[\u0026#39;2ndFlrSF\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasgarage\u0026#39;] = (df[\u0026#39;GarageArea\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasbsmt\u0026#39;] = (df[\u0026#39;TotalBsmtSF\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasfireplace\u0026#39;] = (df[\u0026#39;Fireplaces\u0026#39;] \u0026gt; 0).astype(int) return df custom_feature_step = FunctionTransformer(add_custom_features) Mutual Information From my experiment, this seems to be the most useful tool.\n1 2 3 4 from sklearn.feature_selection import SelectKBest, mutual_info_regression # Select top k features based on MI mi_selector = SelectKBest(score_func=mutual_info_regression, k=50) # or \u0026#39;k=\u0026#34;all\u0026#34;\u0026#39; to get scores PCA This is not used in the end.\n1 2 from sklearn.decomposition import PCA pca = PCA(n_components=50) # You can tune this Model-Based Feature Selection Again, this is not used in the end. Mutual information already turns out to be very effective.\n1 2 3 4 5 from sklearn.feature_selection import SelectFromModel from xgboost import XGBRegressor # Use a fitted model to select features with importance above threshold model_selector = SelectFromModel(XGBRegressor(n_estimators=100), threshold=\u0026#34;median\u0026#34;) Model Evaluation 1 2 3 4 5 6 7 8 9 10 from sklearn.pipeline import make_pipeline from sklearn.feature_selection import SelectKBest, mutual_info_regression pipeline = Pipeline([ (\u0026#34;custom_features\u0026#34;, custom_feature_step), (\u0026#34;encoder\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;feature_select\u0026#34;, SelectKBest(score_func=mutual_info_regression, k=60)), (\u0026#34;model\u0026#34;, XGBRegressor()) ]) 1 2 3 4 # Then use it with cross_val_score score = cross_val_score(pipeline, X, y_log, cv=5, scoring=rmsle_scorer) print(f\u0026#34;RMSLE scores: {-score}\u0026#34;) # Negate to get actual RMSLE values print(f\u0026#34;Mean RMSLE: {-score.mean():.5f}\u0026#34;) RMSLE scores: [0.13717768 0.15245099 0.1511411 0.12531991 0.13995213] Mean RMSLE: 0.14121 We can see a slight improvement compared to the baseline model above.\nHyperparameter Tuning and Final Predictions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # import optuna # from sklearn.model_selection import cross_val_score # def objective(trial): # xgb_params = dict( # max_depth=trial.suggest_int(\u0026#34;max_depth\u0026#34;, 2, 10), # learning_rate=trial.suggest_float(\u0026#34;learning_rate\u0026#34;, 1e-4, 1e-1, log=True), # n_estimators=trial.suggest_int(\u0026#34;n_estimators\u0026#34;, 1000, 8000), # min_child_weight=trial.suggest_int(\u0026#34;min_child_weight\u0026#34;, 1, 10), # colsample_bytree=trial.suggest_float(\u0026#34;colsample_bytree\u0026#34;, 0.2, 1.0), # subsample=trial.suggest_float(\u0026#34;subsample\u0026#34;, 0.2, 1.0), # reg_alpha=trial.suggest_float(\u0026#34;reg_alpha\u0026#34;, 1e-4, 1e2, log=True), # reg_lambda=trial.suggest_float(\u0026#34;reg_lambda\u0026#34;, 1e-4, 1e2, log=True), # ) # pipeline.set_params(**{f\u0026#34;model__{key}\u0026#34;: val for key, val in xgb_params.items()}) # score = cross_val_score(pipeline, X, y_log, cv=5, scoring=rmsle_scorer) # return -score.mean() # Minimize RMSLE # # Run Optuna optimization # study = optuna.create_study(direction=\u0026#34;minimize\u0026#34;) # study.optimize(objective, n_trials=10) 1 2 3 4 5 6 7 8 9 # This is the set of parameters for my submission xgb_params = {\u0026#39;max_depth\u0026#39;: 7, \u0026#39;learning_rate\u0026#39;: 0.004565565417769295, \u0026#39;n_estimators\u0026#39;: 4701, \u0026#39;min_child_weight\u0026#39;: 9, \u0026#39;colsample_bytree\u0026#39;: 0.5911157102802619, \u0026#39;subsample\u0026#39;: 0.265969852467484, \u0026#39;reg_alpha\u0026#39;: 0.030977607695995966, \u0026#39;reg_lambda\u0026#39;: 0.168357167207514} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from sklearn.metrics import mean_squared_log_error # Train the pipeline on the entire training set X = df_train.copy() y = X.pop(\u0026#39;SalePrice\u0026#39;) y_log = np.log(y) # Initialize the model pipeline = Pipeline([ (\u0026#34;custom_features\u0026#34;, custom_feature_step), (\u0026#34;encoder\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;feature_select\u0026#34;, SelectKBest(score_func=mutual_info_regression, k=60)), (\u0026#34;model\u0026#34;, XGBRegressor()) ]) # Properly prefix parameter names with \u0026#34;model__\u0026#34; # best_params_prefixed = {f\u0026#34;model__{key}\u0026#34;: val for key, val in study.best_params.items()} best_params_prefixed = {f\u0026#34;model__{key}\u0026#34;: val for key, val in xgb_params.items()} # Set the best parameters to the pipeline pipeline.set_params(**best_params_prefixed) # Train the model pipeline.fit(X, y_log) 1 2 3 # Predict on the test set test_predictions = pipeline.predict(df_test) predictions = np.exp(test_predictions) 1 2 3 # Save the final result output = pd.DataFrame({\u0026#39;Id\u0026#39;: df_test.index, \u0026#39;SalePrice\u0026#39;: predictions}) output.to_csv(\u0026#39;my_submission.csv\u0026#39;, index=False) ","permalink":"http://localhost:1313/projects/housing-price-prediction-with-xgboost/","summary":"\u003cp\u003eThis is my \u003ca href=\"https://www.kaggle.com/code/houmingyi/housing-price-prediction-with-xgboost\"\u003eKaggle notebook\u003c/a\u003e for the \u003ca href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\"\u003eHouse Prices - Advanced Regression Techniques\u003c/a\u003e competition on Kaggle. The goal of this competition is to predict the sale price of homes in Ames, Iowa, based on various features of the homes.\u003c/p\u003e\n\u003cp\u003eI will be using various techniques learned in the course to predict the sale price of homes. The techniques include: preprocessing the data, feature engineering, and using different machine learning models to make predictions. I will also be using cross-validation to evaluate the performance of the models and to avoid overfitting.\u003c/p\u003e","title":"Housing Price Prediction with XGBoost"},{"content":"This project implements a Monte Carlo simulation for the Fokker-Planck equation using Metal, a framework for GPU programming on Apple devices. The simulation is designed to model the diffusion of particles in a potential field, which is common in statistical physics and machine learning.\nThe codes are written in Julia and utilize the Metal.jl package to leverage the GPU for parallel computation. You can find the repository for this project here.\nKramers-Fokker-Planck Equation The Kramers-Fokker-Planck equation is the generator of stochastic process modeling a particle\u0026rsquo;s movement in a potential field. The stationary solution of the Fokker-Planck equation is the equilibrium distribution of the particle\u0026rsquo;s position.\n$$ \\begin{cases} \\rm{d}X_t = V_t \\rm{d}t,\\newline \\rm{d}V_t = \\rm{d}B_t, \\end{cases} $$$$\\cal{L} = \\frac{1}{2}\\frac{\\partial^2}{\\partial v^2} + v\\frac{\\partial}{\\partial x}.$$We consider the process in the box $Q = (-1, 1)\\times(-1, 1)$, and simulate the process starting at a point $z_0 = (x_0, v_0)\\in Q$ until it hits the boundary of the box.\nThe Monte Carlo simulation involves:\nA Metal kernel function that updates the position of the particle according to the discretized SDE, namely $$ x_{n+1} = x_n + v_n \\Delta t, \\quad v_{n+1} = v_n + \\sqrt{\\Delta t} \\xi_n,$$ where $\\xi_n \\sim \\mathcal{N}(0, 1)$ is the standard normal random variable. Since the Metal kernel does not support random number generation, we also write a simple GPU friendly random number generator based on xorshift32 and Box-Muller method. The Kernel function does not support branching, the iteration will be fixed steps, and we use mask to stop the iteration when the particle hits the boundary. We simulate the process for a large number of particles and plot the harmonic measure on the boundary of the annulus. Features of the Project GPU friendly random number generator The random number generator is based on the xorshift32 algorithm, which is a simple and efficient pseudo-random number generator. The Box-Muller transform is used to generate normally distributed random numbers from uniformly distributed random numbers.\nGiven a seed ranged from 0 to 2^32-1, the xorshift32 algorithm generates a new seed by performing bitwise operations on the current seed.\n1 2 3 4 5 6 function xorshift32(seed::UInt32)::UInt32 seed ⊻= (seed \u0026lt;\u0026lt; 13) seed ⊻= (seed \u0026gt;\u0026gt; 17) seed ⊻= (seed \u0026lt;\u0026lt; 5) return seed end Then we transform this seed to a float number in the range of (0, 1) using the following function:\n1 2 3 4 function xorshift32_float(seed::UInt32)::Float32 value = Float32(xorshift32(seed)) * 2.3283064f-10 # Scale to [0,1) return max(value, 1.0f-16) # Ensure it\u0026#39;s in (0,1) end Finally, we use the Box-Muller transform to generate normally distributed random numbers:\n1 2 3 4 5 function box_muller(u1::Float32, u2::Float32) r = sqrt(-2.0f0 * log(u1)) theta = 2.0f0 * Float32(pi) * u2 return r * cos(theta) end Masks to avoid branching The Metal.jl kernel does not support branching, so we need to avoid using if statements in the kernel code. Instead, we use masks to control the flow of the simulation. The core update function for the problem in the cube $Q$ is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 for step in 1:num_steps # Boolean masks for exit conditions mask_x = (x \u0026lt; -1.0f0 || x \u0026gt; 1.0f0) ? 1 : 0 mask_v = (v \u0026lt; -1.0f0 || v \u0026gt; 1.0f0) ? 1 : 0 mask_exit = mask_x | mask_v # Combine masks (exit if either condition is true) continue_mask = 1 - mask_exit # 1 = active, 0 = exited # Generate two uniform distributed random numbers seed1 = xorshift32(seed1) seed2 = xorshift32(seed2) random_number1 = xorshift32_float(seed1) random_number2 = xorshift32_float(seed2) # Generate a normal distributed noise noise = box_muller(random_number1, random_number2) # Perturb the seeds to avoid deterministic patterns seed1 += UInt32(i) seed2 += UInt32(i) # Update position and velocity and store previous state if not exit x_prev, v_prev = continue_mask * x + mask_exit * x_prev, continue_mask * v + mask_exit * v_prev x += continue_mask * (v * time_step) v += continue_mask * (sqrt(time_step) * noise) end The mask_exit variable is used to check if the particle has exited the box. If it has, we set the continue_mask to 0, which effectively stops the simulation for that particle. The x_prev and v_prev variables are used to store the previous state of the particle before it exited.\nExample plots Consider the following Dirichlet boundary condition: Our codes can simulate the solution efficiently. The following plot shows the full solution and also a zoomed-in view of the solution near the singular boundary: In addition, we can plot the exit points distribution on the boundary for a starting point. The following is an example in the annulus: ","permalink":"http://localhost:1313/projects/monte-carlo-kfp-metal/","summary":"\u003cp\u003eThis project implements a Monte Carlo simulation for the Fokker-Planck equation using \u003ccode\u003eMetal\u003c/code\u003e, a framework for GPU programming on Apple devices. The simulation is designed to model the diffusion of particles in a potential field, which is common in statistical physics and machine learning.\u003c/p\u003e\n\u003cp\u003eThe codes are written in \u003ccode\u003eJulia\u003c/code\u003e and utilize the \u003ccode\u003eMetal.jl\u003c/code\u003e package to leverage the GPU for parallel computation. You can find the repository for this project \u003ca href=\"https://github.com/mingyi-ai/Monte_Carlo_KFP\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"Monte Carlo Simulation for Fokker-Planck Equations using Metal"},{"content":"In this post, we will explore the numerics of Hermite polynomials, as part of a numerical solver project for Kramers hyperbolic system. In particular, we try to compute eigenvectors numerically for the Hermite generating matrix, which is a tridiagonal matrix given by the recurrence relation\n$$ x \\mathrm{H}_{n}(x) = \\sqrt{n+1} \\mathrm{H}_{n+1}(x) - \\sqrt{n} \\mathrm{H}_{n-1}(x) $$for \\( n \\geq 1 \\) with \\(\\mathrm{H}_{n}(x)\\) being the \\(n\\)-th normalized Hermite polynomial. The generating matrix is given by\n$$ \\mathrm{T}_n = \\begin{pmatrix} 0 \u0026 \\sqrt{1} \u0026 0 \u0026 \\cdots \u0026 \\cdots \\\\ \\sqrt{1} \u0026 0 \u0026 \\sqrt{2} \u0026 \\cdots \u0026 \\cdots \\\\ 0 \u0026 \\sqrt{2} \u0026 \\ddots \u0026 \\cdots \u0026 \\cdots \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 0 \u0026 \\sqrt{n-1} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\sqrt{n-1} \u0026 0 \\end{pmatrix}. $$All the codes below are implemented in Julia.\nTheoretical eigenvalues and eigenvectors Recall that the Hermite polynomials are defined by\n$$ \\mathrm{He}_n(x) = (-1)^n \\exp(x^2/2) \\frac{\\mathrm{d}^n}{\\mathrm{d}x^n} \\left( \\exp(-x^2/2) \\right) $$for $n \\geq 0$. The first few Hermite polynomials are given by\n$$ \\mathrm{He}_0(x) = 1, \\\\ \\mathrm{He}_1(x) = x, \\\\ \\mathrm{He}_2(x) = x^2 - 1, \\\\ \\mathrm{He}_3(x) = x^3 - 3x. $$The Hermite polynomials can be computed using the recurrence relation\n$$ \\mathrm{He}_{n+1}(x) = x \\mathrm{He}_{n}(x) - n \\mathrm{He}_{n-1}(x) $$for $n \\geq 2$ with $\\mathrm{He}_0(x) = 1$ and $\\mathrm{He}_1(x) = x$.\nThe Hermite polynomials are orthogonal with respect to the (normalized) weight function $w(x) = \\exp(-x^2/2)/\\sqrt{2\\pi}$ on the interval $(-\\infty, \\infty)$. The $\\mathrm{L}^2$ norm of $\\mathrm{He}_n(x)$ with respect to this weight function is given by\n$$ ||\\mathrm{He}_n||_{\\mathrm{L}^2} = \\sqrt{n!} $$ for $n \\geq 0$. Let\u0026rsquo;s denote the normalized Hermite polynomials by\n$$ \\mathrm{H}_n = \\frac{\\mathrm{He}_n}{||\\mathrm{He}_n||_{\\mathrm{L}^2}} = \\frac{\\mathrm{He}_n}{\\sqrt{n!}}. $$It is well-known that the eigenvalues of the Hermite generating matrix $\\mathrm{T}_n$ of size $n$ are given by the roots of the Hermite polynomial $\\mathrm{H}_n(x)$, and the eigenvectors are given by the values of the Hermite polynomials at the roots, namely\n$$ \\mathrm{T}_n \\mathbf{v}_{n,k} = \\lambda_{n,k} \\mathbf{v}_{n,k} $$ where $\\lambda_{n,k}$ is the $k$-th root of $\\mathrm{He}_n(x)$, and\n$$ \\mathbf{v}_{n,k} = \\begin{pmatrix} \\mathrm{H}_0(\\lambda_{n,k}) \\\\ \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\cdots \\\\ \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\end{pmatrix} $$ is the corresponding eigenvector.\nTo see this, we do a direct computation\n$$ \\mathrm{T}_n \\mathbf{v}_{n,k} = \\begin{pmatrix} \\sqrt{1} \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\sqrt{1} \\mathrm{H}_0(\\lambda_{n,k}) + \\sqrt{2} \\mathrm{H}_2(\\lambda_{n,k}) \\\\ \\cdots\\\\ \\sqrt{n-2} \\mathrm{H}_{n-3}(\\lambda_{n,k}) + \\sqrt{n-1} \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\\\ \\sqrt{n-1} \\mathrm{H}_{n-2}(\\lambda_{n,k}) + \\sqrt{n} \\underbrace{\\mathrm{H}_{n}(\\lambda_{n,k})}_{=0} \\end{pmatrix} = \\begin{pmatrix} \\lambda_{n,k} \\mathrm{H}_0(\\lambda_{n,k}) \\\\ \\lambda_{n,k} \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\cdots \\\\ \\lambda_{n,k} \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\end{pmatrix} = \\lambda_{n,k} \\mathbf{v}_{n,k}. $$Let\u0026rsquo;s define $\\mathbf{w}_{n,k} = {\\mathbf{v}_{n,k}}/{||\\mathbf{v}_{n,k}||_2}$, which is the normalized eigenvector, and\n$$ \\mathrm{P}_n := \\begin{pmatrix} \\mathbf{w}_{n,1} \u0026 \\mathbf{w}_{n,2} \u0026 \\cdots \u0026 \\mathbf{w}_{n,n} \\end{pmatrix} $$is the change of basis matrix. It follows that\n$$ \\mathrm{P}_n^T \\mathrm{T}_n \\mathrm{P}_n = \\mathrm{D}_n:= \\mathrm{diag}(\\lambda_{n,1}, \\lambda_{n,1}, \\cdots, \\lambda_{n,n}). $$Thus, in theory, we can compute the eigenvalues and eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ by computing the roots of the normalized Hermite polynomial $\\mathrm{H}_n(x)$ and evaluating the normalized Hermite polynomials at these roots.\nNumerical properties of the Hermite polynomials Numerical instability when evaluating the Hermite polynomials The first thing we need to note is that the Hermite polynomials\u0026rsquo; coefficients grow very quickly as $n$ increases. Very large and very small coefficients exist in the polynomial, which can lead to numerical instability when evaluating the polynomial. For example,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 using Polynomials function He_symbolic(n::Int) He_0 = Polynomial([1.0]) if n == 0 return He_0 end He_1 = Polynomial([0.0, 1.0]) if n == 1 return He_1 end for k in 2:n He_2 = Polynomial([0.0, 1.0]) * He_1 - (k - 1) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end 1 He_symbolic(15) -2.027025e6∙x + 4.729725e6∙x3 - 2.837835e6∙x5 + 675675.0∙x7 - 75075.0∙x9 + 4095.0∙x11 - 105.0∙x13 + 1.0∙x15\nFor $n=20$, the largest coefficient is around $10^{6}$, while the smallest is always $1.0$, this could lead to numerical instability when evaluating the polynomial directly. To solve this problem, we can use the recurrence relation to evaluate the polynomial, which should give us a more stable result for small $n$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function He(n::Int, x::Float64) He_0 = 1.0 if n == 0 return He_0 end He_1 = x for k in 2:n He_2 = x * He_1 - (k - 1) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end 1 2 3 4 5 6 x = 1.1 n = 51 val_direct = He_symbolic(n)(x) val_recursive = He(n, x) val_direct, val_recursive (-5.624972601518976e32, -5.624972601518795e32) We can see relatively small but noticeable discrepancies between the two methods, since Polynomials.jl already uses numerically stable methods to evaluate a polynomial. For our purposes, we use the recursive evaluation.\nNumerical instability when computing the roots of the Hermite polynomials The second thing we need to note is that the roots of the Hermite polynomials are very close to each other, especially for large $n$. This can lead to numerical instability when computing the roots of the polynomial. For example, for $n=15$, the roots are\n1 println(sort(roots(He_symbolic(15)))) [-6.36394788882981, -5.190093591304892, -4.196207711268877, -3.2890824243988406, -2.4324368270097385, -1.6067100690287315, -0.7991290683245481, 0.0, 0.7991290683245479, 1.6067100690287317, 2.4324368270097474, 3.2890824243988055, 4.196207711268944, 5.190093591304826, 6.36394788882981] As we can see, roots are clustered around $x=0$, and does not become more spaced out as $n$ increases. To see this, we can plot the roots of the Hermite polynomials for $n=1, \\dots, 20$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 using Plots # Collect roots for n=1:20 root_data = [] for n in 1:20 push!(root_data, roots(He_symbolic(n))) end # Flatten the root data and associate each root with its corresponding n x_vals = vcat(root_data...) y_vals = vcat([fill(n, length(root_data[n])) for n in 1:20]...) # Plot the roots distribution scatter(x_vals, y_vals, xlabel=\u0026#34;Roots\u0026#34;, ylabel=\u0026#34;n\u0026#34;, title=\u0026#34;Roots Distribution of Hermite polynomials for n=1:20\u0026#34;, legend=false) To efficiently compute the roots of the Hermite polynomials, we can use the generating matrix $\\mathrm{T}_n$ and compute the eigenvalues of the matrix. This is a numerically stable method as the matrix is tridiagonal, symmetric, and there exists numerically stable algorithms to compute the eigenvalues and eigenvectors of such matrices.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 function T(n::Int) d = zeros(n) e = sqrt.(1:n-1) T = SymTridiagonal(d, e) return T end using LinearAlgebra function get_eigen(n::Int) Tn = T(n) F = eigen(Tn) # Eigenvalues and eigenvectors idx = sortperm(F.values) # Sort eigenvalues and eigenvectors eigenvals = F.values[idx] # Sorted eigenvalues (roots) eigenvecs = F.vectors[:, idx] # Sorted eigenvectors # One step of Newton\u0026#39;s method to refine the roots He_nx = He.(n, eigenvals) # Hermite polynomial at the roots He_nm1x = He.(n-1, eigenvals) # Candidate for the derivative eigenvals .-= He_nx ./ (n .* He_nm1x) return eigenvals, eigenvecs end get_eigen (generic function with 1 method) 1 2 3 eigenvals, _ = get_eigen(5) println(\u0026#34;Eigenvalues for n=5: \u0026#34;, eigenvals) println(sort(roots(He_symbolic(5)))) Eigenvalues for n=5: [-2.8569700138728056, -1.355626179974266, 0.0, 1.355626179974266, 2.8569700138728056] [-2.856970013872804, -1.3556261799742675, 0.0, 1.3556261799742657, 2.856970013872808] From the above comparison, we can see that the eigenvalues computed from the generating matrix preserve the symmetry of the roots better. As $n$ increases, this algorithm becomes more efficient than the root-finding algorithm, as the roots become more clustered together.\nNormalized eigenvectors of the Hermite generating matrix Now we are in a position to compute the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$. Note that we already computed the eigenvalues and eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ in the previous section using a numerically robust method. In this section, we will construct the normalized eigenvectors of the Hermite generating matrix by evaluating the normalized Hermite polynomials at the roots of the Hermite polynomial, i.e. $\\mathbf{v}_{n,k}$ for $k=1, \\dots, n$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 using SpecialFunctions: logfactorial # An auxiliary function to compute the normalization constant function He_l2norm(n::Int) return sqrt(exp(logfactorial(n))) end function analytic_eigenvecs(n::Int) Pmat = zeros(Float64, n, n) # initializing the matrix rts, _ = get_eigen(n) # Get the roots for i in 1:n root_i = rts[i] for j in 1:n Pmat[j, i] = He(j-1, root_i) / He_l2norm(j-1) end # Normalize each vector (column) Pmat[:, i] /= norm(Pmat[:, i]) end return Pmat end Numerical properties of the eigenvectors Now, we have two ways to compute the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$: one is theoretically precise, the other is numerical. We will compare the two methods and see how well they agree with each other.\nThere are two important properties of the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ that we need to note:\nThe column matrix $\\mathrm{P}_n$ of eigenvectors is orthonormal, i.e. $\\mathrm{P}_n^T \\mathrm{P}_n = \\mathrm{I}_n$. By conjugating the eigenvectors, we can obtain the eigenvalues of the Hermite generating matrix $\\mathrm{T}_n$, which is a diagonal matrix $\\mathrm{D}_n$ with the roots of the Hermite polynomial on the diagonal. $$ \\mathrm{P}_n^T \\mathrm{T}_n \\mathrm{P}_n = \\mathrm{D}_n. $$ We will check how well these properties hold for both methods of computing the eigenvectors. The following codes compute the $\\infty$ norm of the above two properties.\n1 2 3 4 5 6 7 function orthonormality_error(P::Matrix{Float64}) return norm(P\u0026#39; * P - I, Inf) end function diagonal_error(P::Matrix{Float64}, eigenvals::Vector{Float64}, n::Int) return norm(P\u0026#39; * T(n) * P - Diagonal(eigenvals), Inf) end Show Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Define the range of n n_values = 2:200 # Initialize error arrays for both methods analytic_orth_errors = Float64[] analytic_diag_errors = Float64[] numerical_orth_errors = Float64[] numerical_diag_errors = Float64[] # Compute errors for both methods for n in n_values P_analytic = analytic_eigenvecs(n) rts, P_numerical = get_eigen(n) push!(analytic_orth_errors, orthonormality_error(P_analytic)) push!(analytic_diag_errors, diagonal_error(P_analytic, rts, n)) push!(numerical_orth_errors, orthonormality_error(P_numerical)) push!(numerical_diag_errors, diagonal_error(P_numerical, rts, n)) end # Create 2x2 subplots p1 = plot(n_values, log10.(analytic_orth_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Orthonormality Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p1, n_values, log10.(numerical_orth_errors), label=\u0026#34;Numerical\u0026#34;) p2 = plot(n_values, log10.(analytic_diag_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Diagonalization Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p2, n_values, log10.(numerical_diag_errors), label=\u0026#34;Numerical\u0026#34;) plot(p1, p2, layout=(2, 1)) From the plot we can see that the analytical method is marginally better than the numerical method before $n\\approx 170$, but both errors for analytical method blow up after $n\\approx 170$, while the numerical method remains stable. The reason for this is that in the analytical method, we need to compute the normalization factor of the Hermite polynomial, which is given by $||\\mathrm{He}_n||_{\\mathrm{L}^2} = \\sqrt{n!}$, and the factorial grows very quickly, eventually blows up at $n\\approx 170$.\n1 2 3 for n in 169:172 print(He_l2norm(n), \u0026#34;, \u0026#34;) end 2.0661723086434517e152, 2.6939590968143674e153, Inf, Inf, To solve this problem, we can use the recurrence relation to compute the normalized Hermite polynomials as before, which avoids the need to compute the normalization factor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 function Henorm(n::Int, x::Float64) He_0 = 1.0 if n == 0 return He_0 end He_1 = x for k in 2:n He_2 = x * He_1 / sqrt(k) - sqrt((k - 1) / k) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end function analytic_eigenvecs_norm(n::Int) Pmat = zeros(Float64, n, n) # initializing the matrix rts, _ = get_eigen(n) # Get the roots for i in 1:n root_i = rts[i] for j in 1:n Pmat[j, i] = Henorm(j-1, root_i) end # Normalize each vector (column) Pmat[:, i] /= norm(Pmat[:, i]) end return Pmat end Show Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Define the range of n n_values = 2:200 # Initialize error arrays for both methods analytic_orth_errors = Float64[] analytic_diag_errors = Float64[] numerical_orth_errors = Float64[] numerical_diag_errors = Float64[] # Compute errors for both methods for n in n_values P_analytic = analytic_eigenvecs_norm(n) rts, P_numerical = get_eigen(n) push!(analytic_orth_errors, orthonormality_error(P_analytic)) push!(analytic_diag_errors, diagonal_error(P_analytic, rts, n)) push!(numerical_orth_errors, orthonormality_error(P_numerical)) push!(numerical_diag_errors, diagonal_error(P_numerical, rts, n)) end # Create 2x2 subplots p1 = plot(n_values, log10.(analytic_orth_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Orthonormality Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p1, n_values, log10.(numerical_orth_errors), label=\u0026#34;Numerical\u0026#34;) p2 = plot(n_values, log10.(analytic_diag_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Diagonalization Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p2, n_values, log10.(numerical_diag_errors), label=\u0026#34;Numerical\u0026#34;) plot(p1, p2, layout=(2, 1)) After the above changes, we can see that the analytical method is now more stable than the numerical method, and the errors are much smaller. However, in practice, using large $n$ for the Hermite polynomials is not recommended, as there are already numerical instabilities when evaluating the Hermite polynomials.\nDownload the notebook 📥 Download Notebook\n","permalink":"http://localhost:1313/posts/25-05-02_hermite-numerics/","summary":"\u003cp\u003eIn this post, we will explore the numerics of Hermite polynomials, as part of a numerical solver project for Kramers hyperbolic system. In particular, we try to compute eigenvectors numerically for the Hermite generating matrix, which is a tridiagonal matrix given by the recurrence relation\u003c/p\u003e\n$$\nx \\mathrm{H}_{n}(x) = \\sqrt{n+1} \\mathrm{H}_{n+1}(x) - \\sqrt{n} \\mathrm{H}_{n-1}(x)\n$$\u003cp\u003efor \\( n \\geq 1 \\) with \\(\\mathrm{H}_{n}(x)\\) being the \\(n\\)-th normalized Hermite polynomial.\nThe generating matrix is given by\u003c/p\u003e","title":"Numerical Methods for Hermite Polynomials"},{"content":"I am a PhD student in Mathematics at Uppsala University. My research interests lie in the intersection of probability theory, partial differential equations, and machine learning. I am particularly interested in the mathematical analysis of stochastic processes and their applications in machine learning.\nResearch Publications Preprints M. Hou. Variational Formulation and Capacity Estimates for Non-Self-Adjoint Fokker-Planck Operators in Divergence Form. arXiv preprint, 2025. M. Hou. Boundedness of Weak Solutions to Degenerate Kolmogorov Equations of Hypoelliptic Type in Bounded Domains. arXiv preprint, 2024. B. Avelin, and M. Hou. Weak and Perron\u0026rsquo;s Solutions for Stationary Kramers-Fokker-Planck Equations in Bounded Domains. arXiv preprint, 2024. Published B. Avelin, M. Hou, and K. Nyström. A Galerkin Type Method for Kinetic Fokker-Planck Equations Based on Hermite Expansions. Kinet. Relat. Models, 2024. Thesis M. Hou. Behind the Training Dynamics of Neural Networks: Analysis of Fokker-Planck Equations and the Path to Metastability. PhD dissertation, Uppsala University, 2025. Contact Department of Mathematics\nUppsala University\n751 06 Uppsala\nSweden\nmingyi.hou@math.uu.se LinkedIn GitHub ","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003eI am a PhD student in Mathematics at Uppsala University. My research interests lie in the intersection of probability theory, partial differential equations, and machine learning. I am particularly interested in the mathematical analysis of stochastic processes and their applications in machine learning.\u003c/p\u003e\n\u003ch2 id=\"research-publications\"\u003eResearch Publications\u003c/h2\u003e\n\u003ch3 id=\"preprints\"\u003ePreprints\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eM. Hou. Variational Formulation and Capacity Estimates for Non-Self-Adjoint Fokker-Planck Operators in Divergence Form. \u003ca href=\"https://arxiv.org/abs/2502.12036\"\u003earXiv preprint, 2025\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eM. Hou. Boundedness of Weak Solutions to Degenerate Kolmogorov Equations of Hypoelliptic Type in Bounded Domains. \u003ca href=\"https://arxiv.org/abs/2407.00800\"\u003earXiv preprint, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eB. Avelin, and M. Hou. Weak and Perron\u0026rsquo;s Solutions for Stationary Kramers-Fokker-Planck Equations in Bounded Domains. \u003ca href=\"https://arxiv.org/abs/2405.04070\"\u003earXiv preprint, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"published\"\u003ePublished\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eB. Avelin, M. Hou, and K. Nyström. A Galerkin Type Method for Kinetic Fokker-Planck Equations Based on Hermite Expansions. \u003ca href=\"https://doi.org/10.3934/krm.2023035\"\u003eKinet. Relat. Models, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"thesis\"\u003eThesis\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eM. Hou. Behind the Training Dynamics of Neural Networks: Analysis of Fokker-Planck Equations and the Path to Metastability. \u003ca href=\"https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-553381\"\u003ePhD dissertation, Uppsala University, 2025.\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"contact\"\u003eContact\u003c/h2\u003e\n\u003cdiv style=\"display: flex; justify-content: space-between; flex-wrap: wrap;\"\u003e\n  \u003cdiv style=\"flex: 1; min-width: 200px;\"\u003e\n    \u003cp\u003eDepartment of Mathematics\u003cbr\u003e\n    Uppsala University\u003cbr\u003e\n    751 06 Uppsala\u003cbr\u003e\n    Sweden\u003c/p\u003e","title":"About Me"},{"content":"This is my Kaggle notebook for the House Prices - Advanced Regression Techniques competition on Kaggle. The goal of this competition is to predict the sale price of homes in Ames, Iowa, based on various features of the homes.\nI will be using various techniques learned in the course to predict the sale price of homes. The techniques include: preprocessing the data, feature engineering, and using different machine learning models to make predictions. I will also be using cross-validation to evaluate the performance of the models and to avoid overfitting.\nExploratory Data Analysis (EDA) Load Libraries \u0026amp; Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from pathlib import Path import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # Set plot styles sns.set(style=\u0026#34;whitegrid\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (12, 6) # Load data data_dir = Path(\u0026#34;./house-prices-advanced-regression-techniques\u0026#34;) df_train = pd.read_csv(data_dir / \u0026#34;train.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) df_test = pd.read_csv(data_dir / \u0026#34;test.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) print(f\u0026#34;Train shape: {df_train.shape}\u0026#34;) print(f\u0026#34;Test shape: {df_test.shape}\u0026#34;) Train shape: (1460, 80) Test shape: (1459, 79) Understand the Target Variable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Distribution of target sns.histplot(df_train[\u0026#34;SalePrice\u0026#34;], kde=True) plt.title(\u0026#34;SalePrice Distribution\u0026#34;) plt.xlabel(\u0026#34;SalePrice\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.savefig(\u0026#34;saleprice_distribution.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() # Log-transform target to check skew sns.histplot(np.log1p(df_train[\u0026#34;SalePrice\u0026#34;]), kde=True) plt.title(\u0026#34;Log-Transformed SalePrice\u0026#34;) plt.xlabel(\u0026#34;Log(SalePrice + 1)\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.savefig(\u0026#34;log_saleprice_distribution.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() # Summary stats df_train[\u0026#34;SalePrice\u0026#34;].describe() count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000 Name: SalePrice, dtype: float64 Overview of the Dataset 1 2 3 4 5 6 7 df = pd.concat([df_train, df_test], axis=0) # df.info() # df.describe() # Missing value heatmap import missingno as msno msno.matrix(df) \u0026lt;Axes: \u0026gt; Data Cleaning 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def clean_data(df): # Clean to_category = [\u0026#39;MSSubClass\u0026#39;, \u0026#39;MoSold\u0026#39;, \u0026#39;YrSold\u0026#39;, \u0026#39;GarageYrBlt\u0026#39;, \u0026#39;YearBuilt\u0026#39;, \u0026#39;YearRemodAdd\u0026#39;] for col in to_category: df[col] = df[col].astype(str) df[\u0026#39;Functional\u0026#39;] = df[\u0026#39;Functional\u0026#39;].fillna(\u0026#39;Typ\u0026#39;) df[\u0026#34;Electrical\u0026#34;] = df[\u0026#34;Electrical\u0026#34;].fillna(\u0026#39;SBrkr\u0026#39;) df[\u0026#34;KitchenQual\u0026#34;] = df[\u0026#34;KitchenQual\u0026#34;].fillna(\u0026#39;TA\u0026#39;) df[\u0026#34;Exterior1st\u0026#34;] = df[\u0026#34;Exterior1st\u0026#34;].fillna(df[\u0026#34;Exterior1st\u0026#34;].mode()[0]) df[\u0026#34;Exterior2nd\u0026#34;] = df[\u0026#34;Exterior2nd\u0026#34;].fillna(df[\u0026#34;Exterior2nd\u0026#34;].mode()[0]) df[\u0026#34;SaleType\u0026#34;] = df[\u0026#34;SaleType\u0026#34;].fillna(df[\u0026#34;SaleType\u0026#34;].mode()[0]) # Impute # Fill missing values in object columns with \u0026#34;None\u0026#34; objects = [] for i in df.columns: if df[i].dtype == object: objects.append(i) df.update(df[objects].fillna(\u0026#39;None\u0026#39;)) # Fill missing values in numeric columns with 0 numerics = [] for i in df.columns: if df[i].dtype != object: numerics.append(i) df.update(df[numerics].fillna(0)) return df 1 2 3 4 5 6 7 8 9 10 11 12 13 def load_data(): data_dir = Path(\u0026#34;./house-prices-advanced-regression-techniques\u0026#34;) df_train = pd.read_csv(data_dir / \u0026#34;train.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) df_test = pd.read_csv(data_dir / \u0026#34;test.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) # Clean data df = pd.concat([df_train, df_test], axis=0) df = clean_data(df) # Split back into train and test df_train = df.loc[df_train.index, :] df_test = df.loc[df_test.index, :].drop(columns=[\u0026#34;SalePrice\u0026#34;]) return df_train, df_test 1 2 3 4 5 6 7 df_train, df_test = load_data() # Check the cleaned data # train_missing = df_train.isnull().sum() # print(train_missing[train_missing \u0026gt; 0]) # test_missing = df_test.isnull().sum() # print(test_missing[test_missing \u0026gt; 0]) # df_train.info() Correlation with Target (Numerical Features) 1 2 3 4 5 6 7 8 9 10 11 # Compute correlation matrix corr_matrix = df_train.corr(numeric_only=True) # Get top 15 features correlated with SalePrice top_corr = corr_matrix[\u0026#34;SalePrice\u0026#34;].abs().sort_values(ascending=False).head(15) # Visualize sns.heatmap(df_train[top_corr.index].corr(), annot=True, cmap=\u0026#34;coolwarm\u0026#34;, fmt=\u0026#34;.2f\u0026#34;) plt.title(\u0026#34;Top Correlated Features with SalePrice\u0026#34;) plt.savefig(\u0026#34;top_correlated_features.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() Categorical Features Preview 1 2 3 4 5 6 7 8 9 10 11 12 13 categoricals = df_train.select_dtypes(include=\u0026#34;object\u0026#34;).columns print(f\u0026#34;Categorical features: {len(categoricals)}\u0026#34;) # print(categoricals.tolist()) # Example: Visualize average SalePrice by a few important categorical features important_cats = [\u0026#34;Neighborhood\u0026#34;, \u0026#34;ExterQual\u0026#34;, \u0026#34;GarageFinish\u0026#34;, \u0026#34;KitchenQual\u0026#34;] for col in important_cats: sns.boxplot(data=df_train, x=col, y=\u0026#34;SalePrice\u0026#34;) plt.title(f\u0026#34;SalePrice by {col}\u0026#34;) plt.xticks(rotation=45) plt.savefig(f\u0026#34;saleprice_by_{col}.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() Categorical features: 49 Time-Related Patterns 1 2 3 4 5 6 7 sns.boxplot(x=\u0026#34;YrSold\u0026#34;, y=\u0026#34;SalePrice\u0026#34;, data=df_train) plt.title(\u0026#34;SalePrice by Year Sold\u0026#34;) plt.show() sns.scatterplot(x=\u0026#34;YearBuilt\u0026#34;, y=\u0026#34;SalePrice\u0026#34;, data=df_train) plt.title(\u0026#34;SalePrice vs Year Built\u0026#34;) plt.show() It does not look like there is any time related feature.\nFeature Engineering 1 2 3 X = df_train.copy() y = X.pop(\u0026#34;SalePrice\u0026#34;) # X.info() Separate Categorical and Numerical Features 1 2 3 4 # cat_cols = X.select_dtypes(include=[\u0026#39;object\u0026#39;]).columns.tolist() # num_cols = X.select_dtypes(exclude=[\u0026#39;object\u0026#39;]).columns.tolist() # print(f\u0026#34;Categorical columns: {len(cat_cols)}\u0026#34;) # print(f\u0026#34;Numerical columns: {len(num_cols)}\u0026#34;) Encoding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from sklearn.base import BaseEstimator, TransformerMixin from category_encoders import TargetEncoder class Encoder(BaseEstimator, TransformerMixin): def __init__(self, ordered_levels): self.ordered_levels = ordered_levels self.low_cardinality = [] self.high_cardinality = [] self.te = None def fit(self, X, y): X = X.copy() self.ordered_levels = {k: [\u0026#34;None\u0026#34;] + v for k, v in self.ordered_levels.items()} for col, order in self.ordered_levels.items(): if col in X.columns: X[col] = pd.Categorical(X[col], categories=order, ordered=True).codes ordinal_cols = list(self.ordered_levels.keys()) nominal_cols = [col for col in X.select_dtypes(include=\u0026#39;object\u0026#39;).columns if col not in ordinal_cols] self.low_cardinality = [col for col in nominal_cols if X[col].nunique() \u0026lt;= 10] self.high_cardinality = [col for col in nominal_cols if X[col].nunique() \u0026gt; 10] for col in self.low_cardinality: X[col] = X[col].astype(\u0026#34;category\u0026#34;).cat.codes self.te = TargetEncoder() self.te.fit(X[self.high_cardinality], y) return self def transform(self, X): X = X.copy() for col, order in self.ordered_levels.items(): if col in X.columns: X[col] = pd.Categorical(X[col], categories=order, ordered=True).codes for col in self.low_cardinality: if col in X.columns: X[col] = X[col].astype(\u0026#34;category\u0026#34;).cat.codes if self.te: X[self.high_cardinality] = self.te.transform(X[self.high_cardinality]) return X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 five_levels = [\u0026#34;Po\u0026#34;, \u0026#34;Fa\u0026#34;, \u0026#34;TA\u0026#34;, \u0026#34;Gd\u0026#34;, \u0026#34;Ex\u0026#34;] ten_levels = list(range(1, 11)) # 1 - 10 is the correct range! ordered_levels = { \u0026#34;OverallQual\u0026#34;: ten_levels, \u0026#34;OverallCond\u0026#34;: ten_levels, \u0026#34;ExterQual\u0026#34;: five_levels, \u0026#34;ExterCond\u0026#34;: five_levels, \u0026#34;BsmtQual\u0026#34;: five_levels, \u0026#34;BsmtCond\u0026#34;: five_levels, \u0026#34;HeatingQC\u0026#34;: five_levels, \u0026#34;KitchenQual\u0026#34;: five_levels, \u0026#34;FireplaceQu\u0026#34;: five_levels, \u0026#34;GarageQual\u0026#34;: five_levels, \u0026#34;GarageCond\u0026#34;: five_levels, \u0026#34;PoolQC\u0026#34;: five_levels, \u0026#34;LotShape\u0026#34;: [\u0026#34;Reg\u0026#34;, \u0026#34;IR1\u0026#34;, \u0026#34;IR2\u0026#34;, \u0026#34;IR3\u0026#34;], \u0026#34;LandSlope\u0026#34;: [\u0026#34;Sev\u0026#34;, \u0026#34;Mod\u0026#34;, \u0026#34;Gtl\u0026#34;], \u0026#34;BsmtExposure\u0026#34;: [\u0026#34;No\u0026#34;, \u0026#34;Mn\u0026#34;, \u0026#34;Av\u0026#34;, \u0026#34;Gd\u0026#34;], \u0026#34;BsmtFinType1\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;LwQ\u0026#34;, \u0026#34;Rec\u0026#34;, \u0026#34;BLQ\u0026#34;, \u0026#34;ALQ\u0026#34;, \u0026#34;GLQ\u0026#34;], \u0026#34;BsmtFinType2\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;LwQ\u0026#34;, \u0026#34;Rec\u0026#34;, \u0026#34;BLQ\u0026#34;, \u0026#34;ALQ\u0026#34;, \u0026#34;GLQ\u0026#34;], \u0026#34;Functional\u0026#34;: [\u0026#34;Sal\u0026#34;, \u0026#34;Sev\u0026#34;, \u0026#34;Maj1\u0026#34;, \u0026#34;Maj2\u0026#34;, \u0026#34;Mod\u0026#34;, \u0026#34;Min2\u0026#34;, \u0026#34;Min1\u0026#34;, \u0026#34;Typ\u0026#34;], \u0026#34;GarageFinish\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;RFn\u0026#34;, \u0026#34;Fin\u0026#34;], \u0026#34;PavedDrive\u0026#34;: [\u0026#34;N\u0026#34;, \u0026#34;P\u0026#34;, \u0026#34;Y\u0026#34;], \u0026#34;Utilities\u0026#34;: [\u0026#34;NoSeWa\u0026#34;, \u0026#34;NoSewr\u0026#34;, \u0026#34;AllPub\u0026#34;], \u0026#34;CentralAir\u0026#34;: [\u0026#34;N\u0026#34;, \u0026#34;Y\u0026#34;], \u0026#34;Electrical\u0026#34;: [\u0026#34;Mix\u0026#34;, \u0026#34;FuseP\u0026#34;, \u0026#34;FuseF\u0026#34;, \u0026#34;FuseA\u0026#34;, \u0026#34;SBrkr\u0026#34;], \u0026#34;Fence\u0026#34;: [\u0026#34;MnWw\u0026#34;, \u0026#34;GdWo\u0026#34;, \u0026#34;MnPrv\u0026#34;, \u0026#34;GdPrv\u0026#34;], } 1 2 3 4 5 6 7 from sklearn.pipeline import Pipeline from xgboost import XGBRegressor basepipe = Pipeline([ (\u0026#34;encode\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;xgb\u0026#34;, XGBRegressor(random_state=42)) ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from sklearn.metrics import make_scorer, mean_squared_log_error from sklearn.model_selection import cross_val_score import numpy as np # Custom RMSLE scorer (greater_is_better=False because lower is better) rmsle_scorer = make_scorer( lambda y_true, y_pred: np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred))), greater_is_better=False ) y_log = np.log(y) # We train on the log transformed target # Then use it with cross_val_score score = cross_val_score(basepipe, X, y_log, cv=5, scoring=rmsle_scorer) print(f\u0026#34;RMSLE scores: {-score}\u0026#34;) # Negate to get actual RMSLE values print(f\u0026#34;Mean RMSLE: {-score.mean():.5f}\u0026#34;) RMSLE scores: [0.1397286 0.15149179 0.14633292 0.1268186 0.14509794] Mean RMSLE: 0.14189 Transform Skewed Numerical Features This transform is not used in the end.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class SkewedFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self, threshold=0.75): self.threshold = threshold self.skewed_cols = [] def fit(self, X, y=None): X = pd.DataFrame(X) skewness = X.skew().abs() self.skewed_cols = skewness[skewness \u0026gt; self.threshold].index.tolist() return self def transform(self, X): X = pd.DataFrame(X).copy() for col in self.skewed_cols: X[col] = np.log1p(X[col]) return X skew_transformer = SkewedFeatureTransformer() Add New Features 1 2 from sklearn.preprocessing import FunctionTransformer from sklearn.pipeline import Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def add_custom_features(df): df = df.copy() df[\u0026#39;TotalSF\u0026#39;] = df[\u0026#39;TotalBsmtSF\u0026#39;] + df[\u0026#39;1stFlrSF\u0026#39;] + df[\u0026#39;2ndFlrSF\u0026#39;] df[\u0026#39;Total_sqr_footage\u0026#39;] = df[\u0026#39;BsmtFinSF1\u0026#39;] + df[\u0026#39;BsmtFinSF2\u0026#39;] + df[\u0026#39;1stFlrSF\u0026#39;] + df[\u0026#39;2ndFlrSF\u0026#39;] df[\u0026#39;Total_Bathrooms\u0026#39;] = df[\u0026#39;FullBath\u0026#39;] + (0.5 * df[\u0026#39;HalfBath\u0026#39;]) + df[\u0026#39;BsmtFullBath\u0026#39;] + (0.5 * df[\u0026#39;BsmtHalfBath\u0026#39;]) df[\u0026#39;Total_porch_sf\u0026#39;] = df[\u0026#39;OpenPorchSF\u0026#39;] + df[\u0026#39;3SsnPorch\u0026#39;] + df[\u0026#39;EnclosedPorch\u0026#39;] + df[\u0026#39;ScreenPorch\u0026#39;] + df[\u0026#39;WoodDeckSF\u0026#39;] df[\u0026#39;haspool\u0026#39;] = (df[\u0026#39;PoolArea\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;has2ndfloor\u0026#39;] = (df[\u0026#39;2ndFlrSF\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasgarage\u0026#39;] = (df[\u0026#39;GarageArea\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasbsmt\u0026#39;] = (df[\u0026#39;TotalBsmtSF\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasfireplace\u0026#39;] = (df[\u0026#39;Fireplaces\u0026#39;] \u0026gt; 0).astype(int) return df custom_feature_step = FunctionTransformer(add_custom_features) Mutual Information From my experiment, this seems to be the most useful tool.\n1 2 3 4 from sklearn.feature_selection import SelectKBest, mutual_info_regression # Select top k features based on MI mi_selector = SelectKBest(score_func=mutual_info_regression, k=50) # or \u0026#39;k=\u0026#34;all\u0026#34;\u0026#39; to get scores PCA This is not used in the end.\n1 2 from sklearn.decomposition import PCA pca = PCA(n_components=50) # You can tune this Model-Based Feature Selection Again, this is not used in the end. Mutual information already turns out to be very effective.\n1 2 3 4 5 from sklearn.feature_selection import SelectFromModel from xgboost import XGBRegressor # Use a fitted model to select features with importance above threshold model_selector = SelectFromModel(XGBRegressor(n_estimators=100), threshold=\u0026#34;median\u0026#34;) Model Evaluation 1 2 3 4 5 6 7 8 9 10 from sklearn.pipeline import make_pipeline from sklearn.feature_selection import SelectKBest, mutual_info_regression pipeline = Pipeline([ (\u0026#34;custom_features\u0026#34;, custom_feature_step), (\u0026#34;encoder\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;feature_select\u0026#34;, SelectKBest(score_func=mutual_info_regression, k=60)), (\u0026#34;model\u0026#34;, XGBRegressor()) ]) 1 2 3 4 # Then use it with cross_val_score score = cross_val_score(pipeline, X, y_log, cv=5, scoring=rmsle_scorer) print(f\u0026#34;RMSLE scores: {-score}\u0026#34;) # Negate to get actual RMSLE values print(f\u0026#34;Mean RMSLE: {-score.mean():.5f}\u0026#34;) RMSLE scores: [0.13717768 0.15245099 0.1511411 0.12531991 0.13995213] Mean RMSLE: 0.14121 We can see a slight improvement compared to the baseline model above.\nHyperparameter Tuning and Final Predictions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # import optuna # from sklearn.model_selection import cross_val_score # def objective(trial): # xgb_params = dict( # max_depth=trial.suggest_int(\u0026#34;max_depth\u0026#34;, 2, 10), # learning_rate=trial.suggest_float(\u0026#34;learning_rate\u0026#34;, 1e-4, 1e-1, log=True), # n_estimators=trial.suggest_int(\u0026#34;n_estimators\u0026#34;, 1000, 8000), # min_child_weight=trial.suggest_int(\u0026#34;min_child_weight\u0026#34;, 1, 10), # colsample_bytree=trial.suggest_float(\u0026#34;colsample_bytree\u0026#34;, 0.2, 1.0), # subsample=trial.suggest_float(\u0026#34;subsample\u0026#34;, 0.2, 1.0), # reg_alpha=trial.suggest_float(\u0026#34;reg_alpha\u0026#34;, 1e-4, 1e2, log=True), # reg_lambda=trial.suggest_float(\u0026#34;reg_lambda\u0026#34;, 1e-4, 1e2, log=True), # ) # pipeline.set_params(**{f\u0026#34;model__{key}\u0026#34;: val for key, val in xgb_params.items()}) # score = cross_val_score(pipeline, X, y_log, cv=5, scoring=rmsle_scorer) # return -score.mean() # Minimize RMSLE # # Run Optuna optimization # study = optuna.create_study(direction=\u0026#34;minimize\u0026#34;) # study.optimize(objective, n_trials=10) 1 2 3 4 5 6 7 8 9 # This is the set of parameters for my submission xgb_params = {\u0026#39;max_depth\u0026#39;: 7, \u0026#39;learning_rate\u0026#39;: 0.004565565417769295, \u0026#39;n_estimators\u0026#39;: 4701, \u0026#39;min_child_weight\u0026#39;: 9, \u0026#39;colsample_bytree\u0026#39;: 0.5911157102802619, \u0026#39;subsample\u0026#39;: 0.265969852467484, \u0026#39;reg_alpha\u0026#39;: 0.030977607695995966, \u0026#39;reg_lambda\u0026#39;: 0.168357167207514} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from sklearn.metrics import mean_squared_log_error # Train the pipeline on the entire training set X = df_train.copy() y = X.pop(\u0026#39;SalePrice\u0026#39;) y_log = np.log(y) # Initialize the model pipeline = Pipeline([ (\u0026#34;custom_features\u0026#34;, custom_feature_step), (\u0026#34;encoder\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;feature_select\u0026#34;, SelectKBest(score_func=mutual_info_regression, k=60)), (\u0026#34;model\u0026#34;, XGBRegressor()) ]) # Properly prefix parameter names with \u0026#34;model__\u0026#34; # best_params_prefixed = {f\u0026#34;model__{key}\u0026#34;: val for key, val in study.best_params.items()} best_params_prefixed = {f\u0026#34;model__{key}\u0026#34;: val for key, val in xgb_params.items()} # Set the best parameters to the pipeline pipeline.set_params(**best_params_prefixed) # Train the model pipeline.fit(X, y_log) 1 2 3 # Predict on the test set test_predictions = pipeline.predict(df_test) predictions = np.exp(test_predictions) 1 2 3 # Save the final result output = pd.DataFrame({\u0026#39;Id\u0026#39;: df_test.index, \u0026#39;SalePrice\u0026#39;: predictions}) output.to_csv(\u0026#39;my_submission.csv\u0026#39;, index=False) ","permalink":"http://localhost:1313/projects/housing-price-prediction-with-xgboost/","summary":"\u003cp\u003eThis is my \u003ca href=\"https://www.kaggle.com/code/houmingyi/housing-price-prediction-with-xgboost\"\u003eKaggle notebook\u003c/a\u003e for the \u003ca href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\"\u003eHouse Prices - Advanced Regression Techniques\u003c/a\u003e competition on Kaggle. The goal of this competition is to predict the sale price of homes in Ames, Iowa, based on various features of the homes.\u003c/p\u003e\n\u003cp\u003eI will be using various techniques learned in the course to predict the sale price of homes. The techniques include: preprocessing the data, feature engineering, and using different machine learning models to make predictions. I will also be using cross-validation to evaluate the performance of the models and to avoid overfitting.\u003c/p\u003e","title":"Housing Price Prediction with XGBoost"},{"content":"This project implements a Monte Carlo simulation for the Fokker-Planck equation using Metal, a framework for GPU programming on Apple devices. The simulation is designed to model the diffusion of particles in a potential field, which is common in statistical physics and machine learning.\nThe codes are written in Julia and utilize the Metal.jl package to leverage the GPU for parallel computation. You can find the repository for this project here.\nKramers-Fokker-Planck Equation The Kramers-Fokker-Planck equation is the generator of stochastic process modeling a particle\u0026rsquo;s movement in a potential field. The stationary solution of the Fokker-Planck equation is the equilibrium distribution of the particle\u0026rsquo;s position.\n$$ \\begin{cases} \\rm{d}X_t = V_t \\rm{d}t,\\newline \\rm{d}V_t = \\rm{d}B_t, \\end{cases} $$$$\\cal{L} = \\frac{1}{2}\\frac{\\partial^2}{\\partial v^2} + v\\frac{\\partial}{\\partial x}.$$We consider the process in the box $Q = (-1, 1)\\times(-1, 1)$, and simulate the process starting at a point $z_0 = (x_0, v_0)\\in Q$ until it hits the boundary of the box.\nThe Monte Carlo simulation involves:\nA Metal kernel function that updates the position of the particle according to the discretized SDE, namely $$ x_{n+1} = x_n + v_n \\Delta t, \\quad v_{n+1} = v_n + \\sqrt{\\Delta t} \\xi_n,$$ where $\\xi_n \\sim \\mathcal{N}(0, 1)$ is the standard normal random variable. Since the Metal kernel does not support random number generation, we also write a simple GPU friendly random number generator based on xorshift32 and Box-Muller method. The Kernel function does not support branching, the iteration will be fixed steps, and we use mask to stop the iteration when the particle hits the boundary. We simulate the process for a large number of particles and plot the harmonic measure on the boundary of the annulus. Features of the Project GPU friendly random number generator The random number generator is based on the xorshift32 algorithm, which is a simple and efficient pseudo-random number generator. The Box-Muller transform is used to generate normally distributed random numbers from uniformly distributed random numbers.\nGiven a seed ranged from 0 to 2^32-1, the xorshift32 algorithm generates a new seed by performing bitwise operations on the current seed.\n1 2 3 4 5 6 function xorshift32(seed::UInt32)::UInt32 seed ⊻= (seed \u0026lt;\u0026lt; 13) seed ⊻= (seed \u0026gt;\u0026gt; 17) seed ⊻= (seed \u0026lt;\u0026lt; 5) return seed end Then we transform this seed to a float number in the range of (0, 1) using the following function:\n1 2 3 4 function xorshift32_float(seed::UInt32)::Float32 value = Float32(xorshift32(seed)) * 2.3283064f-10 # Scale to [0,1) return max(value, 1.0f-16) # Ensure it\u0026#39;s in (0,1) end Finally, we use the Box-Muller transform to generate normally distributed random numbers:\n1 2 3 4 5 function box_muller(u1::Float32, u2::Float32) r = sqrt(-2.0f0 * log(u1)) theta = 2.0f0 * Float32(pi) * u2 return r * cos(theta) end Masks to avoid branching The Metal.jl kernel does not support branching, so we need to avoid using if statements in the kernel code. Instead, we use masks to control the flow of the simulation. The core update function for the problem in the cube $Q$ is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 for step in 1:num_steps # Boolean masks for exit conditions mask_x = (x \u0026lt; -1.0f0 || x \u0026gt; 1.0f0) ? 1 : 0 mask_v = (v \u0026lt; -1.0f0 || v \u0026gt; 1.0f0) ? 1 : 0 mask_exit = mask_x | mask_v # Combine masks (exit if either condition is true) continue_mask = 1 - mask_exit # 1 = active, 0 = exited # Generate two uniform distributed random numbers seed1 = xorshift32(seed1) seed2 = xorshift32(seed2) random_number1 = xorshift32_float(seed1) random_number2 = xorshift32_float(seed2) # Generate a normal distributed noise noise = box_muller(random_number1, random_number2) # Perturb the seeds to avoid deterministic patterns seed1 += UInt32(i) seed2 += UInt32(i) # Update position and velocity and store previous state if not exit x_prev, v_prev = continue_mask * x + mask_exit * x_prev, continue_mask * v + mask_exit * v_prev x += continue_mask * (v * time_step) v += continue_mask * (sqrt(time_step) * noise) end The mask_exit variable is used to check if the particle has exited the box. If it has, we set the continue_mask to 0, which effectively stops the simulation for that particle. The x_prev and v_prev variables are used to store the previous state of the particle before it exited.\nExample plots Consider the following Dirichlet boundary condition: Our codes can simulate the solution efficiently. The following plot shows the full solution and also a zoomed-in view of the solution near the singular boundary: In addition, we can plot the exit points distribution on the boundary for a starting point. The following is an example in the annulus: ","permalink":"http://localhost:1313/projects/monte-carlo-kfp-metal/","summary":"\u003cp\u003eThis project implements a Monte Carlo simulation for the Fokker-Planck equation using \u003ccode\u003eMetal\u003c/code\u003e, a framework for GPU programming on Apple devices. The simulation is designed to model the diffusion of particles in a potential field, which is common in statistical physics and machine learning.\u003c/p\u003e\n\u003cp\u003eThe codes are written in \u003ccode\u003eJulia\u003c/code\u003e and utilize the \u003ccode\u003eMetal.jl\u003c/code\u003e package to leverage the GPU for parallel computation. You can find the repository for this project \u003ca href=\"https://github.com/mingyi-ai/Monte_Carlo_KFP\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"Monte Carlo Simulation for Fokker-Planck Equations using Metal"},{"content":"In this post, we will explore the numerics of Hermite polynomials, as part of a numerical solver project for Kramers hyperbolic system. In particular, we try to compute eigenvectors numerically for the Hermite generating matrix, which is a tridiagonal matrix given by the recurrence relation\n$$ x \\mathrm{H}_{n}(x) = \\sqrt{n+1} \\mathrm{H}_{n+1}(x) - \\sqrt{n} \\mathrm{H}_{n-1}(x) $$for \\( n \\geq 1 \\) with \\(\\mathrm{H}_{n}(x)\\) being the \\(n\\)-th normalized Hermite polynomial. The generating matrix is given by\n$$ \\mathrm{T}_n = \\begin{pmatrix} 0 \u0026 \\sqrt{1} \u0026 0 \u0026 \\cdots \u0026 \\cdots \\\\ \\sqrt{1} \u0026 0 \u0026 \\sqrt{2} \u0026 \\cdots \u0026 \\cdots \\\\ 0 \u0026 \\sqrt{2} \u0026 \\ddots \u0026 \\cdots \u0026 \\cdots \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 0 \u0026 \\sqrt{n-1} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\sqrt{n-1} \u0026 0 \\end{pmatrix}. $$All the codes below are implemented in Julia.\nTheoretical eigenvalues and eigenvectors Recall that the Hermite polynomials are defined by\n$$ \\mathrm{He}_n(x) = (-1)^n \\exp(x^2/2) \\frac{\\mathrm{d}^n}{\\mathrm{d}x^n} \\left( \\exp(-x^2/2) \\right) $$for $n \\geq 0$. The first few Hermite polynomials are given by\n$$ \\mathrm{He}_0(x) = 1, \\\\ \\mathrm{He}_1(x) = x, \\\\ \\mathrm{He}_2(x) = x^2 - 1, \\\\ \\mathrm{He}_3(x) = x^3 - 3x. $$The Hermite polynomials can be computed using the recurrence relation\n$$ \\mathrm{He}_{n+1}(x) = x \\mathrm{He}_{n}(x) - n \\mathrm{He}_{n-1}(x) $$for $n \\geq 2$ with $\\mathrm{He}_0(x) = 1$ and $\\mathrm{He}_1(x) = x$.\nThe Hermite polynomials are orthogonal with respect to the (normalized) weight function $w(x) = \\exp(-x^2/2)/\\sqrt{2\\pi}$ on the interval $(-\\infty, \\infty)$. The $\\mathrm{L}^2$ norm of $\\mathrm{He}_n(x)$ with respect to this weight function is given by\n$$ ||\\mathrm{He}_n||_{\\mathrm{L}^2} = \\sqrt{n!} $$ for $n \\geq 0$. Let\u0026rsquo;s denote the normalized Hermite polynomials by\n$$ \\mathrm{H}_n = \\frac{\\mathrm{He}_n}{||\\mathrm{He}_n||_{\\mathrm{L}^2}} = \\frac{\\mathrm{He}_n}{\\sqrt{n!}}. $$It is well-known that the eigenvalues of the Hermite generating matrix $\\mathrm{T}_n$ of size $n$ are given by the roots of the Hermite polynomial $\\mathrm{H}_n(x)$, and the eigenvectors are given by the values of the Hermite polynomials at the roots, namely\n$$ \\mathrm{T}_n \\mathbf{v}_{n,k} = \\lambda_{n,k} \\mathbf{v}_{n,k} $$ where $\\lambda_{n,k}$ is the $k$-th root of $\\mathrm{He}_n(x)$, and\n$$ \\mathbf{v}_{n,k} = \\begin{pmatrix} \\mathrm{H}_0(\\lambda_{n,k}) \\\\ \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\cdots \\\\ \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\end{pmatrix} $$ is the corresponding eigenvector.\nTo see this, we do a direct computation\n$$ \\mathrm{T}_n \\mathbf{v}_{n,k} = \\begin{pmatrix} \\sqrt{1} \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\sqrt{1} \\mathrm{H}_0(\\lambda_{n,k}) + \\sqrt{2} \\mathrm{H}_2(\\lambda_{n,k}) \\\\ \\cdots\\\\ \\sqrt{n-2} \\mathrm{H}_{n-3}(\\lambda_{n,k}) + \\sqrt{n-1} \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\\\ \\sqrt{n-1} \\mathrm{H}_{n-2}(\\lambda_{n,k}) + \\sqrt{n} \\underbrace{\\mathrm{H}_{n}(\\lambda_{n,k})}_{=0} \\end{pmatrix} = \\begin{pmatrix} \\lambda_{n,k} \\mathrm{H}_0(\\lambda_{n,k}) \\\\ \\lambda_{n,k} \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\cdots \\\\ \\lambda_{n,k} \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\end{pmatrix} = \\lambda_{n,k} \\mathbf{v}_{n,k}. $$Let\u0026rsquo;s define $\\mathbf{w}_{n,k} = {\\mathbf{v}_{n,k}}/{||\\mathbf{v}_{n,k}||_2}$, which is the normalized eigenvector, and\n$$ \\mathrm{P}_n := \\begin{pmatrix} \\mathbf{w}_{n,1} \u0026 \\mathbf{w}_{n,2} \u0026 \\cdots \u0026 \\mathbf{w}_{n,n} \\end{pmatrix} $$is the change of basis matrix. It follows that\n$$ \\mathrm{P}_n^T \\mathrm{T}_n \\mathrm{P}_n = \\mathrm{D}_n:= \\mathrm{diag}(\\lambda_{n,1}, \\lambda_{n,1}, \\cdots, \\lambda_{n,n}). $$Thus, in theory, we can compute the eigenvalues and eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ by computing the roots of the normalized Hermite polynomial $\\mathrm{H}_n(x)$ and evaluating the normalized Hermite polynomials at these roots.\nNumerical properties of the Hermite polynomials Numerical instability when evaluating the Hermite polynomials The first thing we need to note is that the Hermite polynomials\u0026rsquo; coefficients grow very quickly as $n$ increases. Very large and very small coefficients exist in the polynomial, which can lead to numerical instability when evaluating the polynomial. For example,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 using Polynomials function He_symbolic(n::Int) He_0 = Polynomial([1.0]) if n == 0 return He_0 end He_1 = Polynomial([0.0, 1.0]) if n == 1 return He_1 end for k in 2:n He_2 = Polynomial([0.0, 1.0]) * He_1 - (k - 1) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end 1 He_symbolic(15) -2.027025e6∙x + 4.729725e6∙x3 - 2.837835e6∙x5 + 675675.0∙x7 - 75075.0∙x9 + 4095.0∙x11 - 105.0∙x13 + 1.0∙x15\nFor $n=15$, the largest coefficient is around $10^{6}$, while the smallest is always $1.0$, this could lead to numerical instability when evaluating the polynomial directly. To solve this problem, we can use the recurrence relation to evaluate the polynomial, which should give us a more stable result for small $n$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function He(n::Int, x::Float64) He_0 = 1.0 if n == 0 return He_0 end He_1 = x for k in 2:n He_2 = x * He_1 - (k - 1) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end 1 2 3 4 5 6 x = 1.1 n = 51 val_direct = He_symbolic(n)(x) val_recursive = He(n, x) val_direct, val_recursive (-5.624972601518976e32, -5.624972601518795e32) We can see relatively small but noticeable discrepancies between the two methods, since Polynomials.jl already uses numerically stable methods to evaluate a polynomial. For our purposes, we use the recursive evaluation.\nNumerical instability when computing the roots of the Hermite polynomials The second thing we need to note is that the roots of the Hermite polynomials are very close to each other, especially for large $n$. This can lead to numerical instability when computing the roots of the polynomial. For example, for $n=15$, the roots are\n1 println(sort(roots(He_symbolic(15)))) [-6.36394788882981, -5.190093591304892, -4.196207711268877, -3.2890824243988406, -2.4324368270097385, -1.6067100690287315, -0.7991290683245481, 0.0, 0.7991290683245479, 1.6067100690287317, 2.4324368270097474, 3.2890824243988055, 4.196207711268944, 5.190093591304826, 6.36394788882981] As we can see, roots are clustered around $x=0$, and does not become more spaced out as $n$ increases. To see this, we can plot the roots of the Hermite polynomials for $n=1, \\dots, 20$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 using Plots # Collect roots for n=1:20 root_data = [] for n in 1:20 push!(root_data, roots(He_symbolic(n))) end # Flatten the root data and associate each root with its corresponding n x_vals = vcat(root_data...) y_vals = vcat([fill(n, length(root_data[n])) for n in 1:20]...) # Plot the roots distribution scatter(x_vals, y_vals, xlabel=\u0026#34;Roots\u0026#34;, ylabel=\u0026#34;n\u0026#34;, title=\u0026#34;Roots Distribution of Hermite polynomials for n=1:20\u0026#34;, legend=false) To efficiently compute the roots of the Hermite polynomials, we can use the generating matrix $\\mathrm{T}_n$ and compute the eigenvalues of the matrix. This is a numerically stable method as the matrix is tridiagonal, symmetric, and there exists numerically stable algorithms to compute the eigenvalues and eigenvectors of such matrices.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 function T(n::Int) d = zeros(n) e = sqrt.(1:n-1) T = SymTridiagonal(d, e) return T end using LinearAlgebra function get_eigen(n::Int) Tn = T(n) F = eigen(Tn) # Eigenvalues and eigenvectors idx = sortperm(F.values) # Sort eigenvalues and eigenvectors eigenvals = F.values[idx] # Sorted eigenvalues (roots) eigenvecs = F.vectors[:, idx] # Sorted eigenvectors # One step of Newton\u0026#39;s method to refine the roots He_nx = He.(n, eigenvals) # Hermite polynomial at the roots He_nm1x = He.(n-1, eigenvals) # Candidate for the derivative eigenvals .-= He_nx ./ (n .* He_nm1x) return eigenvals, eigenvecs end get_eigen (generic function with 1 method) 1 2 3 eigenvals, _ = get_eigen(5) println(\u0026#34;Eigenvalues for n=5: \u0026#34;, eigenvals) println(sort(roots(He_symbolic(5)))) Eigenvalues for n=5: [-2.8569700138728056, -1.355626179974266, 0.0, 1.355626179974266, 2.8569700138728056] [-2.856970013872804, -1.3556261799742675, 0.0, 1.3556261799742657, 2.856970013872808] From the above comparison, we can see that the eigenvalues computed from the generating matrix preserve the symmetry of the roots better. As $n$ increases, this algorithm becomes more efficient than the root-finding algorithm, as the roots become more clustered together.\nNormalized eigenvectors of the Hermite generating matrix Now we are in a position to compute the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$. Note that we already computed the eigenvalues and eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ in the previous section using a numerically robust method. In this section, we will construct the normalized eigenvectors of the Hermite generating matrix by evaluating the normalized Hermite polynomials at the roots of the Hermite polynomial, i.e. $\\mathbf{v}_{n,k}$ for $k=1, \\dots, n$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 using SpecialFunctions: logfactorial # An auxiliary function to compute the normalization constant function He_l2norm(n::Int) return sqrt(exp(logfactorial(n))) end function analytic_eigenvecs(n::Int) Pmat = zeros(Float64, n, n) # initializing the matrix rts, _ = get_eigen(n) # Get the roots for i in 1:n root_i = rts[i] for j in 1:n Pmat[j, i] = He(j-1, root_i) / He_l2norm(j-1) end # Normalize each vector (column) Pmat[:, i] /= norm(Pmat[:, i]) end return Pmat end Numerical properties of the eigenvectors Now, we have two ways to compute the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$: one is theoretically precise, the other is numerical. We will compare the two methods and see how well they agree with each other.\nThere are two important properties of the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ that we need to note:\nThe column matrix $\\mathrm{P}_n$ of eigenvectors is orthonormal, i.e. $\\mathrm{P}_n^T \\mathrm{P}_n = \\mathrm{I}_n$. By conjugating the eigenvectors, we can obtain the eigenvalues of the Hermite generating matrix $\\mathrm{T}_n$, which is a diagonal matrix $\\mathrm{D}_n$ with the roots of the Hermite polynomial on the diagonal. $$ \\mathrm{P}_n^T \\mathrm{T}_n \\mathrm{P}_n = \\mathrm{D}_n. $$ We will check how well these properties hold for both methods of computing the eigenvectors. The following codes compute the $\\infty$ norm of the above two properties.\n1 2 3 4 5 6 7 function orthonormality_error(P::Matrix{Float64}) return norm(P\u0026#39; * P - I, Inf) end function diagonal_error(P::Matrix{Float64}, eigenvals::Vector{Float64}, n::Int) return norm(P\u0026#39; * T(n) * P - Diagonal(eigenvals), Inf) end Show Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Define the range of n n_values = 2:200 # Initialize error arrays for both methods analytic_orth_errors = Float64[] analytic_diag_errors = Float64[] numerical_orth_errors = Float64[] numerical_diag_errors = Float64[] # Compute errors for both methods for n in n_values P_analytic = analytic_eigenvecs(n) rts, P_numerical = get_eigen(n) push!(analytic_orth_errors, orthonormality_error(P_analytic)) push!(analytic_diag_errors, diagonal_error(P_analytic, rts, n)) push!(numerical_orth_errors, orthonormality_error(P_numerical)) push!(numerical_diag_errors, diagonal_error(P_numerical, rts, n)) end # Create 2x2 subplots p1 = plot(n_values, log10.(analytic_orth_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Orthonormality Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p1, n_values, log10.(numerical_orth_errors), label=\u0026#34;Numerical\u0026#34;) p2 = plot(n_values, log10.(analytic_diag_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Diagonalization Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p2, n_values, log10.(numerical_diag_errors), label=\u0026#34;Numerical\u0026#34;) plot(p1, p2, layout=(2, 1)) From the plot we can see that the analytical method is marginally better than the numerical method before $n\\approx 170$, but both errors for analytical method blow up after $n\\approx 170$, while the numerical method remains stable. The reason for this is that in the analytical method, we need to compute the normalization factor of the Hermite polynomial, which is given by $||\\mathrm{He}_n||_{\\mathrm{L}^2} = \\sqrt{n!}$, and the factorial grows very quickly, eventually blows up at $n\\approx 170$.\n1 2 3 for n in 169:172 print(He_l2norm(n), \u0026#34;, \u0026#34;) end 2.0661723086434517e152, 2.6939590968143674e153, Inf, Inf, To solve this problem, we can use the recurrence relation to compute the normalized Hermite polynomials as before, which avoids the need to compute the normalization factor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 function Henorm(n::Int, x::Float64) He_0 = 1.0 if n == 0 return He_0 end He_1 = x for k in 2:n He_2 = x * He_1 / sqrt(k) - sqrt((k - 1) / k) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end function analytic_eigenvecs_norm(n::Int) Pmat = zeros(Float64, n, n) # initializing the matrix rts, _ = get_eigen(n) # Get the roots for i in 1:n root_i = rts[i] for j in 1:n Pmat[j, i] = Henorm(j-1, root_i) end # Normalize each vector (column) Pmat[:, i] /= norm(Pmat[:, i]) end return Pmat end Show Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Define the range of n n_values = 2:200 # Initialize error arrays for both methods analytic_orth_errors = Float64[] analytic_diag_errors = Float64[] numerical_orth_errors = Float64[] numerical_diag_errors = Float64[] # Compute errors for both methods for n in n_values P_analytic = analytic_eigenvecs_norm(n) rts, P_numerical = get_eigen(n) push!(analytic_orth_errors, orthonormality_error(P_analytic)) push!(analytic_diag_errors, diagonal_error(P_analytic, rts, n)) push!(numerical_orth_errors, orthonormality_error(P_numerical)) push!(numerical_diag_errors, diagonal_error(P_numerical, rts, n)) end # Create 2x2 subplots p1 = plot(n_values, log10.(analytic_orth_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Orthonormality Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p1, n_values, log10.(numerical_orth_errors), label=\u0026#34;Numerical\u0026#34;) p2 = plot(n_values, log10.(analytic_diag_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Diagonalization Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p2, n_values, log10.(numerical_diag_errors), label=\u0026#34;Numerical\u0026#34;) plot(p1, p2, layout=(2, 1)) After the above changes, we can see that the analytical method is now more stable than the numerical method, and the errors are much smaller. However, in practice, using large $n$ for the Hermite polynomials is not recommended, as there are already numerical instabilities when evaluating the Hermite polynomials.\nDownload the notebook 📥 Download Notebook\n","permalink":"http://localhost:1313/posts/25-05-02_hermite-numerics/","summary":"\u003cp\u003eIn this post, we will explore the numerics of Hermite polynomials, as part of a numerical solver project for Kramers hyperbolic system. In particular, we try to compute eigenvectors numerically for the Hermite generating matrix, which is a tridiagonal matrix given by the recurrence relation\u003c/p\u003e\n$$\nx \\mathrm{H}_{n}(x) = \\sqrt{n+1} \\mathrm{H}_{n+1}(x) - \\sqrt{n} \\mathrm{H}_{n-1}(x)\n$$\u003cp\u003efor \\( n \\geq 1 \\) with \\(\\mathrm{H}_{n}(x)\\) being the \\(n\\)-th normalized Hermite polynomial.\nThe generating matrix is given by\u003c/p\u003e","title":"Numerical Methods for Hermite Polynomials"},{"content":"I am a PhD student in Mathematics at Uppsala University. My research interests lie in the intersection of probability theory, partial differential equations, and machine learning. I am particularly interested in the mathematical analysis of stochastic processes and their applications in machine learning.\nResearch Publications Preprints M. Hou. Variational Formulation and Capacity Estimates for Non-Self-Adjoint Fokker-Planck Operators in Divergence Form. arXiv preprint, 2025. M. Hou. Boundedness of Weak Solutions to Degenerate Kolmogorov Equations of Hypoelliptic Type in Bounded Domains. arXiv preprint, 2024. B. Avelin, and M. Hou. Weak and Perron\u0026rsquo;s Solutions for Stationary Kramers-Fokker-Planck Equations in Bounded Domains. arXiv preprint, 2024. Published B. Avelin, M. Hou, and K. Nyström. A Galerkin Type Method for Kinetic Fokker-Planck Equations Based on Hermite Expansions. Kinet. Relat. Models, 2024. Thesis M. Hou. Behind the Training Dynamics of Neural Networks: Analysis of Fokker-Planck Equations and the Path to Metastability. PhD dissertation, Uppsala University, 2025. Contact Department of Mathematics\nUppsala University\n751 06 Uppsala\nSweden\nmingyi.hou@math.uu.se LinkedIn GitHub ","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003eI am a PhD student in Mathematics at Uppsala University. My research interests lie in the intersection of probability theory, partial differential equations, and machine learning. I am particularly interested in the mathematical analysis of stochastic processes and their applications in machine learning.\u003c/p\u003e\n\u003ch2 id=\"research-publications\"\u003eResearch Publications\u003c/h2\u003e\n\u003ch3 id=\"preprints\"\u003ePreprints\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eM. Hou. Variational Formulation and Capacity Estimates for Non-Self-Adjoint Fokker-Planck Operators in Divergence Form. \u003ca href=\"https://arxiv.org/abs/2502.12036\"\u003earXiv preprint, 2025\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eM. Hou. Boundedness of Weak Solutions to Degenerate Kolmogorov Equations of Hypoelliptic Type in Bounded Domains. \u003ca href=\"https://arxiv.org/abs/2407.00800\"\u003earXiv preprint, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eB. Avelin, and M. Hou. Weak and Perron\u0026rsquo;s Solutions for Stationary Kramers-Fokker-Planck Equations in Bounded Domains. \u003ca href=\"https://arxiv.org/abs/2405.04070\"\u003earXiv preprint, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"published\"\u003ePublished\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eB. Avelin, M. Hou, and K. Nyström. A Galerkin Type Method for Kinetic Fokker-Planck Equations Based on Hermite Expansions. \u003ca href=\"https://doi.org/10.3934/krm.2023035\"\u003eKinet. Relat. Models, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"thesis\"\u003eThesis\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eM. Hou. Behind the Training Dynamics of Neural Networks: Analysis of Fokker-Planck Equations and the Path to Metastability. \u003ca href=\"https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-553381\"\u003ePhD dissertation, Uppsala University, 2025.\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"contact\"\u003eContact\u003c/h2\u003e\n\u003cdiv style=\"display: flex; justify-content: space-between; flex-wrap: wrap;\"\u003e\n  \u003cdiv style=\"flex: 1; min-width: 200px;\"\u003e\n    \u003cp\u003eDepartment of Mathematics\u003cbr\u003e\n    Uppsala University\u003cbr\u003e\n    751 06 Uppsala\u003cbr\u003e\n    Sweden\u003c/p\u003e","title":"About Me"},{"content":"This is my Kaggle notebook for the House Prices - Advanced Regression Techniques competition on Kaggle. The goal of this competition is to predict the sale price of homes in Ames, Iowa, based on various features of the homes.\nI will be using various techniques learned in the course to predict the sale price of homes. The techniques include: preprocessing the data, feature engineering, and using different machine learning models to make predictions. I will also be using cross-validation to evaluate the performance of the models and to avoid overfitting.\nExploratory Data Analysis (EDA) Load Libraries \u0026amp; Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from pathlib import Path import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # Set plot styles sns.set(style=\u0026#34;whitegrid\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (12, 6) # Load data data_dir = Path(\u0026#34;./house-prices-advanced-regression-techniques\u0026#34;) df_train = pd.read_csv(data_dir / \u0026#34;train.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) df_test = pd.read_csv(data_dir / \u0026#34;test.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) print(f\u0026#34;Train shape: {df_train.shape}\u0026#34;) print(f\u0026#34;Test shape: {df_test.shape}\u0026#34;) Train shape: (1460, 80) Test shape: (1459, 79) Understand the Target Variable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Distribution of target sns.histplot(df_train[\u0026#34;SalePrice\u0026#34;], kde=True) plt.title(\u0026#34;SalePrice Distribution\u0026#34;) plt.xlabel(\u0026#34;SalePrice\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.savefig(\u0026#34;saleprice_distribution.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() # Log-transform target to check skew sns.histplot(np.log1p(df_train[\u0026#34;SalePrice\u0026#34;]), kde=True) plt.title(\u0026#34;Log-Transformed SalePrice\u0026#34;) plt.xlabel(\u0026#34;Log(SalePrice + 1)\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.savefig(\u0026#34;log_saleprice_distribution.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() # Summary stats df_train[\u0026#34;SalePrice\u0026#34;].describe() count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000 Name: SalePrice, dtype: float64 Overview of the Dataset 1 2 3 4 5 6 7 df = pd.concat([df_train, df_test], axis=0) # df.info() # df.describe() # Missing value heatmap import missingno as msno msno.matrix(df) \u0026lt;Axes: \u0026gt; Data Cleaning 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def clean_data(df): # Clean to_category = [\u0026#39;MSSubClass\u0026#39;, \u0026#39;MoSold\u0026#39;, \u0026#39;YrSold\u0026#39;, \u0026#39;GarageYrBlt\u0026#39;, \u0026#39;YearBuilt\u0026#39;, \u0026#39;YearRemodAdd\u0026#39;] for col in to_category: df[col] = df[col].astype(str) df[\u0026#39;Functional\u0026#39;] = df[\u0026#39;Functional\u0026#39;].fillna(\u0026#39;Typ\u0026#39;) df[\u0026#34;Electrical\u0026#34;] = df[\u0026#34;Electrical\u0026#34;].fillna(\u0026#39;SBrkr\u0026#39;) df[\u0026#34;KitchenQual\u0026#34;] = df[\u0026#34;KitchenQual\u0026#34;].fillna(\u0026#39;TA\u0026#39;) df[\u0026#34;Exterior1st\u0026#34;] = df[\u0026#34;Exterior1st\u0026#34;].fillna(df[\u0026#34;Exterior1st\u0026#34;].mode()[0]) df[\u0026#34;Exterior2nd\u0026#34;] = df[\u0026#34;Exterior2nd\u0026#34;].fillna(df[\u0026#34;Exterior2nd\u0026#34;].mode()[0]) df[\u0026#34;SaleType\u0026#34;] = df[\u0026#34;SaleType\u0026#34;].fillna(df[\u0026#34;SaleType\u0026#34;].mode()[0]) # Impute # Fill missing values in object columns with \u0026#34;None\u0026#34; objects = [] for i in df.columns: if df[i].dtype == object: objects.append(i) df.update(df[objects].fillna(\u0026#39;None\u0026#39;)) # Fill missing values in numeric columns with 0 numerics = [] for i in df.columns: if df[i].dtype != object: numerics.append(i) df.update(df[numerics].fillna(0)) return df 1 2 3 4 5 6 7 8 9 10 11 12 13 def load_data(): data_dir = Path(\u0026#34;./house-prices-advanced-regression-techniques\u0026#34;) df_train = pd.read_csv(data_dir / \u0026#34;train.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) df_test = pd.read_csv(data_dir / \u0026#34;test.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) # Clean data df = pd.concat([df_train, df_test], axis=0) df = clean_data(df) # Split back into train and test df_train = df.loc[df_train.index, :] df_test = df.loc[df_test.index, :].drop(columns=[\u0026#34;SalePrice\u0026#34;]) return df_train, df_test 1 2 3 4 5 6 7 df_train, df_test = load_data() # Check the cleaned data # train_missing = df_train.isnull().sum() # print(train_missing[train_missing \u0026gt; 0]) # test_missing = df_test.isnull().sum() # print(test_missing[test_missing \u0026gt; 0]) # df_train.info() Correlation with Target (Numerical Features) 1 2 3 4 5 6 7 8 9 10 11 # Compute correlation matrix corr_matrix = df_train.corr(numeric_only=True) # Get top 15 features correlated with SalePrice top_corr = corr_matrix[\u0026#34;SalePrice\u0026#34;].abs().sort_values(ascending=False).head(15) # Visualize sns.heatmap(df_train[top_corr.index].corr(), annot=True, cmap=\u0026#34;coolwarm\u0026#34;, fmt=\u0026#34;.2f\u0026#34;) plt.title(\u0026#34;Top Correlated Features with SalePrice\u0026#34;) plt.savefig(\u0026#34;top_correlated_features.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() Categorical Features Preview 1 2 3 4 5 6 7 8 9 10 11 12 13 categoricals = df_train.select_dtypes(include=\u0026#34;object\u0026#34;).columns print(f\u0026#34;Categorical features: {len(categoricals)}\u0026#34;) # print(categoricals.tolist()) # Example: Visualize average SalePrice by a few important categorical features important_cats = [\u0026#34;Neighborhood\u0026#34;, \u0026#34;ExterQual\u0026#34;, \u0026#34;GarageFinish\u0026#34;, \u0026#34;KitchenQual\u0026#34;] for col in important_cats: sns.boxplot(data=df_train, x=col, y=\u0026#34;SalePrice\u0026#34;) plt.title(f\u0026#34;SalePrice by {col}\u0026#34;) plt.xticks(rotation=45) plt.savefig(f\u0026#34;saleprice_by_{col}.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() Categorical features: 49 Time-Related Patterns 1 2 3 4 5 6 7 sns.boxplot(x=\u0026#34;YrSold\u0026#34;, y=\u0026#34;SalePrice\u0026#34;, data=df_train) plt.title(\u0026#34;SalePrice by Year Sold\u0026#34;) plt.show() sns.scatterplot(x=\u0026#34;YearBuilt\u0026#34;, y=\u0026#34;SalePrice\u0026#34;, data=df_train) plt.title(\u0026#34;SalePrice vs Year Built\u0026#34;) plt.show() It does not look like there is any time related feature.\nFeature Engineering 1 2 3 X = df_train.copy() y = X.pop(\u0026#34;SalePrice\u0026#34;) # X.info() Separate Categorical and Numerical Features 1 2 3 4 # cat_cols = X.select_dtypes(include=[\u0026#39;object\u0026#39;]).columns.tolist() # num_cols = X.select_dtypes(exclude=[\u0026#39;object\u0026#39;]).columns.tolist() # print(f\u0026#34;Categorical columns: {len(cat_cols)}\u0026#34;) # print(f\u0026#34;Numerical columns: {len(num_cols)}\u0026#34;) Encoding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from sklearn.base import BaseEstimator, TransformerMixin from category_encoders import TargetEncoder class Encoder(BaseEstimator, TransformerMixin): def __init__(self, ordered_levels): self.ordered_levels = ordered_levels self.low_cardinality = [] self.high_cardinality = [] self.te = None def fit(self, X, y): X = X.copy() self.ordered_levels = {k: [\u0026#34;None\u0026#34;] + v for k, v in self.ordered_levels.items()} for col, order in self.ordered_levels.items(): if col in X.columns: X[col] = pd.Categorical(X[col], categories=order, ordered=True).codes ordinal_cols = list(self.ordered_levels.keys()) nominal_cols = [col for col in X.select_dtypes(include=\u0026#39;object\u0026#39;).columns if col not in ordinal_cols] self.low_cardinality = [col for col in nominal_cols if X[col].nunique() \u0026lt;= 10] self.high_cardinality = [col for col in nominal_cols if X[col].nunique() \u0026gt; 10] for col in self.low_cardinality: X[col] = X[col].astype(\u0026#34;category\u0026#34;).cat.codes self.te = TargetEncoder() self.te.fit(X[self.high_cardinality], y) return self def transform(self, X): X = X.copy() for col, order in self.ordered_levels.items(): if col in X.columns: X[col] = pd.Categorical(X[col], categories=order, ordered=True).codes for col in self.low_cardinality: if col in X.columns: X[col] = X[col].astype(\u0026#34;category\u0026#34;).cat.codes if self.te: X[self.high_cardinality] = self.te.transform(X[self.high_cardinality]) return X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 five_levels = [\u0026#34;Po\u0026#34;, \u0026#34;Fa\u0026#34;, \u0026#34;TA\u0026#34;, \u0026#34;Gd\u0026#34;, \u0026#34;Ex\u0026#34;] ten_levels = list(range(1, 11)) # 1 - 10 is the correct range! ordered_levels = { \u0026#34;OverallQual\u0026#34;: ten_levels, \u0026#34;OverallCond\u0026#34;: ten_levels, \u0026#34;ExterQual\u0026#34;: five_levels, \u0026#34;ExterCond\u0026#34;: five_levels, \u0026#34;BsmtQual\u0026#34;: five_levels, \u0026#34;BsmtCond\u0026#34;: five_levels, \u0026#34;HeatingQC\u0026#34;: five_levels, \u0026#34;KitchenQual\u0026#34;: five_levels, \u0026#34;FireplaceQu\u0026#34;: five_levels, \u0026#34;GarageQual\u0026#34;: five_levels, \u0026#34;GarageCond\u0026#34;: five_levels, \u0026#34;PoolQC\u0026#34;: five_levels, \u0026#34;LotShape\u0026#34;: [\u0026#34;Reg\u0026#34;, \u0026#34;IR1\u0026#34;, \u0026#34;IR2\u0026#34;, \u0026#34;IR3\u0026#34;], \u0026#34;LandSlope\u0026#34;: [\u0026#34;Sev\u0026#34;, \u0026#34;Mod\u0026#34;, \u0026#34;Gtl\u0026#34;], \u0026#34;BsmtExposure\u0026#34;: [\u0026#34;No\u0026#34;, \u0026#34;Mn\u0026#34;, \u0026#34;Av\u0026#34;, \u0026#34;Gd\u0026#34;], \u0026#34;BsmtFinType1\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;LwQ\u0026#34;, \u0026#34;Rec\u0026#34;, \u0026#34;BLQ\u0026#34;, \u0026#34;ALQ\u0026#34;, \u0026#34;GLQ\u0026#34;], \u0026#34;BsmtFinType2\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;LwQ\u0026#34;, \u0026#34;Rec\u0026#34;, \u0026#34;BLQ\u0026#34;, \u0026#34;ALQ\u0026#34;, \u0026#34;GLQ\u0026#34;], \u0026#34;Functional\u0026#34;: [\u0026#34;Sal\u0026#34;, \u0026#34;Sev\u0026#34;, \u0026#34;Maj1\u0026#34;, \u0026#34;Maj2\u0026#34;, \u0026#34;Mod\u0026#34;, \u0026#34;Min2\u0026#34;, \u0026#34;Min1\u0026#34;, \u0026#34;Typ\u0026#34;], \u0026#34;GarageFinish\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;RFn\u0026#34;, \u0026#34;Fin\u0026#34;], \u0026#34;PavedDrive\u0026#34;: [\u0026#34;N\u0026#34;, \u0026#34;P\u0026#34;, \u0026#34;Y\u0026#34;], \u0026#34;Utilities\u0026#34;: [\u0026#34;NoSeWa\u0026#34;, \u0026#34;NoSewr\u0026#34;, \u0026#34;AllPub\u0026#34;], \u0026#34;CentralAir\u0026#34;: [\u0026#34;N\u0026#34;, \u0026#34;Y\u0026#34;], \u0026#34;Electrical\u0026#34;: [\u0026#34;Mix\u0026#34;, \u0026#34;FuseP\u0026#34;, \u0026#34;FuseF\u0026#34;, \u0026#34;FuseA\u0026#34;, \u0026#34;SBrkr\u0026#34;], \u0026#34;Fence\u0026#34;: [\u0026#34;MnWw\u0026#34;, \u0026#34;GdWo\u0026#34;, \u0026#34;MnPrv\u0026#34;, \u0026#34;GdPrv\u0026#34;], } 1 2 3 4 5 6 7 from sklearn.pipeline import Pipeline from xgboost import XGBRegressor basepipe = Pipeline([ (\u0026#34;encode\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;xgb\u0026#34;, XGBRegressor(random_state=42)) ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from sklearn.metrics import make_scorer, mean_squared_log_error from sklearn.model_selection import cross_val_score import numpy as np # Custom RMSLE scorer (greater_is_better=False because lower is better) rmsle_scorer = make_scorer( lambda y_true, y_pred: np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred))), greater_is_better=False ) y_log = np.log(y) # We train on the log transformed target # Then use it with cross_val_score score = cross_val_score(basepipe, X, y_log, cv=5, scoring=rmsle_scorer) print(f\u0026#34;RMSLE scores: {-score}\u0026#34;) # Negate to get actual RMSLE values print(f\u0026#34;Mean RMSLE: {-score.mean():.5f}\u0026#34;) RMSLE scores: [0.1397286 0.15149179 0.14633292 0.1268186 0.14509794] Mean RMSLE: 0.14189 Transform Skewed Numerical Features This transform is not used in the end.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class SkewedFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self, threshold=0.75): self.threshold = threshold self.skewed_cols = [] def fit(self, X, y=None): X = pd.DataFrame(X) skewness = X.skew().abs() self.skewed_cols = skewness[skewness \u0026gt; self.threshold].index.tolist() return self def transform(self, X): X = pd.DataFrame(X).copy() for col in self.skewed_cols: X[col] = np.log1p(X[col]) return X skew_transformer = SkewedFeatureTransformer() Add New Features 1 2 from sklearn.preprocessing import FunctionTransformer from sklearn.pipeline import Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def add_custom_features(df): df = df.copy() df[\u0026#39;TotalSF\u0026#39;] = df[\u0026#39;TotalBsmtSF\u0026#39;] + df[\u0026#39;1stFlrSF\u0026#39;] + df[\u0026#39;2ndFlrSF\u0026#39;] df[\u0026#39;Total_sqr_footage\u0026#39;] = df[\u0026#39;BsmtFinSF1\u0026#39;] + df[\u0026#39;BsmtFinSF2\u0026#39;] + df[\u0026#39;1stFlrSF\u0026#39;] + df[\u0026#39;2ndFlrSF\u0026#39;] df[\u0026#39;Total_Bathrooms\u0026#39;] = df[\u0026#39;FullBath\u0026#39;] + (0.5 * df[\u0026#39;HalfBath\u0026#39;]) + df[\u0026#39;BsmtFullBath\u0026#39;] + (0.5 * df[\u0026#39;BsmtHalfBath\u0026#39;]) df[\u0026#39;Total_porch_sf\u0026#39;] = df[\u0026#39;OpenPorchSF\u0026#39;] + df[\u0026#39;3SsnPorch\u0026#39;] + df[\u0026#39;EnclosedPorch\u0026#39;] + df[\u0026#39;ScreenPorch\u0026#39;] + df[\u0026#39;WoodDeckSF\u0026#39;] df[\u0026#39;haspool\u0026#39;] = (df[\u0026#39;PoolArea\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;has2ndfloor\u0026#39;] = (df[\u0026#39;2ndFlrSF\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasgarage\u0026#39;] = (df[\u0026#39;GarageArea\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasbsmt\u0026#39;] = (df[\u0026#39;TotalBsmtSF\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasfireplace\u0026#39;] = (df[\u0026#39;Fireplaces\u0026#39;] \u0026gt; 0).astype(int) return df custom_feature_step = FunctionTransformer(add_custom_features) Mutual Information From my experiment, this seems to be the most useful tool.\n1 2 3 4 from sklearn.feature_selection import SelectKBest, mutual_info_regression # Select top k features based on MI mi_selector = SelectKBest(score_func=mutual_info_regression, k=50) # or \u0026#39;k=\u0026#34;all\u0026#34;\u0026#39; to get scores PCA This is not used in the end.\n1 2 from sklearn.decomposition import PCA pca = PCA(n_components=50) # You can tune this Model-Based Feature Selection Again, this is not used in the end. Mutual information already turns out to be very effective.\n1 2 3 4 5 from sklearn.feature_selection import SelectFromModel from xgboost import XGBRegressor # Use a fitted model to select features with importance above threshold model_selector = SelectFromModel(XGBRegressor(n_estimators=100), threshold=\u0026#34;median\u0026#34;) Model Evaluation 1 2 3 4 5 6 7 8 9 10 from sklearn.pipeline import make_pipeline from sklearn.feature_selection import SelectKBest, mutual_info_regression pipeline = Pipeline([ (\u0026#34;custom_features\u0026#34;, custom_feature_step), (\u0026#34;encoder\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;feature_select\u0026#34;, SelectKBest(score_func=mutual_info_regression, k=60)), (\u0026#34;model\u0026#34;, XGBRegressor()) ]) 1 2 3 4 # Then use it with cross_val_score score = cross_val_score(pipeline, X, y_log, cv=5, scoring=rmsle_scorer) print(f\u0026#34;RMSLE scores: {-score}\u0026#34;) # Negate to get actual RMSLE values print(f\u0026#34;Mean RMSLE: {-score.mean():.5f}\u0026#34;) RMSLE scores: [0.13717768 0.15245099 0.1511411 0.12531991 0.13995213] Mean RMSLE: 0.14121 We can see a slight improvement compared to the baseline model above.\nHyperparameter Tuning and Final Predictions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # import optuna # from sklearn.model_selection import cross_val_score # def objective(trial): # xgb_params = dict( # max_depth=trial.suggest_int(\u0026#34;max_depth\u0026#34;, 2, 10), # learning_rate=trial.suggest_float(\u0026#34;learning_rate\u0026#34;, 1e-4, 1e-1, log=True), # n_estimators=trial.suggest_int(\u0026#34;n_estimators\u0026#34;, 1000, 8000), # min_child_weight=trial.suggest_int(\u0026#34;min_child_weight\u0026#34;, 1, 10), # colsample_bytree=trial.suggest_float(\u0026#34;colsample_bytree\u0026#34;, 0.2, 1.0), # subsample=trial.suggest_float(\u0026#34;subsample\u0026#34;, 0.2, 1.0), # reg_alpha=trial.suggest_float(\u0026#34;reg_alpha\u0026#34;, 1e-4, 1e2, log=True), # reg_lambda=trial.suggest_float(\u0026#34;reg_lambda\u0026#34;, 1e-4, 1e2, log=True), # ) # pipeline.set_params(**{f\u0026#34;model__{key}\u0026#34;: val for key, val in xgb_params.items()}) # score = cross_val_score(pipeline, X, y_log, cv=5, scoring=rmsle_scorer) # return -score.mean() # Minimize RMSLE # # Run Optuna optimization # study = optuna.create_study(direction=\u0026#34;minimize\u0026#34;) # study.optimize(objective, n_trials=10) 1 2 3 4 5 6 7 8 9 # This is the set of parameters for my submission xgb_params = {\u0026#39;max_depth\u0026#39;: 7, \u0026#39;learning_rate\u0026#39;: 0.004565565417769295, \u0026#39;n_estimators\u0026#39;: 4701, \u0026#39;min_child_weight\u0026#39;: 9, \u0026#39;colsample_bytree\u0026#39;: 0.5911157102802619, \u0026#39;subsample\u0026#39;: 0.265969852467484, \u0026#39;reg_alpha\u0026#39;: 0.030977607695995966, \u0026#39;reg_lambda\u0026#39;: 0.168357167207514} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from sklearn.metrics import mean_squared_log_error # Train the pipeline on the entire training set X = df_train.copy() y = X.pop(\u0026#39;SalePrice\u0026#39;) y_log = np.log(y) # Initialize the model pipeline = Pipeline([ (\u0026#34;custom_features\u0026#34;, custom_feature_step), (\u0026#34;encoder\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;feature_select\u0026#34;, SelectKBest(score_func=mutual_info_regression, k=60)), (\u0026#34;model\u0026#34;, XGBRegressor()) ]) # Properly prefix parameter names with \u0026#34;model__\u0026#34; # best_params_prefixed = {f\u0026#34;model__{key}\u0026#34;: val for key, val in study.best_params.items()} best_params_prefixed = {f\u0026#34;model__{key}\u0026#34;: val for key, val in xgb_params.items()} # Set the best parameters to the pipeline pipeline.set_params(**best_params_prefixed) # Train the model pipeline.fit(X, y_log) 1 2 3 # Predict on the test set test_predictions = pipeline.predict(df_test) predictions = np.exp(test_predictions) 1 2 3 # Save the final result output = pd.DataFrame({\u0026#39;Id\u0026#39;: df_test.index, \u0026#39;SalePrice\u0026#39;: predictions}) output.to_csv(\u0026#39;my_submission.csv\u0026#39;, index=False) ","permalink":"http://localhost:1313/projects/housing-price-prediction-with-xgboost/","summary":"\u003cp\u003eThis is my \u003ca href=\"https://www.kaggle.com/code/houmingyi/housing-price-prediction-with-xgboost\"\u003eKaggle notebook\u003c/a\u003e for the \u003ca href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\"\u003eHouse Prices - Advanced Regression Techniques\u003c/a\u003e competition on Kaggle. The goal of this competition is to predict the sale price of homes in Ames, Iowa, based on various features of the homes.\u003c/p\u003e\n\u003cp\u003eI will be using various techniques learned in the course to predict the sale price of homes. The techniques include: preprocessing the data, feature engineering, and using different machine learning models to make predictions. I will also be using cross-validation to evaluate the performance of the models and to avoid overfitting.\u003c/p\u003e","title":"Housing Price Prediction with XGBoost"},{"content":"This project implements a Monte Carlo simulation for the Fokker-Planck equation using Metal, a framework for GPU programming on Apple devices. The simulation is designed to model the diffusion of particles in a potential field, which is common in statistical physics and machine learning.\nThe codes are written in Julia and utilize the Metal.jl package to leverage the GPU for parallel computation. You can find the repository for this project here.\nKramers-Fokker-Planck Equation The Kramers-Fokker-Planck equation is the generator of stochastic process modeling a particle\u0026rsquo;s movement in a potential field. The stationary solution of the Fokker-Planck equation is the equilibrium distribution of the particle\u0026rsquo;s position.\n$$ \\begin{cases} \\rm{d}X_t = V_t \\rm{d}t,\\newline \\rm{d}V_t = \\rm{d}B_t, \\end{cases} $$$$\\cal{L} = \\frac{1}{2}\\frac{\\partial^2}{\\partial v^2} + v\\frac{\\partial}{\\partial x}.$$We consider the process in the box $Q = (-1, 1)\\times(-1, 1)$, and simulate the process starting at a point $z_0 = (x_0, v_0)\\in Q$ until it hits the boundary of the box.\nThe Monte Carlo simulation involves:\nA Metal kernel function that updates the position of the particle according to the discretized SDE, namely $$ x_{n+1} = x_n + v_n \\Delta t, \\quad v_{n+1} = v_n + \\sqrt{\\Delta t} \\xi_n,$$ where $\\xi_n \\sim \\mathcal{N}(0, 1)$ is the standard normal random variable. Since the Metal kernel does not support random number generation, we also write a simple GPU friendly random number generator based on xorshift32 and Box-Muller method. The Kernel function does not support branching, the iteration will be fixed steps, and we use mask to stop the iteration when the particle hits the boundary. We simulate the process for a large number of particles and plot the harmonic measure on the boundary of the annulus. Features of the Project GPU friendly random number generator The random number generator is based on the xorshift32 algorithm, which is a simple and efficient pseudo-random number generator. The Box-Muller transform is used to generate normally distributed random numbers from uniformly distributed random numbers.\nGiven a seed ranged from 0 to 2^32-1, the xorshift32 algorithm generates a new seed by performing bitwise operations on the current seed.\n1 2 3 4 5 6 function xorshift32(seed::UInt32)::UInt32 seed ⊻= (seed \u0026lt;\u0026lt; 13) seed ⊻= (seed \u0026gt;\u0026gt; 17) seed ⊻= (seed \u0026lt;\u0026lt; 5) return seed end Then we transform this seed to a float number in the range of (0, 1) using the following function:\n1 2 3 4 function xorshift32_float(seed::UInt32)::Float32 value = Float32(xorshift32(seed)) * 2.3283064f-10 # Scale to [0,1) return max(value, 1.0f-16) # Ensure it\u0026#39;s in (0,1) end Finally, we use the Box-Muller transform to generate normally distributed random numbers:\n1 2 3 4 5 function box_muller(u1::Float32, u2::Float32) r = sqrt(-2.0f0 * log(u1)) theta = 2.0f0 * Float32(pi) * u2 return r * cos(theta) end Masks to avoid branching The Metal.jl kernel does not support branching, so we need to avoid using if statements in the kernel code. Instead, we use masks to control the flow of the simulation. The core update function for the problem in the cube $Q$ is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 for step in 1:num_steps # Boolean masks for exit conditions mask_x = (x \u0026lt; -1.0f0 || x \u0026gt; 1.0f0) ? 1 : 0 mask_v = (v \u0026lt; -1.0f0 || v \u0026gt; 1.0f0) ? 1 : 0 mask_exit = mask_x | mask_v # Combine masks (exit if either condition is true) continue_mask = 1 - mask_exit # 1 = active, 0 = exited # Generate two uniform distributed random numbers seed1 = xorshift32(seed1) seed2 = xorshift32(seed2) random_number1 = xorshift32_float(seed1) random_number2 = xorshift32_float(seed2) # Generate a normal distributed noise noise = box_muller(random_number1, random_number2) # Perturb the seeds to avoid deterministic patterns seed1 += UInt32(i) seed2 += UInt32(i) # Update position and velocity and store previous state if not exit x_prev, v_prev = continue_mask * x + mask_exit * x_prev, continue_mask * v + mask_exit * v_prev x += continue_mask * (v * time_step) v += continue_mask * (sqrt(time_step) * noise) end The mask_exit variable is used to check if the particle has exited the box. If it has, we set the continue_mask to 0, which effectively stops the simulation for that particle. The x_prev and v_prev variables are used to store the previous state of the particle before it exited.\nExample plots Consider the following Dirichlet boundary condition: Our codes can simulate the solution efficiently. The following plot shows the full solution and also a zoomed-in view of the solution near the singular boundary: In addition, we can plot the exit points distribution on the boundary for a starting point. The following is an example in the annulus: ","permalink":"http://localhost:1313/projects/monte-carlo-kfp-metal/","summary":"\u003cp\u003eThis project implements a Monte Carlo simulation for the Fokker-Planck equation using \u003ccode\u003eMetal\u003c/code\u003e, a framework for GPU programming on Apple devices. The simulation is designed to model the diffusion of particles in a potential field, which is common in statistical physics and machine learning.\u003c/p\u003e\n\u003cp\u003eThe codes are written in \u003ccode\u003eJulia\u003c/code\u003e and utilize the \u003ccode\u003eMetal.jl\u003c/code\u003e package to leverage the GPU for parallel computation. You can find the repository for this project \u003ca href=\"https://github.com/mingyi-ai/Monte_Carlo_KFP\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"Monte Carlo Simulation for Fokker-Planck Equations using Metal"},{"content":"In this post, we will explore the numerics of Hermite polynomials, as part of a numerical solver project for Kramers hyperbolic system. In particular, we try to compute eigenvectors numerically for the Hermite generating matrix, which is a tridiagonal matrix given by the recurrence relation\n$$ x \\mathrm{H}_{n}(x) = \\sqrt{n+1} \\mathrm{H}_{n+1}(x) - \\sqrt{n} \\mathrm{H}_{n-1}(x) $$for \\( n \\geq 1 \\) with \\(\\mathrm{H}_{n}(x)\\) being the \\(n\\)-th normalized Hermite polynomial. The generating matrix is given by\n$$ \\mathrm{T}_n = \\begin{pmatrix} 0 \u0026 \\sqrt{1} \u0026 0 \u0026 \\cdots \u0026 \\cdots \\\\ \\sqrt{1} \u0026 0 \u0026 \\sqrt{2} \u0026 \\cdots \u0026 \\cdots \\\\ 0 \u0026 \\sqrt{2} \u0026 \\ddots \u0026 \\cdots \u0026 \\cdots \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 0 \u0026 \\sqrt{n-1} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\sqrt{n-1} \u0026 0 \\end{pmatrix}. $$All the codes below are implemented in Julia.\nTheoretical eigenvalues and eigenvectors Recall that the Hermite polynomials are defined by\n$$ \\mathrm{He}_n(x) = (-1)^n \\exp(x^2/2) \\frac{\\mathrm{d}^n}{\\mathrm{d}x^n} \\left( \\exp(-x^2/2) \\right) $$for $n \\geq 0$. The first few Hermite polynomials are given by\n$$ \\mathrm{He}_0(x) = 1, \\\\ \\mathrm{He}_1(x) = x, \\\\ \\mathrm{He}_2(x) = x^2 - 1, \\\\ \\mathrm{He}_3(x) = x^3 - 3x. $$The Hermite polynomials can be computed using the recurrence relation\n$$ \\mathrm{He}_{n+1}(x) = x \\mathrm{He}_{n}(x) - n \\mathrm{He}_{n-1}(x) $$for $n \\geq 2$ with $\\mathrm{He}_0(x) = 1$ and $\\mathrm{He}_1(x) = x$.\nThe Hermite polynomials are orthogonal with respect to the (normalized) weight function $w(x) = \\exp(-x^2/2)/\\sqrt{2\\pi}$ on the interval $(-\\infty, \\infty)$. The $\\mathrm{L}^2$ norm of $\\mathrm{He}_n(x)$ with respect to this weight function is given by\n$$ ||\\mathrm{He}_n||_{\\mathrm{L}^2} = \\sqrt{n!} $$ for $n \\geq 0$. Let\u0026rsquo;s denote the normalized Hermite polynomials by\n$$ \\mathrm{H}_n = \\frac{\\mathrm{He}_n}{||\\mathrm{He}_n||_{\\mathrm{L}^2}} = \\frac{\\mathrm{He}_n}{\\sqrt{n!}}. $$It is well-known that the eigenvalues of the Hermite generating matrix $\\mathrm{T}_n$ of size $n$ are given by the roots of the Hermite polynomial $\\mathrm{H}_n(x)$, and the eigenvectors are given by the values of the Hermite polynomials at the roots, namely\n$$ \\mathrm{T}_n \\mathbf{v}_{n,k} = \\lambda_{n,k} \\mathbf{v}_{n,k} $$ where $\\lambda_{n,k}$ is the $k$-th root of $\\mathrm{He}_n(x)$, and\n$$ \\mathbf{v}_{n,k} = \\begin{pmatrix} \\mathrm{H}_0(\\lambda_{n,k}) \\\\ \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\cdots \\\\ \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\end{pmatrix} $$ is the corresponding eigenvector.\nTo see this, we do a direct computation\n$$ \\mathrm{T}_n \\mathbf{v}_{n,k} = \\begin{pmatrix} \\sqrt{1} \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\sqrt{1} \\mathrm{H}_0(\\lambda_{n,k}) + \\sqrt{2} \\mathrm{H}_2(\\lambda_{n,k}) \\\\ \\cdots\\\\ \\sqrt{n-2} \\mathrm{H}_{n-3}(\\lambda_{n,k}) + \\sqrt{n-1} \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\\\ \\sqrt{n-1} \\mathrm{H}_{n-2}(\\lambda_{n,k}) + \\sqrt{n} \\underbrace{\\mathrm{H}_{n}(\\lambda_{n,k})}_{=0} \\end{pmatrix} = \\begin{pmatrix} \\lambda_{n,k} \\mathrm{H}_0(\\lambda_{n,k}) \\\\ \\lambda_{n,k} \\mathrm{H}_1(\\lambda_{n,k}) \\\\ \\cdots \\\\ \\lambda_{n,k} \\mathrm{H}_{n-1}(\\lambda_{n,k}) \\end{pmatrix} = \\lambda_{n,k} \\mathbf{v}_{n,k}. $$Let\u0026rsquo;s define $\\mathbf{w}_{n,k} = {\\mathbf{v}_{n,k}}/{||\\mathbf{v}_{n,k}||_2}$, which is the normalized eigenvector, and\n$$ \\mathrm{P}_n := \\begin{pmatrix} \\mathbf{w}_{n,1} \u0026 \\mathbf{w}_{n,2} \u0026 \\cdots \u0026 \\mathbf{w}_{n,n} \\end{pmatrix} $$is the change of basis matrix. It follows that\n$$ \\mathrm{P}_n^T \\mathrm{T}_n \\mathrm{P}_n = \\mathrm{D}_n:= \\mathrm{diag}(\\lambda_{n,1}, \\lambda_{n,1}, \\cdots, \\lambda_{n,n}). $$Thus, in theory, we can compute the eigenvalues and eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ by computing the roots of the normalized Hermite polynomial $\\mathrm{H}_n(x)$ and evaluating the normalized Hermite polynomials at these roots.\nNumerical properties of the Hermite polynomials Numerical instability when evaluating the Hermite polynomials The first thing we need to note is that the Hermite polynomials\u0026rsquo; coefficients grow very quickly as $n$ increases. Very large and very small coefficients exist in the polynomial, which can lead to numerical instability when evaluating the polynomial. For example,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 using Polynomials function He_symbolic(n::Int) He_0 = Polynomial([1.0]) if n == 0 return He_0 end He_1 = Polynomial([0.0, 1.0]) if n == 1 return He_1 end for k in 2:n He_2 = Polynomial([0.0, 1.0]) * He_1 - (k - 1) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end 1 He_symbolic(15) -2.027025e6∙x + 4.729725e6∙x3 - 2.837835e6∙x5 + 675675.0∙x7 - 75075.0∙x9 + 4095.0∙x11 - 105.0∙x13 + 1.0∙x15\nFor $n=15$, the largest coefficient is around $10^{6}$, while the smallest is always $1.0$, this could lead to numerical instability when evaluating the polynomial directly. To solve this problem, we can use the recurrence relation to evaluate the polynomial, which should give us a more stable result for large $n$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function He(n::Int, x::Float64) He_0 = 1.0 if n == 0 return He_0 end He_1 = x for k in 2:n He_2 = x * He_1 - (k - 1) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end 1 2 3 4 5 6 x = 1.1 n = 51 val_direct = He_symbolic(n)(x) val_recursive = He(n, x) val_direct, val_recursive (-5.624972601518976e32, -5.624972601518795e32) We can see relatively small but noticeable discrepancies between the two methods, since Polynomials.jl already uses numerically stable methods to evaluate a polynomial. For our purposes, we use the recursive evaluation.\nNumerical instability when computing the roots of the Hermite polynomials The second thing we need to note is that the roots of the Hermite polynomials are very close to each other, especially for large $n$. This can lead to numerical instability when computing the roots of the polynomial. For example, for $n=15$, the roots are\n1 println(sort(roots(He_symbolic(15)))) [-6.36394788882981, -5.190093591304892, -4.196207711268877, -3.2890824243988406, -2.4324368270097385, -1.6067100690287315, -0.7991290683245481, 0.0, 0.7991290683245479, 1.6067100690287317, 2.4324368270097474, 3.2890824243988055, 4.196207711268944, 5.190093591304826, 6.36394788882981] As we can see, roots are clustered around $x=0$, and does not become more spaced out as $n$ increases. To see this, we can plot the roots of the Hermite polynomials for $n=1, \\dots, 20$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 using Plots # Collect roots for n=1:20 root_data = [] for n in 1:20 push!(root_data, roots(He_symbolic(n))) end # Flatten the root data and associate each root with its corresponding n x_vals = vcat(root_data...) y_vals = vcat([fill(n, length(root_data[n])) for n in 1:20]...) # Plot the roots distribution scatter(x_vals, y_vals, xlabel=\u0026#34;Roots\u0026#34;, ylabel=\u0026#34;n\u0026#34;, title=\u0026#34;Roots Distribution of Hermite polynomials for n=1:20\u0026#34;, legend=false) To efficiently compute the roots of the Hermite polynomials, we can use the generating matrix $\\mathrm{T}_n$ and compute the eigenvalues of the matrix. This is a numerically stable method as the matrix is tridiagonal, symmetric, and there exists numerically stable algorithms to compute the eigenvalues and eigenvectors of such matrices.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 function T(n::Int) d = zeros(n) e = sqrt.(1:n-1) T = SymTridiagonal(d, e) return T end using LinearAlgebra function get_eigen(n::Int) Tn = T(n) F = eigen(Tn) # Eigenvalues and eigenvectors idx = sortperm(F.values) # Sort eigenvalues and eigenvectors eigenvals = F.values[idx] # Sorted eigenvalues (roots) eigenvecs = F.vectors[:, idx] # Sorted eigenvectors # One step of Newton\u0026#39;s method to refine the roots He_nx = He.(n, eigenvals) # Hermite polynomial at the roots He_nm1x = He.(n-1, eigenvals) # Candidate for the derivative eigenvals .-= He_nx ./ (n .* He_nm1x) return eigenvals, eigenvecs end get_eigen (generic function with 1 method) 1 2 3 eigenvals, _ = get_eigen(5) println(\u0026#34;Eigenvalues for n=5: \u0026#34;, eigenvals) println(sort(roots(He_symbolic(5)))) Eigenvalues for n=5: [-2.8569700138728056, -1.355626179974266, 0.0, 1.355626179974266, 2.8569700138728056] [-2.856970013872804, -1.3556261799742675, 0.0, 1.3556261799742657, 2.856970013872808] From the above comparison, we can see that the eigenvalues computed from the generating matrix preserve the symmetry of the roots better. As $n$ increases, this algorithm becomes more efficient than the root-finding algorithm, as the roots become more clustered together.\nNormalized eigenvectors of the Hermite generating matrix Now we are in a position to compute the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$. Note that we already computed the eigenvalues and eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ in the previous section using a numerically robust method. In this section, we will construct the normalized eigenvectors of the Hermite generating matrix by evaluating the normalized Hermite polynomials at the roots of the Hermite polynomial, i.e. $\\mathbf{v}_{n,k}$ for $k=1, \\dots, n$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 using SpecialFunctions: logfactorial # An auxiliary function to compute the normalization constant function He_l2norm(n::Int) return sqrt(exp(logfactorial(n))) end function analytic_eigenvecs(n::Int) Pmat = zeros(Float64, n, n) # initializing the matrix rts, _ = get_eigen(n) # Get the roots for i in 1:n root_i = rts[i] for j in 1:n Pmat[j, i] = He(j-1, root_i) / He_l2norm(j-1) end # Normalize each vector (column) Pmat[:, i] /= norm(Pmat[:, i]) end return Pmat end Numerical properties of the eigenvectors Now, we have two ways to compute the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$: one is theoretically precise, the other is numerical. We will compare the two methods and see how well they agree with each other.\nThere are two important properties of the eigenvectors of the Hermite generating matrix $\\mathrm{T}_n$ that we need to note:\nThe column matrix $\\mathrm{P}_n$ of eigenvectors is orthonormal, i.e. $\\mathrm{P}_n^T \\mathrm{P}_n = \\mathrm{I}_n$. By conjugating the eigenvectors, we can obtain the eigenvalues of the Hermite generating matrix $\\mathrm{T}_n$, which is a diagonal matrix $\\mathrm{D}_n$ with the roots of the Hermite polynomial on the diagonal. $$ \\mathrm{P}_n^T \\mathrm{T}_n \\mathrm{P}_n = \\mathrm{D}_n. $$ We will check how well these properties hold for both methods of computing the eigenvectors. The following codes compute the $\\infty$ norm of the above two properties.\n1 2 3 4 5 6 7 function orthonormality_error(P::Matrix{Float64}) return norm(P\u0026#39; * P - I, Inf) end function diagonal_error(P::Matrix{Float64}, eigenvals::Vector{Float64}, n::Int) return norm(P\u0026#39; * T(n) * P - Diagonal(eigenvals), Inf) end Show Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Define the range of n n_values = 2:200 # Initialize error arrays for both methods analytic_orth_errors = Float64[] analytic_diag_errors = Float64[] numerical_orth_errors = Float64[] numerical_diag_errors = Float64[] # Compute errors for both methods for n in n_values P_analytic = analytic_eigenvecs(n) rts, P_numerical = get_eigen(n) push!(analytic_orth_errors, orthonormality_error(P_analytic)) push!(analytic_diag_errors, diagonal_error(P_analytic, rts, n)) push!(numerical_orth_errors, orthonormality_error(P_numerical)) push!(numerical_diag_errors, diagonal_error(P_numerical, rts, n)) end # Create 2x2 subplots p1 = plot(n_values, log10.(analytic_orth_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Orthonormality Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p1, n_values, log10.(numerical_orth_errors), label=\u0026#34;Numerical\u0026#34;) p2 = plot(n_values, log10.(analytic_diag_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Diagonalization Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p2, n_values, log10.(numerical_diag_errors), label=\u0026#34;Numerical\u0026#34;) plot(p1, p2, layout=(2, 1)) From the plot we can see that the analytical method is marginally better than the numerical method before $n\\approx 170$, but both errors for analytical method blow up after $n\\approx 170$, while the numerical method remains stable. The reason for this is that in the analytical method, we need to compute the normalization factor of the Hermite polynomial, which is given by $||\\mathrm{He}_n||_{\\mathrm{L}^2} = \\sqrt{n!}$, and the factorial grows very quickly, eventually blows up at $n\\approx 170$.\n1 2 3 for n in 169:172 print(He_l2norm(n), \u0026#34;, \u0026#34;) end 2.0661723086434517e152, 2.6939590968143674e153, Inf, Inf, To solve this problem, we can use the recurrence relation to compute the normalized Hermite polynomials as before, which avoids the need to compute the normalization factor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 function Henorm(n::Int, x::Float64) He_0 = 1.0 if n == 0 return He_0 end He_1 = x for k in 2:n He_2 = x * He_1 / sqrt(k) - sqrt((k - 1) / k) * He_0 He_0, He_1 = He_1, He_2 end return He_1 end function analytic_eigenvecs_norm(n::Int) Pmat = zeros(Float64, n, n) # initializing the matrix rts, _ = get_eigen(n) # Get the roots for i in 1:n root_i = rts[i] for j in 1:n Pmat[j, i] = Henorm(j-1, root_i) end # Normalize each vector (column) Pmat[:, i] /= norm(Pmat[:, i]) end return Pmat end Show Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Define the range of n n_values = 2:200 # Initialize error arrays for both methods analytic_orth_errors = Float64[] analytic_diag_errors = Float64[] numerical_orth_errors = Float64[] numerical_diag_errors = Float64[] # Compute errors for both methods for n in n_values P_analytic = analytic_eigenvecs_norm(n) rts, P_numerical = get_eigen(n) push!(analytic_orth_errors, orthonormality_error(P_analytic)) push!(analytic_diag_errors, diagonal_error(P_analytic, rts, n)) push!(numerical_orth_errors, orthonormality_error(P_numerical)) push!(numerical_diag_errors, diagonal_error(P_numerical, rts, n)) end # Create 2x2 subplots p1 = plot(n_values, log10.(analytic_orth_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Orthonormality Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p1, n_values, log10.(numerical_orth_errors), label=\u0026#34;Numerical\u0026#34;) p2 = plot(n_values, log10.(analytic_diag_errors), label=\u0026#34;Analytic\u0026#34;, title=\u0026#34;Log-scale Diagonalization Error\u0026#34;, xlabel=\u0026#34;n\u0026#34;, ylabel=\u0026#34;Error\u0026#34;) plot!(p2, n_values, log10.(numerical_diag_errors), label=\u0026#34;Numerical\u0026#34;) plot(p1, p2, layout=(2, 1)) After the above changes, we can see that the analytical method is now more stable than the numerical method, and the errors are much smaller. However, in practice, using large $n$ for the Hermite polynomials is not recommended, as there are already numerical instabilities when evaluating the Hermite polynomials.\nDownload the notebook 📥 Download Notebook\n","permalink":"http://localhost:1313/posts/25-05-02_hermite-numerics/","summary":"\u003cp\u003eIn this post, we will explore the numerics of Hermite polynomials, as part of a numerical solver project for Kramers hyperbolic system. In particular, we try to compute eigenvectors numerically for the Hermite generating matrix, which is a tridiagonal matrix given by the recurrence relation\u003c/p\u003e\n$$\nx \\mathrm{H}_{n}(x) = \\sqrt{n+1} \\mathrm{H}_{n+1}(x) - \\sqrt{n} \\mathrm{H}_{n-1}(x)\n$$\u003cp\u003efor \\( n \\geq 1 \\) with \\(\\mathrm{H}_{n}(x)\\) being the \\(n\\)-th normalized Hermite polynomial.\nThe generating matrix is given by\u003c/p\u003e","title":"Numerical Methods for Hermite Polynomials"},{"content":"I am a PhD student in Mathematics at Uppsala University. My research interests lie in the intersection of probability theory, partial differential equations, and machine learning. I am particularly interested in the mathematical analysis of stochastic processes and their applications in machine learning.\nResearch Publications Preprints M. Hou. Variational Formulation and Capacity Estimates for Non-Self-Adjoint Fokker-Planck Operators in Divergence Form. arXiv preprint, 2025. M. Hou. Boundedness of Weak Solutions to Degenerate Kolmogorov Equations of Hypoelliptic Type in Bounded Domains. arXiv preprint, 2024. B. Avelin, and M. Hou. Weak and Perron\u0026rsquo;s Solutions for Stationary Kramers-Fokker-Planck Equations in Bounded Domains. arXiv preprint, 2024. Published B. Avelin, M. Hou, and K. Nyström. A Galerkin Type Method for Kinetic Fokker-Planck Equations Based on Hermite Expansions. Kinet. Relat. Models, 2024. Thesis M. Hou. Behind the Training Dynamics of Neural Networks: Analysis of Fokker-Planck Equations and the Path to Metastability. PhD dissertation, Uppsala University, 2025. Contact Department of Mathematics\nUppsala University\n751 06 Uppsala\nSweden\nmingyi.hou@math.uu.se LinkedIn GitHub ","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003eI am a PhD student in Mathematics at Uppsala University. My research interests lie in the intersection of probability theory, partial differential equations, and machine learning. I am particularly interested in the mathematical analysis of stochastic processes and their applications in machine learning.\u003c/p\u003e\n\u003ch2 id=\"research-publications\"\u003eResearch Publications\u003c/h2\u003e\n\u003ch3 id=\"preprints\"\u003ePreprints\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eM. Hou. Variational Formulation and Capacity Estimates for Non-Self-Adjoint Fokker-Planck Operators in Divergence Form. \u003ca href=\"https://arxiv.org/abs/2502.12036\"\u003earXiv preprint, 2025\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eM. Hou. Boundedness of Weak Solutions to Degenerate Kolmogorov Equations of Hypoelliptic Type in Bounded Domains. \u003ca href=\"https://arxiv.org/abs/2407.00800\"\u003earXiv preprint, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eB. Avelin, and M. Hou. Weak and Perron\u0026rsquo;s Solutions for Stationary Kramers-Fokker-Planck Equations in Bounded Domains. \u003ca href=\"https://arxiv.org/abs/2405.04070\"\u003earXiv preprint, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"published\"\u003ePublished\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eB. Avelin, M. Hou, and K. Nyström. A Galerkin Type Method for Kinetic Fokker-Planck Equations Based on Hermite Expansions. \u003ca href=\"https://doi.org/10.3934/krm.2023035\"\u003eKinet. Relat. Models, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"thesis\"\u003eThesis\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eM. Hou. Behind the Training Dynamics of Neural Networks: Analysis of Fokker-Planck Equations and the Path to Metastability. \u003ca href=\"https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-553381\"\u003ePhD dissertation, Uppsala University, 2025.\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"contact\"\u003eContact\u003c/h2\u003e\n\u003cdiv style=\"display: flex; justify-content: space-between; flex-wrap: wrap;\"\u003e\n  \u003cdiv style=\"flex: 1; min-width: 200px;\"\u003e\n    \u003cp\u003eDepartment of Mathematics\u003cbr\u003e\n    Uppsala University\u003cbr\u003e\n    751 06 Uppsala\u003cbr\u003e\n    Sweden\u003c/p\u003e","title":"About Me"},{"content":"This is my Kaggle notebook for the House Prices - Advanced Regression Techniques competition on Kaggle. The goal of this competition is to predict the sale price of homes in Ames, Iowa, based on various features of the homes.\nI will be using various techniques learned in the course to predict the sale price of homes. The techniques include: preprocessing the data, feature engineering, and using different machine learning models to make predictions. I will also be using cross-validation to evaluate the performance of the models and to avoid overfitting.\nExploratory Data Analysis (EDA) Load Libraries \u0026amp; Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from pathlib import Path import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # Set plot styles sns.set(style=\u0026#34;whitegrid\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (12, 6) # Load data data_dir = Path(\u0026#34;./house-prices-advanced-regression-techniques\u0026#34;) df_train = pd.read_csv(data_dir / \u0026#34;train.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) df_test = pd.read_csv(data_dir / \u0026#34;test.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) print(f\u0026#34;Train shape: {df_train.shape}\u0026#34;) print(f\u0026#34;Test shape: {df_test.shape}\u0026#34;) Train shape: (1460, 80) Test shape: (1459, 79) Understand the Target Variable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Distribution of target sns.histplot(df_train[\u0026#34;SalePrice\u0026#34;], kde=True) plt.title(\u0026#34;SalePrice Distribution\u0026#34;) plt.xlabel(\u0026#34;SalePrice\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.savefig(\u0026#34;saleprice_distribution.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() # Log-transform target to check skew sns.histplot(np.log1p(df_train[\u0026#34;SalePrice\u0026#34;]), kde=True) plt.title(\u0026#34;Log-Transformed SalePrice\u0026#34;) plt.xlabel(\u0026#34;Log(SalePrice + 1)\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.savefig(\u0026#34;log_saleprice_distribution.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() # Summary stats df_train[\u0026#34;SalePrice\u0026#34;].describe() count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000 Name: SalePrice, dtype: float64 Overview of the Dataset 1 2 3 4 5 6 7 df = pd.concat([df_train, df_test], axis=0) # df.info() # df.describe() # Missing value heatmap import missingno as msno msno.matrix(df) \u0026lt;Axes: \u0026gt; Data Cleaning 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def clean_data(df): # Clean to_category = [\u0026#39;MSSubClass\u0026#39;, \u0026#39;MoSold\u0026#39;, \u0026#39;YrSold\u0026#39;, \u0026#39;GarageYrBlt\u0026#39;, \u0026#39;YearBuilt\u0026#39;, \u0026#39;YearRemodAdd\u0026#39;] for col in to_category: df[col] = df[col].astype(str) df[\u0026#39;Functional\u0026#39;] = df[\u0026#39;Functional\u0026#39;].fillna(\u0026#39;Typ\u0026#39;) df[\u0026#34;Electrical\u0026#34;] = df[\u0026#34;Electrical\u0026#34;].fillna(\u0026#39;SBrkr\u0026#39;) df[\u0026#34;KitchenQual\u0026#34;] = df[\u0026#34;KitchenQual\u0026#34;].fillna(\u0026#39;TA\u0026#39;) df[\u0026#34;Exterior1st\u0026#34;] = df[\u0026#34;Exterior1st\u0026#34;].fillna(df[\u0026#34;Exterior1st\u0026#34;].mode()[0]) df[\u0026#34;Exterior2nd\u0026#34;] = df[\u0026#34;Exterior2nd\u0026#34;].fillna(df[\u0026#34;Exterior2nd\u0026#34;].mode()[0]) df[\u0026#34;SaleType\u0026#34;] = df[\u0026#34;SaleType\u0026#34;].fillna(df[\u0026#34;SaleType\u0026#34;].mode()[0]) # Impute # Fill missing values in object columns with \u0026#34;None\u0026#34; objects = [] for i in df.columns: if df[i].dtype == object: objects.append(i) df.update(df[objects].fillna(\u0026#39;None\u0026#39;)) # Fill missing values in numeric columns with 0 numerics = [] for i in df.columns: if df[i].dtype != object: numerics.append(i) df.update(df[numerics].fillna(0)) return df 1 2 3 4 5 6 7 8 9 10 11 12 13 def load_data(): data_dir = Path(\u0026#34;./house-prices-advanced-regression-techniques\u0026#34;) df_train = pd.read_csv(data_dir / \u0026#34;train.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) df_test = pd.read_csv(data_dir / \u0026#34;test.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) # Clean data df = pd.concat([df_train, df_test], axis=0) df = clean_data(df) # Split back into train and test df_train = df.loc[df_train.index, :] df_test = df.loc[df_test.index, :].drop(columns=[\u0026#34;SalePrice\u0026#34;]) return df_train, df_test 1 2 3 4 5 6 7 df_train, df_test = load_data() # Check the cleaned data # train_missing = df_train.isnull().sum() # print(train_missing[train_missing \u0026gt; 0]) # test_missing = df_test.isnull().sum() # print(test_missing[test_missing \u0026gt; 0]) # df_train.info() Correlation with Target (Numerical Features) 1 2 3 4 5 6 7 8 9 10 11 # Compute correlation matrix corr_matrix = df_train.corr(numeric_only=True) # Get top 15 features correlated with SalePrice top_corr = corr_matrix[\u0026#34;SalePrice\u0026#34;].abs().sort_values(ascending=False).head(15) # Visualize sns.heatmap(df_train[top_corr.index].corr(), annot=True, cmap=\u0026#34;coolwarm\u0026#34;, fmt=\u0026#34;.2f\u0026#34;) plt.title(\u0026#34;Top Correlated Features with SalePrice\u0026#34;) plt.savefig(\u0026#34;top_correlated_features.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() Categorical Features Preview 1 2 3 4 5 6 7 8 9 10 11 12 13 categoricals = df_train.select_dtypes(include=\u0026#34;object\u0026#34;).columns print(f\u0026#34;Categorical features: {len(categoricals)}\u0026#34;) # print(categoricals.tolist()) # Example: Visualize average SalePrice by a few important categorical features important_cats = [\u0026#34;Neighborhood\u0026#34;, \u0026#34;ExterQual\u0026#34;, \u0026#34;GarageFinish\u0026#34;, \u0026#34;KitchenQual\u0026#34;] for col in important_cats: sns.boxplot(data=df_train, x=col, y=\u0026#34;SalePrice\u0026#34;) plt.title(f\u0026#34;SalePrice by {col}\u0026#34;) plt.xticks(rotation=45) plt.savefig(f\u0026#34;saleprice_by_{col}.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() Categorical features: 49 Time-Related Patterns 1 2 3 4 5 6 7 sns.boxplot(x=\u0026#34;YrSold\u0026#34;, y=\u0026#34;SalePrice\u0026#34;, data=df_train) plt.title(\u0026#34;SalePrice by Year Sold\u0026#34;) plt.show() sns.scatterplot(x=\u0026#34;YearBuilt\u0026#34;, y=\u0026#34;SalePrice\u0026#34;, data=df_train) plt.title(\u0026#34;SalePrice vs Year Built\u0026#34;) plt.show() It does not look like there is any time related feature.\nFeature Engineering 1 2 3 X = df_train.copy() y = X.pop(\u0026#34;SalePrice\u0026#34;) # X.info() Separate Categorical and Numerical Features 1 2 3 4 # cat_cols = X.select_dtypes(include=[\u0026#39;object\u0026#39;]).columns.tolist() # num_cols = X.select_dtypes(exclude=[\u0026#39;object\u0026#39;]).columns.tolist() # print(f\u0026#34;Categorical columns: {len(cat_cols)}\u0026#34;) # print(f\u0026#34;Numerical columns: {len(num_cols)}\u0026#34;) Encoding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from sklearn.base import BaseEstimator, TransformerMixin from category_encoders import TargetEncoder class Encoder(BaseEstimator, TransformerMixin): def __init__(self, ordered_levels): self.ordered_levels = ordered_levels self.low_cardinality = [] self.high_cardinality = [] self.te = None def fit(self, X, y): X = X.copy() self.ordered_levels = {k: [\u0026#34;None\u0026#34;] + v for k, v in self.ordered_levels.items()} for col, order in self.ordered_levels.items(): if col in X.columns: X[col] = pd.Categorical(X[col], categories=order, ordered=True).codes ordinal_cols = list(self.ordered_levels.keys()) nominal_cols = [col for col in X.select_dtypes(include=\u0026#39;object\u0026#39;).columns if col not in ordinal_cols] self.low_cardinality = [col for col in nominal_cols if X[col].nunique() \u0026lt;= 10] self.high_cardinality = [col for col in nominal_cols if X[col].nunique() \u0026gt; 10] for col in self.low_cardinality: X[col] = X[col].astype(\u0026#34;category\u0026#34;).cat.codes self.te = TargetEncoder() self.te.fit(X[self.high_cardinality], y) return self def transform(self, X): X = X.copy() for col, order in self.ordered_levels.items(): if col in X.columns: X[col] = pd.Categorical(X[col], categories=order, ordered=True).codes for col in self.low_cardinality: if col in X.columns: X[col] = X[col].astype(\u0026#34;category\u0026#34;).cat.codes if self.te: X[self.high_cardinality] = self.te.transform(X[self.high_cardinality]) return X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 five_levels = [\u0026#34;Po\u0026#34;, \u0026#34;Fa\u0026#34;, \u0026#34;TA\u0026#34;, \u0026#34;Gd\u0026#34;, \u0026#34;Ex\u0026#34;] ten_levels = list(range(1, 11)) # 1 - 10 is the correct range! ordered_levels = { \u0026#34;OverallQual\u0026#34;: ten_levels, \u0026#34;OverallCond\u0026#34;: ten_levels, \u0026#34;ExterQual\u0026#34;: five_levels, \u0026#34;ExterCond\u0026#34;: five_levels, \u0026#34;BsmtQual\u0026#34;: five_levels, \u0026#34;BsmtCond\u0026#34;: five_levels, \u0026#34;HeatingQC\u0026#34;: five_levels, \u0026#34;KitchenQual\u0026#34;: five_levels, \u0026#34;FireplaceQu\u0026#34;: five_levels, \u0026#34;GarageQual\u0026#34;: five_levels, \u0026#34;GarageCond\u0026#34;: five_levels, \u0026#34;PoolQC\u0026#34;: five_levels, \u0026#34;LotShape\u0026#34;: [\u0026#34;Reg\u0026#34;, \u0026#34;IR1\u0026#34;, \u0026#34;IR2\u0026#34;, \u0026#34;IR3\u0026#34;], \u0026#34;LandSlope\u0026#34;: [\u0026#34;Sev\u0026#34;, \u0026#34;Mod\u0026#34;, \u0026#34;Gtl\u0026#34;], \u0026#34;BsmtExposure\u0026#34;: [\u0026#34;No\u0026#34;, \u0026#34;Mn\u0026#34;, \u0026#34;Av\u0026#34;, \u0026#34;Gd\u0026#34;], \u0026#34;BsmtFinType1\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;LwQ\u0026#34;, \u0026#34;Rec\u0026#34;, \u0026#34;BLQ\u0026#34;, \u0026#34;ALQ\u0026#34;, \u0026#34;GLQ\u0026#34;], \u0026#34;BsmtFinType2\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;LwQ\u0026#34;, \u0026#34;Rec\u0026#34;, \u0026#34;BLQ\u0026#34;, \u0026#34;ALQ\u0026#34;, \u0026#34;GLQ\u0026#34;], \u0026#34;Functional\u0026#34;: [\u0026#34;Sal\u0026#34;, \u0026#34;Sev\u0026#34;, \u0026#34;Maj1\u0026#34;, \u0026#34;Maj2\u0026#34;, \u0026#34;Mod\u0026#34;, \u0026#34;Min2\u0026#34;, \u0026#34;Min1\u0026#34;, \u0026#34;Typ\u0026#34;], \u0026#34;GarageFinish\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;RFn\u0026#34;, \u0026#34;Fin\u0026#34;], \u0026#34;PavedDrive\u0026#34;: [\u0026#34;N\u0026#34;, \u0026#34;P\u0026#34;, \u0026#34;Y\u0026#34;], \u0026#34;Utilities\u0026#34;: [\u0026#34;NoSeWa\u0026#34;, \u0026#34;NoSewr\u0026#34;, \u0026#34;AllPub\u0026#34;], \u0026#34;CentralAir\u0026#34;: [\u0026#34;N\u0026#34;, \u0026#34;Y\u0026#34;], \u0026#34;Electrical\u0026#34;: [\u0026#34;Mix\u0026#34;, \u0026#34;FuseP\u0026#34;, \u0026#34;FuseF\u0026#34;, \u0026#34;FuseA\u0026#34;, \u0026#34;SBrkr\u0026#34;], \u0026#34;Fence\u0026#34;: [\u0026#34;MnWw\u0026#34;, \u0026#34;GdWo\u0026#34;, \u0026#34;MnPrv\u0026#34;, \u0026#34;GdPrv\u0026#34;], } 1 2 3 4 5 6 7 from sklearn.pipeline import Pipeline from xgboost import XGBRegressor basepipe = Pipeline([ (\u0026#34;encode\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;xgb\u0026#34;, XGBRegressor(random_state=42)) ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from sklearn.metrics import make_scorer, mean_squared_log_error from sklearn.model_selection import cross_val_score import numpy as np # Custom RMSLE scorer (greater_is_better=False because lower is better) rmsle_scorer = make_scorer( lambda y_true, y_pred: np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred))), greater_is_better=False ) y_log = np.log(y) # We train on the log transformed target # Then use it with cross_val_score score = cross_val_score(basepipe, X, y_log, cv=5, scoring=rmsle_scorer) print(f\u0026#34;RMSLE scores: {-score}\u0026#34;) # Negate to get actual RMSLE values print(f\u0026#34;Mean RMSLE: {-score.mean():.5f}\u0026#34;) RMSLE scores: [0.1397286 0.15149179 0.14633292 0.1268186 0.14509794] Mean RMSLE: 0.14189 Transform Skewed Numerical Features This transform is not used in the end.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class SkewedFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self, threshold=0.75): self.threshold = threshold self.skewed_cols = [] def fit(self, X, y=None): X = pd.DataFrame(X) skewness = X.skew().abs() self.skewed_cols = skewness[skewness \u0026gt; self.threshold].index.tolist() return self def transform(self, X): X = pd.DataFrame(X).copy() for col in self.skewed_cols: X[col] = np.log1p(X[col]) return X skew_transformer = SkewedFeatureTransformer() Add New Features 1 2 from sklearn.preprocessing import FunctionTransformer from sklearn.pipeline import Pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def add_custom_features(df): df = df.copy() df[\u0026#39;TotalSF\u0026#39;] = df[\u0026#39;TotalBsmtSF\u0026#39;] + df[\u0026#39;1stFlrSF\u0026#39;] + df[\u0026#39;2ndFlrSF\u0026#39;] df[\u0026#39;Total_sqr_footage\u0026#39;] = df[\u0026#39;BsmtFinSF1\u0026#39;] + df[\u0026#39;BsmtFinSF2\u0026#39;] + df[\u0026#39;1stFlrSF\u0026#39;] + df[\u0026#39;2ndFlrSF\u0026#39;] df[\u0026#39;Total_Bathrooms\u0026#39;] = df[\u0026#39;FullBath\u0026#39;] + (0.5 * df[\u0026#39;HalfBath\u0026#39;]) + df[\u0026#39;BsmtFullBath\u0026#39;] + (0.5 * df[\u0026#39;BsmtHalfBath\u0026#39;]) df[\u0026#39;Total_porch_sf\u0026#39;] = df[\u0026#39;OpenPorchSF\u0026#39;] + df[\u0026#39;3SsnPorch\u0026#39;] + df[\u0026#39;EnclosedPorch\u0026#39;] + df[\u0026#39;ScreenPorch\u0026#39;] + df[\u0026#39;WoodDeckSF\u0026#39;] df[\u0026#39;haspool\u0026#39;] = (df[\u0026#39;PoolArea\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;has2ndfloor\u0026#39;] = (df[\u0026#39;2ndFlrSF\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasgarage\u0026#39;] = (df[\u0026#39;GarageArea\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasbsmt\u0026#39;] = (df[\u0026#39;TotalBsmtSF\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasfireplace\u0026#39;] = (df[\u0026#39;Fireplaces\u0026#39;] \u0026gt; 0).astype(int) return df custom_feature_step = FunctionTransformer(add_custom_features) Mutual Information From my experiment, this seems to be the most useful tool.\n1 2 3 4 from sklearn.feature_selection import SelectKBest, mutual_info_regression # Select top k features based on MI mi_selector = SelectKBest(score_func=mutual_info_regression, k=50) # or \u0026#39;k=\u0026#34;all\u0026#34;\u0026#39; to get scores PCA This is not used in the end.\n1 2 from sklearn.decomposition import PCA pca = PCA(n_components=50) # You can tune this Model-Based Feature Selection Again, this is not used in the end. Mutual information already turns out to be very effective.\n1 2 3 4 5 from sklearn.feature_selection import SelectFromModel from xgboost import XGBRegressor # Use a fitted model to select features with importance above threshold model_selector = SelectFromModel(XGBRegressor(n_estimators=100), threshold=\u0026#34;median\u0026#34;) Model Evaluation 1 2 3 4 5 6 7 8 9 10 from sklearn.pipeline import make_pipeline from sklearn.feature_selection import SelectKBest, mutual_info_regression pipeline = Pipeline([ (\u0026#34;custom_features\u0026#34;, custom_feature_step), (\u0026#34;encoder\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;feature_select\u0026#34;, SelectKBest(score_func=mutual_info_regression, k=60)), (\u0026#34;model\u0026#34;, XGBRegressor()) ]) 1 2 3 4 # Then use it with cross_val_score score = cross_val_score(pipeline, X, y_log, cv=5, scoring=rmsle_scorer) print(f\u0026#34;RMSLE scores: {-score}\u0026#34;) # Negate to get actual RMSLE values print(f\u0026#34;Mean RMSLE: {-score.mean():.5f}\u0026#34;) RMSLE scores: [0.13717768 0.15245099 0.1511411 0.12531991 0.13995213] Mean RMSLE: 0.14121 We can see a slight improvement compared to the baseline model above.\nHyperparameter Tuning and Final Predictions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # import optuna # from sklearn.model_selection import cross_val_score # def objective(trial): # xgb_params = dict( # max_depth=trial.suggest_int(\u0026#34;max_depth\u0026#34;, 2, 10), # learning_rate=trial.suggest_float(\u0026#34;learning_rate\u0026#34;, 1e-4, 1e-1, log=True), # n_estimators=trial.suggest_int(\u0026#34;n_estimators\u0026#34;, 1000, 8000), # min_child_weight=trial.suggest_int(\u0026#34;min_child_weight\u0026#34;, 1, 10), # colsample_bytree=trial.suggest_float(\u0026#34;colsample_bytree\u0026#34;, 0.2, 1.0), # subsample=trial.suggest_float(\u0026#34;subsample\u0026#34;, 0.2, 1.0), # reg_alpha=trial.suggest_float(\u0026#34;reg_alpha\u0026#34;, 1e-4, 1e2, log=True), # reg_lambda=trial.suggest_float(\u0026#34;reg_lambda\u0026#34;, 1e-4, 1e2, log=True), # ) # pipeline.set_params(**{f\u0026#34;model__{key}\u0026#34;: val for key, val in xgb_params.items()}) # score = cross_val_score(pipeline, X, y_log, cv=5, scoring=rmsle_scorer) # return -score.mean() # Minimize RMSLE # # Run Optuna optimization # study = optuna.create_study(direction=\u0026#34;minimize\u0026#34;) # study.optimize(objective, n_trials=10) 1 2 3 4 5 6 7 8 9 # This is the set of parameters for my submission xgb_params = {\u0026#39;max_depth\u0026#39;: 7, \u0026#39;learning_rate\u0026#39;: 0.004565565417769295, \u0026#39;n_estimators\u0026#39;: 4701, \u0026#39;min_child_weight\u0026#39;: 9, \u0026#39;colsample_bytree\u0026#39;: 0.5911157102802619, \u0026#39;subsample\u0026#39;: 0.265969852467484, \u0026#39;reg_alpha\u0026#39;: 0.030977607695995966, \u0026#39;reg_lambda\u0026#39;: 0.168357167207514} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from sklearn.metrics import mean_squared_log_error # Train the pipeline on the entire training set X = df_train.copy() y = X.pop(\u0026#39;SalePrice\u0026#39;) y_log = np.log(y) # Initialize the model pipeline = Pipeline([ (\u0026#34;custom_features\u0026#34;, custom_feature_step), (\u0026#34;encoder\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;feature_select\u0026#34;, SelectKBest(score_func=mutual_info_regression, k=60)), (\u0026#34;model\u0026#34;, XGBRegressor()) ]) # Properly prefix parameter names with \u0026#34;model__\u0026#34; # best_params_prefixed = {f\u0026#34;model__{key}\u0026#34;: val for key, val in study.best_params.items()} best_params_prefixed = {f\u0026#34;model__{key}\u0026#34;: val for key, val in xgb_params.items()} # Set the best parameters to the pipeline pipeline.set_params(**best_params_prefixed) # Train the model pipeline.fit(X, y_log) 1 2 3 # Predict on the test set test_predictions = pipeline.predict(df_test) predictions = np.exp(test_predictions) 1 2 3 # Save the final result output = pd.DataFrame({\u0026#39;Id\u0026#39;: df_test.index, \u0026#39;SalePrice\u0026#39;: predictions}) output.to_csv(\u0026#39;my_submission.csv\u0026#39;, index=False) ","permalink":"http://localhost:1313/projects/housing-price-prediction-with-xgboost/","summary":"\u003cp\u003eThis is my \u003ca href=\"https://www.kaggle.com/code/houmingyi/housing-price-prediction-with-xgboost\"\u003eKaggle notebook\u003c/a\u003e for the \u003ca href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\"\u003eHouse Prices - Advanced Regression Techniques\u003c/a\u003e competition on Kaggle. The goal of this competition is to predict the sale price of homes in Ames, Iowa, based on various features of the homes.\u003c/p\u003e\n\u003cp\u003eI will be using various techniques learned in the course to predict the sale price of homes. The techniques include: preprocessing the data, feature engineering, and using different machine learning models to make predictions. I will also be using cross-validation to evaluate the performance of the models and to avoid overfitting.\u003c/p\u003e","title":"Housing Price Prediction with XGBoost"},{"content":"This project implements a Monte Carlo simulation for the Fokker-Planck equation using Metal, a framework for GPU programming on Apple devices. The simulation is designed to model the diffusion of particles in a potential field, which is common in statistical physics and machine learning.\nThe codes are written in Julia and utilize the Metal.jl package to leverage the GPU for parallel computation. You can find the repository for this project here.\nKramers-Fokker-Planck Equation The Kramers-Fokker-Planck equation is the generator of stochastic process modeling a particle\u0026rsquo;s movement in a potential field. The stationary solution of the Fokker-Planck equation is the equilibrium distribution of the particle\u0026rsquo;s position.\n$$ \\begin{cases} \\rm{d}X_t = V_t \\rm{d}t,\\newline \\rm{d}V_t = \\rm{d}B_t, \\end{cases} $$$$\\cal{L} = \\frac{1}{2}\\frac{\\partial^2}{\\partial v^2} + v\\frac{\\partial}{\\partial x}.$$We consider the process in the box $Q = (-1, 1)\\times(-1, 1)$, and simulate the process starting at a point $z_0 = (x_0, v_0)\\in Q$ until it hits the boundary of the box.\nThe Monte Carlo simulation involves:\nA Metal kernel function that updates the position of the particle according to the discretized SDE, namely $$ x_{n+1} = x_n + v_n \\Delta t, \\quad v_{n+1} = v_n + \\sqrt{\\Delta t} \\xi_n,$$ where $\\xi_n \\sim \\mathcal{N}(0, 1)$ is the standard normal random variable. Since the Metal kernel does not support random number generation, we also write a simple GPU friendly random number generator based on xorshift32 and Box-Muller method. The Kernel function does not support branching, the iteration will be fixed steps, and we use mask to stop the iteration when the particle hits the boundary. We simulate the process for a large number of particles and plot the harmonic measure on the boundary of the annulus. Features of the Project GPU friendly random number generator The random number generator is based on the xorshift32 algorithm, which is a simple and efficient pseudo-random number generator. The Box-Muller transform is used to generate normally distributed random numbers from uniformly distributed random numbers.\nGiven a seed ranged from 0 to 2^32-1, the xorshift32 algorithm generates a new seed by performing bitwise operations on the current seed.\n1 2 3 4 5 6 function xorshift32(seed::UInt32)::UInt32 seed ⊻= (seed \u0026lt;\u0026lt; 13) seed ⊻= (seed \u0026gt;\u0026gt; 17) seed ⊻= (seed \u0026lt;\u0026lt; 5) return seed end Then we transform this seed to a float number in the range of (0, 1) using the following function:\n1 2 3 4 function xorshift32_float(seed::UInt32)::Float32 value = Float32(xorshift32(seed)) * 2.3283064f-10 # Scale to [0,1) return max(value, 1.0f-16) # Ensure it\u0026#39;s in (0,1) end Finally, we use the Box-Muller transform to generate normally distributed random numbers:\n1 2 3 4 5 function box_muller(u1::Float32, u2::Float32) r = sqrt(-2.0f0 * log(u1)) theta = 2.0f0 * Float32(pi) * u2 return r * cos(theta) end Masks to avoid branching The Metal.jl kernel does not support branching, so we need to avoid using if statements in the kernel code. Instead, we use masks to control the flow of the simulation. The core update function for the problem in the cube $Q$ is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 for step in 1:num_steps # Boolean masks for exit conditions mask_x = (x \u0026lt; -1.0f0 || x \u0026gt; 1.0f0) ? 1 : 0 mask_v = (v \u0026lt; -1.0f0 || v \u0026gt; 1.0f0) ? 1 : 0 mask_exit = mask_x | mask_v # Combine masks (exit if either condition is true) continue_mask = 1 - mask_exit # 1 = active, 0 = exited # Generate two uniform distributed random numbers seed1 = xorshift32(seed1) seed2 = xorshift32(seed2) random_number1 = xorshift32_float(seed1) random_number2 = xorshift32_float(seed2) # Generate a normal distributed noise noise = box_muller(random_number1, random_number2) # Perturb the seeds to avoid deterministic patterns seed1 += UInt32(i) seed2 += UInt32(i) # Update position and velocity and store previous state if not exit x_prev, v_prev = continue_mask * x + mask_exit * x_prev, continue_mask * v + mask_exit * v_prev x += continue_mask * (v * time_step) v += continue_mask * (sqrt(time_step) * noise) end The mask_exit variable is used to check if the particle has exited the box. If it has, we set the continue_mask to 0, which effectively stops the simulation for that particle. The x_prev and v_prev variables are used to store the previous state of the particle before it exited.\nExample plots Consider the following Dirichlet boundary condition: Our codes can simulate the solution efficiently. The following plot shows the full solution and also a zoomed-in view of the solution near the singular boundary: In addition, we can plot the exit points distribution on the boundary for a starting point. The following is an example in the annulus: ","permalink":"http://localhost:1313/projects/monte-carlo-kfp-metal/","summary":"\u003cp\u003eThis project implements a Monte Carlo simulation for the Fokker-Planck equation using \u003ccode\u003eMetal\u003c/code\u003e, a framework for GPU programming on Apple devices. The simulation is designed to model the diffusion of particles in a potential field, which is common in statistical physics and machine learning.\u003c/p\u003e\n\u003cp\u003eThe codes are written in \u003ccode\u003eJulia\u003c/code\u003e and utilize the \u003ccode\u003eMetal.jl\u003c/code\u003e package to leverage the GPU for parallel computation. You can find the repository for this project \u003ca href=\"https://github.com/mingyi-ai/Monte_Carlo_KFP\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"Monte Carlo Simulation for Fokker-Planck Equations using Metal"}]