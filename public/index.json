[{"content":"Introduction This is bold text, and this is emphasized text.\nVisit the Hugo website!\n","permalink":"http://localhost:1313/posts/my-first-post/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis is \u003cstrong\u003ebold\u003c/strong\u003e text, and this is \u003cem\u003eemphasized\u003c/em\u003e text.\u003c/p\u003e\n\u003cp\u003eVisit the \u003ca href=\"https://gohugo.io\"\u003eHugo\u003c/a\u003e website!\u003c/p\u003e","title":"My First Post"},{"content":"I am a PhD student in Mathematics at Uppsala University. My research interests lie in the intersection of probability theory, partial differential equations, and machine learning. I am particularly interested in the mathematical analysis of stochastic processes and their applications in machine learning.\nResearch Publications Preprints M. Hou. Variational Formulation and Capacity Estimates for Non-Self-Adjoint Fokker-Planck Operators in Divergence Form. arXiv preprint, 2025. M. Hou. Boundedness of Weak Solutions to Degenerate Kolmogorov Equations of Hypoelliptic Type in Bounded Domains. arXiv preprint, 2024. B. Avelin, and M. Hou. Weak and Perron\u0026rsquo;s Solutions for Stationary Kramers-Fokker-Planck Equations in Bounded Domains. arXiv preprint, 2024. Published B. Avelin, M. Hou, and K. Nyström. A Galerkin Type Method for Kinetic Fokker-Planck Equations Based on Hermite Expansions. Kinet. Relat. Models, 2024. Contact Department of Mathematics\nUppsala University\n751 06 Uppsala\nSweden\nmingyi.hou@math.uu.se LinkedIn GitHub ","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003eI am a PhD student in Mathematics at Uppsala University. My research interests lie in the intersection of probability theory, partial differential equations, and machine learning. I am particularly interested in the mathematical analysis of stochastic processes and their applications in machine learning.\u003c/p\u003e\n\u003ch2 id=\"research-publications\"\u003eResearch Publications\u003c/h2\u003e\n\u003ch3 id=\"preprints\"\u003ePreprints\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eM. Hou. Variational Formulation and Capacity Estimates for Non-Self-Adjoint Fokker-Planck Operators in Divergence Form. \u003ca href=\"https://arxiv.org/abs/2502.12036\"\u003earXiv preprint, 2025\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eM. Hou. Boundedness of Weak Solutions to Degenerate Kolmogorov Equations of Hypoelliptic Type in Bounded Domains. \u003ca href=\"https://arxiv.org/abs/2407.00800\"\u003earXiv preprint, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eB. Avelin, and M. Hou. Weak and Perron\u0026rsquo;s Solutions for Stationary Kramers-Fokker-Planck Equations in Bounded Domains. \u003ca href=\"https://arxiv.org/abs/2405.04070\"\u003earXiv preprint, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"published\"\u003ePublished\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eB. Avelin, M. Hou, and K. Nyström. A Galerkin Type Method for Kinetic Fokker-Planck Equations Based on Hermite Expansions. \u003ca href=\"https://doi.org/10.3934/krm.2023035\"\u003eKinet. Relat. Models, 2024\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"contact\"\u003eContact\u003c/h2\u003e\n\u003cdiv style=\"display: flex; justify-content: space-between; flex-wrap: wrap;\"\u003e\n  \u003cdiv style=\"flex: 1; min-width: 200px;\"\u003e\n    \u003cp\u003eDepartment of Mathematics\u003cbr\u003e\n    Uppsala University\u003cbr\u003e\n    751 06 Uppsala\u003cbr\u003e\n    Sweden\u003c/p\u003e","title":"About Me"},{"content":"This is my Kaggle notebook for the House Prices - Advanced Regression Techniques competition on Kaggle. The goal of this competition is to predict the sale price of homes in Ames, Iowa, based on various features of the homes.\nI will be using various techniques learned in the course to predict the sale price of homes. The techniques include: preprocessing the data, feature engineering, and using different machine learning models to make predictions. I will also be using cross-validation to evaluate the performance of the models and to avoid overfitting.\nExploratory Data Analysis (EDA) Load Libraries \u0026amp; Data import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from pathlib import Path import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # Set plot styles sns.set(style=\u0026#34;whitegrid\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (12, 6) # Load data data_dir = Path(\u0026#34;./house-prices-advanced-regression-techniques\u0026#34;) df_train = pd.read_csv(data_dir / \u0026#34;train.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) df_test = pd.read_csv(data_dir / \u0026#34;test.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) print(f\u0026#34;Train shape: {df_train.shape}\u0026#34;) print(f\u0026#34;Test shape: {df_test.shape}\u0026#34;) Train shape: (1460, 80) Test shape: (1459, 79) Understand the Target Variable # Distribution of target sns.histplot(df_train[\u0026#34;SalePrice\u0026#34;], kde=True) plt.title(\u0026#34;SalePrice Distribution\u0026#34;) plt.xlabel(\u0026#34;SalePrice\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.savefig(\u0026#34;saleprice_distribution.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() # Log-transform target to check skew sns.histplot(np.log1p(df_train[\u0026#34;SalePrice\u0026#34;]), kde=True) plt.title(\u0026#34;Log-Transformed SalePrice\u0026#34;) plt.xlabel(\u0026#34;Log(SalePrice + 1)\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.savefig(\u0026#34;log_saleprice_distribution.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() # Summary stats df_train[\u0026#34;SalePrice\u0026#34;].describe() count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000 Name: SalePrice, dtype: float64 Overview of the Dataset df = pd.concat([df_train, df_test], axis=0) # df.info() # df.describe() # Missing value heatmap import missingno as msno msno.matrix(df) \u0026lt;Axes: \u0026gt; Data Cleaning def clean_data(df): # Clean to_category = [\u0026#39;MSSubClass\u0026#39;, \u0026#39;MoSold\u0026#39;, \u0026#39;YrSold\u0026#39;, \u0026#39;GarageYrBlt\u0026#39;, \u0026#39;YearBuilt\u0026#39;, \u0026#39;YearRemodAdd\u0026#39;] for col in to_category: df[col] = df[col].astype(str) df[\u0026#39;Functional\u0026#39;] = df[\u0026#39;Functional\u0026#39;].fillna(\u0026#39;Typ\u0026#39;) df[\u0026#34;Electrical\u0026#34;] = df[\u0026#34;Electrical\u0026#34;].fillna(\u0026#39;SBrkr\u0026#39;) df[\u0026#34;KitchenQual\u0026#34;] = df[\u0026#34;KitchenQual\u0026#34;].fillna(\u0026#39;TA\u0026#39;) df[\u0026#34;Exterior1st\u0026#34;] = df[\u0026#34;Exterior1st\u0026#34;].fillna(df[\u0026#34;Exterior1st\u0026#34;].mode()[0]) df[\u0026#34;Exterior2nd\u0026#34;] = df[\u0026#34;Exterior2nd\u0026#34;].fillna(df[\u0026#34;Exterior2nd\u0026#34;].mode()[0]) df[\u0026#34;SaleType\u0026#34;] = df[\u0026#34;SaleType\u0026#34;].fillna(df[\u0026#34;SaleType\u0026#34;].mode()[0]) # Impute # Fill missing values in object columns with \u0026#34;None\u0026#34; objects = [] for i in df.columns: if df[i].dtype == object: objects.append(i) df.update(df[objects].fillna(\u0026#39;None\u0026#39;)) # Fill missing values in numeric columns with 0 numerics = [] for i in df.columns: if df[i].dtype != object: numerics.append(i) df.update(df[numerics].fillna(0)) return df def load_data(): data_dir = Path(\u0026#34;./house-prices-advanced-regression-techniques\u0026#34;) df_train = pd.read_csv(data_dir / \u0026#34;train.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) df_test = pd.read_csv(data_dir / \u0026#34;test.csv\u0026#34;, index_col=\u0026#34;Id\u0026#34;) # Clean data df = pd.concat([df_train, df_test], axis=0) df = clean_data(df) # Split back into train and test df_train = df.loc[df_train.index, :] df_test = df.loc[df_test.index, :].drop(columns=[\u0026#34;SalePrice\u0026#34;]) return df_train, df_test df_train, df_test = load_data() # Check the cleaned data # train_missing = df_train.isnull().sum() # print(train_missing[train_missing \u0026gt; 0]) # test_missing = df_test.isnull().sum() # print(test_missing[test_missing \u0026gt; 0]) # df_train.info() Correlation with Target (Numerical Features) # Compute correlation matrix corr_matrix = df_train.corr(numeric_only=True) # Get top 15 features correlated with SalePrice top_corr = corr_matrix[\u0026#34;SalePrice\u0026#34;].abs().sort_values(ascending=False).head(15) # Visualize sns.heatmap(df_train[top_corr.index].corr(), annot=True, cmap=\u0026#34;coolwarm\u0026#34;, fmt=\u0026#34;.2f\u0026#34;) plt.title(\u0026#34;Top Correlated Features with SalePrice\u0026#34;) plt.savefig(\u0026#34;top_correlated_features.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() Categorical Features Preview categoricals = df_train.select_dtypes(include=\u0026#34;object\u0026#34;).columns print(f\u0026#34;Categorical features: {len(categoricals)}\u0026#34;) # print(categoricals.tolist()) # Example: Visualize average SalePrice by a few important categorical features important_cats = [\u0026#34;Neighborhood\u0026#34;, \u0026#34;ExterQual\u0026#34;, \u0026#34;GarageFinish\u0026#34;, \u0026#34;KitchenQual\u0026#34;] for col in important_cats: sns.boxplot(data=df_train, x=col, y=\u0026#34;SalePrice\u0026#34;) plt.title(f\u0026#34;SalePrice by {col}\u0026#34;) plt.xticks(rotation=45) plt.savefig(f\u0026#34;saleprice_by_{col}.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) plt.show() Categorical features: 49 Time-Related Patterns sns.boxplot(x=\u0026#34;YrSold\u0026#34;, y=\u0026#34;SalePrice\u0026#34;, data=df_train) plt.title(\u0026#34;SalePrice by Year Sold\u0026#34;) plt.show() sns.scatterplot(x=\u0026#34;YearBuilt\u0026#34;, y=\u0026#34;SalePrice\u0026#34;, data=df_train) plt.title(\u0026#34;SalePrice vs Year Built\u0026#34;) plt.show() It does not look like there is any time related feature.\nFeature Engineering X = df_train.copy() y = X.pop(\u0026#34;SalePrice\u0026#34;) # X.info() Separate Categorical and Numerical Features # cat_cols = X.select_dtypes(include=[\u0026#39;object\u0026#39;]).columns.tolist() # num_cols = X.select_dtypes(exclude=[\u0026#39;object\u0026#39;]).columns.tolist() # print(f\u0026#34;Categorical columns: {len(cat_cols)}\u0026#34;) # print(f\u0026#34;Numerical columns: {len(num_cols)}\u0026#34;) Encoding from sklearn.base import BaseEstimator, TransformerMixin from category_encoders import TargetEncoder class Encoder(BaseEstimator, TransformerMixin): def __init__(self, ordered_levels): self.ordered_levels = ordered_levels self.low_cardinality = [] self.high_cardinality = [] self.te = None def fit(self, X, y): X = X.copy() self.ordered_levels = {k: [\u0026#34;None\u0026#34;] + v for k, v in self.ordered_levels.items()} for col, order in self.ordered_levels.items(): if col in X.columns: X[col] = pd.Categorical(X[col], categories=order, ordered=True).codes ordinal_cols = list(self.ordered_levels.keys()) nominal_cols = [col for col in X.select_dtypes(include=\u0026#39;object\u0026#39;).columns if col not in ordinal_cols] self.low_cardinality = [col for col in nominal_cols if X[col].nunique() \u0026lt;= 10] self.high_cardinality = [col for col in nominal_cols if X[col].nunique() \u0026gt; 10] for col in self.low_cardinality: X[col] = X[col].astype(\u0026#34;category\u0026#34;).cat.codes self.te = TargetEncoder() self.te.fit(X[self.high_cardinality], y) return self def transform(self, X): X = X.copy() for col, order in self.ordered_levels.items(): if col in X.columns: X[col] = pd.Categorical(X[col], categories=order, ordered=True).codes for col in self.low_cardinality: if col in X.columns: X[col] = X[col].astype(\u0026#34;category\u0026#34;).cat.codes if self.te: X[self.high_cardinality] = self.te.transform(X[self.high_cardinality]) return X five_levels = [\u0026#34;Po\u0026#34;, \u0026#34;Fa\u0026#34;, \u0026#34;TA\u0026#34;, \u0026#34;Gd\u0026#34;, \u0026#34;Ex\u0026#34;] ten_levels = list(range(1, 11)) # 1 - 10 is the correct range! ordered_levels = { \u0026#34;OverallQual\u0026#34;: ten_levels, \u0026#34;OverallCond\u0026#34;: ten_levels, \u0026#34;ExterQual\u0026#34;: five_levels, \u0026#34;ExterCond\u0026#34;: five_levels, \u0026#34;BsmtQual\u0026#34;: five_levels, \u0026#34;BsmtCond\u0026#34;: five_levels, \u0026#34;HeatingQC\u0026#34;: five_levels, \u0026#34;KitchenQual\u0026#34;: five_levels, \u0026#34;FireplaceQu\u0026#34;: five_levels, \u0026#34;GarageQual\u0026#34;: five_levels, \u0026#34;GarageCond\u0026#34;: five_levels, \u0026#34;PoolQC\u0026#34;: five_levels, \u0026#34;LotShape\u0026#34;: [\u0026#34;Reg\u0026#34;, \u0026#34;IR1\u0026#34;, \u0026#34;IR2\u0026#34;, \u0026#34;IR3\u0026#34;], \u0026#34;LandSlope\u0026#34;: [\u0026#34;Sev\u0026#34;, \u0026#34;Mod\u0026#34;, \u0026#34;Gtl\u0026#34;], \u0026#34;BsmtExposure\u0026#34;: [\u0026#34;No\u0026#34;, \u0026#34;Mn\u0026#34;, \u0026#34;Av\u0026#34;, \u0026#34;Gd\u0026#34;], \u0026#34;BsmtFinType1\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;LwQ\u0026#34;, \u0026#34;Rec\u0026#34;, \u0026#34;BLQ\u0026#34;, \u0026#34;ALQ\u0026#34;, \u0026#34;GLQ\u0026#34;], \u0026#34;BsmtFinType2\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;LwQ\u0026#34;, \u0026#34;Rec\u0026#34;, \u0026#34;BLQ\u0026#34;, \u0026#34;ALQ\u0026#34;, \u0026#34;GLQ\u0026#34;], \u0026#34;Functional\u0026#34;: [\u0026#34;Sal\u0026#34;, \u0026#34;Sev\u0026#34;, \u0026#34;Maj1\u0026#34;, \u0026#34;Maj2\u0026#34;, \u0026#34;Mod\u0026#34;, \u0026#34;Min2\u0026#34;, \u0026#34;Min1\u0026#34;, \u0026#34;Typ\u0026#34;], \u0026#34;GarageFinish\u0026#34;: [\u0026#34;Unf\u0026#34;, \u0026#34;RFn\u0026#34;, \u0026#34;Fin\u0026#34;], \u0026#34;PavedDrive\u0026#34;: [\u0026#34;N\u0026#34;, \u0026#34;P\u0026#34;, \u0026#34;Y\u0026#34;], \u0026#34;Utilities\u0026#34;: [\u0026#34;NoSeWa\u0026#34;, \u0026#34;NoSewr\u0026#34;, \u0026#34;AllPub\u0026#34;], \u0026#34;CentralAir\u0026#34;: [\u0026#34;N\u0026#34;, \u0026#34;Y\u0026#34;], \u0026#34;Electrical\u0026#34;: [\u0026#34;Mix\u0026#34;, \u0026#34;FuseP\u0026#34;, \u0026#34;FuseF\u0026#34;, \u0026#34;FuseA\u0026#34;, \u0026#34;SBrkr\u0026#34;], \u0026#34;Fence\u0026#34;: [\u0026#34;MnWw\u0026#34;, \u0026#34;GdWo\u0026#34;, \u0026#34;MnPrv\u0026#34;, \u0026#34;GdPrv\u0026#34;], } from sklearn.pipeline import Pipeline from xgboost import XGBRegressor basepipe = Pipeline([ (\u0026#34;encode\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;xgb\u0026#34;, XGBRegressor(random_state=42)) ]) from sklearn.metrics import make_scorer, mean_squared_log_error from sklearn.model_selection import cross_val_score import numpy as np # Custom RMSLE scorer (greater_is_better=False because lower is better) rmsle_scorer = make_scorer( lambda y_true, y_pred: np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred))), greater_is_better=False ) y_log = np.log(y) # We train on the log transformed target # Then use it with cross_val_score score = cross_val_score(basepipe, X, y_log, cv=5, scoring=rmsle_scorer) print(f\u0026#34;RMSLE scores: {-score}\u0026#34;) # Negate to get actual RMSLE values print(f\u0026#34;Mean RMSLE: {-score.mean():.5f}\u0026#34;) RMSLE scores: [0.1397286 0.15149179 0.14633292 0.1268186 0.14509794] Mean RMSLE: 0.14189 Transform Skewed Numerical Features This transform is not used in the end.\nclass SkewedFeatureTransformer(BaseEstimator, TransformerMixin): def __init__(self, threshold=0.75): self.threshold = threshold self.skewed_cols = [] def fit(self, X, y=None): X = pd.DataFrame(X) skewness = X.skew().abs() self.skewed_cols = skewness[skewness \u0026gt; self.threshold].index.tolist() return self def transform(self, X): X = pd.DataFrame(X).copy() for col in self.skewed_cols: X[col] = np.log1p(X[col]) return X skew_transformer = SkewedFeatureTransformer() Add New Features from sklearn.preprocessing import FunctionTransformer from sklearn.pipeline import Pipeline def add_custom_features(df): df = df.copy() df[\u0026#39;TotalSF\u0026#39;] = df[\u0026#39;TotalBsmtSF\u0026#39;] + df[\u0026#39;1stFlrSF\u0026#39;] + df[\u0026#39;2ndFlrSF\u0026#39;] df[\u0026#39;Total_sqr_footage\u0026#39;] = df[\u0026#39;BsmtFinSF1\u0026#39;] + df[\u0026#39;BsmtFinSF2\u0026#39;] + df[\u0026#39;1stFlrSF\u0026#39;] + df[\u0026#39;2ndFlrSF\u0026#39;] df[\u0026#39;Total_Bathrooms\u0026#39;] = df[\u0026#39;FullBath\u0026#39;] + (0.5 * df[\u0026#39;HalfBath\u0026#39;]) + df[\u0026#39;BsmtFullBath\u0026#39;] + (0.5 * df[\u0026#39;BsmtHalfBath\u0026#39;]) df[\u0026#39;Total_porch_sf\u0026#39;] = df[\u0026#39;OpenPorchSF\u0026#39;] + df[\u0026#39;3SsnPorch\u0026#39;] + df[\u0026#39;EnclosedPorch\u0026#39;] + df[\u0026#39;ScreenPorch\u0026#39;] + df[\u0026#39;WoodDeckSF\u0026#39;] df[\u0026#39;haspool\u0026#39;] = (df[\u0026#39;PoolArea\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;has2ndfloor\u0026#39;] = (df[\u0026#39;2ndFlrSF\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasgarage\u0026#39;] = (df[\u0026#39;GarageArea\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasbsmt\u0026#39;] = (df[\u0026#39;TotalBsmtSF\u0026#39;] \u0026gt; 0).astype(int) df[\u0026#39;hasfireplace\u0026#39;] = (df[\u0026#39;Fireplaces\u0026#39;] \u0026gt; 0).astype(int) return df custom_feature_step = FunctionTransformer(add_custom_features) Mutual Information From my experiment, this seems to be the most useful tool.\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression # Select top k features based on MI mi_selector = SelectKBest(score_func=mutual_info_regression, k=50) # or \u0026#39;k=\u0026#34;all\u0026#34;\u0026#39; to get scores PCA This is not used in the end.\nfrom sklearn.decomposition import PCA pca = PCA(n_components=50) # You can tune this Model-Based Feature Selection Again, this is not used in the end. Mutual information already turns out to be very effective.\nfrom sklearn.feature_selection import SelectFromModel from xgboost import XGBRegressor # Use a fitted model to select features with importance above threshold model_selector = SelectFromModel(XGBRegressor(n_estimators=100), threshold=\u0026#34;median\u0026#34;) Model Evaluation from sklearn.pipeline import make_pipeline from sklearn.feature_selection import SelectKBest, mutual_info_regression pipeline = Pipeline([ (\u0026#34;custom_features\u0026#34;, custom_feature_step), (\u0026#34;encoder\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;feature_select\u0026#34;, SelectKBest(score_func=mutual_info_regression, k=60)), (\u0026#34;model\u0026#34;, XGBRegressor()) ]) # Then use it with cross_val_score score = cross_val_score(pipeline, X, y_log, cv=5, scoring=rmsle_scorer) print(f\u0026#34;RMSLE scores: {-score}\u0026#34;) # Negate to get actual RMSLE values print(f\u0026#34;Mean RMSLE: {-score.mean():.5f}\u0026#34;) RMSLE scores: [0.13717768 0.15245099 0.1511411 0.12531991 0.13995213] Mean RMSLE: 0.14121 We can see a slight improvement compared to the baseline model above.\nHyperparameter Tuning and Final Predictions # import optuna # from sklearn.model_selection import cross_val_score # def objective(trial): # xgb_params = dict( # max_depth=trial.suggest_int(\u0026#34;max_depth\u0026#34;, 2, 10), # learning_rate=trial.suggest_float(\u0026#34;learning_rate\u0026#34;, 1e-4, 1e-1, log=True), # n_estimators=trial.suggest_int(\u0026#34;n_estimators\u0026#34;, 1000, 8000), # min_child_weight=trial.suggest_int(\u0026#34;min_child_weight\u0026#34;, 1, 10), # colsample_bytree=trial.suggest_float(\u0026#34;colsample_bytree\u0026#34;, 0.2, 1.0), # subsample=trial.suggest_float(\u0026#34;subsample\u0026#34;, 0.2, 1.0), # reg_alpha=trial.suggest_float(\u0026#34;reg_alpha\u0026#34;, 1e-4, 1e2, log=True), # reg_lambda=trial.suggest_float(\u0026#34;reg_lambda\u0026#34;, 1e-4, 1e2, log=True), # ) # pipeline.set_params(**{f\u0026#34;model__{key}\u0026#34;: val for key, val in xgb_params.items()}) # score = cross_val_score(pipeline, X, y_log, cv=5, scoring=rmsle_scorer) # return -score.mean() # Minimize RMSLE # # Run Optuna optimization # study = optuna.create_study(direction=\u0026#34;minimize\u0026#34;) # study.optimize(objective, n_trials=10) # This is the set of parameters for my submission xgb_params = {\u0026#39;max_depth\u0026#39;: 7, \u0026#39;learning_rate\u0026#39;: 0.004565565417769295, \u0026#39;n_estimators\u0026#39;: 4701, \u0026#39;min_child_weight\u0026#39;: 9, \u0026#39;colsample_bytree\u0026#39;: 0.5911157102802619, \u0026#39;subsample\u0026#39;: 0.265969852467484, \u0026#39;reg_alpha\u0026#39;: 0.030977607695995966, \u0026#39;reg_lambda\u0026#39;: 0.168357167207514} from sklearn.metrics import mean_squared_log_error # Train the pipeline on the entire training set X = df_train.copy() y = X.pop(\u0026#39;SalePrice\u0026#39;) y_log = np.log(y) # Initialize the model pipeline = Pipeline([ (\u0026#34;custom_features\u0026#34;, custom_feature_step), (\u0026#34;encoder\u0026#34;, Encoder(ordered_levels=ordered_levels)), (\u0026#34;feature_select\u0026#34;, SelectKBest(score_func=mutual_info_regression, k=60)), (\u0026#34;model\u0026#34;, XGBRegressor()) ]) # Properly prefix parameter names with \u0026#34;model__\u0026#34; # best_params_prefixed = {f\u0026#34;model__{key}\u0026#34;: val for key, val in study.best_params.items()} best_params_prefixed = {f\u0026#34;model__{key}\u0026#34;: val for key, val in xgb_params.items()} # Set the best parameters to the pipeline pipeline.set_params(**best_params_prefixed) # Train the model pipeline.fit(X, y_log) # Predict on the test set test_predictions = pipeline.predict(df_test) predictions = np.exp(test_predictions) # Save the final result output = pd.DataFrame({\u0026#39;Id\u0026#39;: df_test.index, \u0026#39;SalePrice\u0026#39;: predictions}) output.to_csv(\u0026#39;my_submission.csv\u0026#39;, index=False) ","permalink":"http://localhost:1313/projects/housing-price-prediction-with-xgboost/","summary":"\u003cp\u003eThis is my \u003ca href=\"https://www.kaggle.com/code/houmingyi/housing-price-prediction-with-xgboost\"\u003eKaggle notebook\u003c/a\u003e for the \u003ca href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\"\u003eHouse Prices - Advanced Regression Techniques\u003c/a\u003e competition on Kaggle. The goal of this competition is to predict the sale price of homes in Ames, Iowa, based on various features of the homes.\u003c/p\u003e\n\u003cp\u003eI will be using various techniques learned in the course to predict the sale price of homes. The techniques include: preprocessing the data, feature engineering, and using different machine learning models to make predictions. I will also be using cross-validation to evaluate the performance of the models and to avoid overfitting.\u003c/p\u003e","title":"Housing Price Prediction with XGBoost"},{"content":"This project implements a Monte Carlo simulation for the Fokker-Planck equation using Metal, a framework for GPU programming on Apple devices. The simulation is designed to model the diffusion of particles in a potential field, which is common in statistical physics and machine learning.\nThe codes are written in Julia and utilize the Metal.jl package to leverage the GPU for parallel computation. You can find the repository for this project here.\nKramers-Fokker-Planck Equation The Kramers-Fokker-Planck equation is the generator of stochastic process modeling a particle\u0026rsquo;s movement in a potential field. The stationary solution of the Fokker-Planck equation is the equilibrium distribution of the particle\u0026rsquo;s position.\nOne example considered in this project is the following SDE: $$ \\begin{cases} \\rm{d}X_t = V_t \\rm{d}t,\\newline \\rm{d}V_t = \\rm{d}B_t, \\end{cases} $$ where $B_t$ is the standard Brownian motion. Denote by $z = (x, v)\\in \\mathbb{R}\\times\\mathbb{R}$. Its generator is given by: $$\\cal{L} = \\frac{1}{2}\\frac{\\partial^2}{\\partial v^2} + v\\frac{\\partial}{\\partial x}.$$\nWe consider the process in the box $Q = (-1, 1)\\times(-1, 1)$, and simulate the process starting at a point $z_0 = (x_0, v_0)\\in Q$ until it hits the boundary of the box.\nThe Monte Carlo simulation involves:\nA Metal kernel function that updates the position of the particle according to the discretized SDE, namely $$ x_{n+1} = x_n + v_n \\Delta t, \\quad v_{n+1} = v_n + \\sqrt{\\Delta t} \\xi_n,$$ where $\\xi_n \\sim \\mathcal{N}(0, 1)$ is the standard normal random variable. Since the Metal kernel does not support random number generation, we also write a simple GPU friendly random number generator based on xorshift32 and Box-Muller method. The Kernel function does not support branching, the iteration will be fixed steps, and we use mask to stop the iteration when the particle hits the boundary. We simulate the process for a large number of particles and plot the harmonic measure on the boundary of the annulus. Features of the Project GPU friendly random number generator The random number generator is based on the xorshift32 algorithm, which is a simple and efficient pseudo-random number generator. The Box-Muller transform is used to generate normally distributed random numbers from uniformly distributed random numbers.\nGiven a seed ranged from 0 to 2^32-1, the xorshift32 algorithm generates a new seed by performing bitwise operations on the current seed.\nfunction xorshift32(seed::UInt32)::UInt32 seed ⊻= (seed \u0026lt;\u0026lt; 13) seed ⊻= (seed \u0026gt;\u0026gt; 17) seed ⊻= (seed \u0026lt;\u0026lt; 5) return seed end Then we transform this seed to a float number in the range of (0, 1) using the following function:\nfunction xorshift32_float(seed::UInt32)::Float32 value = Float32(xorshift32(seed)) * 2.3283064f-10 # Scale to [0,1) return max(value, 1.0f-16) # Ensure it\u0026#39;s in (0,1) end Finally, we use the Box-Muller transform to generate normally distributed random numbers:\nfunction box_muller(u1::Float32, u2::Float32) r = sqrt(-2.0f0 * log(u1)) theta = 2.0f0 * Float32(pi) * u2 return r * cos(theta) end Masks to avoid branching The Metal.jl kernel does not support branching, so we need to avoid using if statements in the kernel code. Instead, we use masks to control the flow of the simulation. The core update function for the problem in the cube $Q$ is as follows:\nfor step in 1:num_steps # Boolean masks for exit conditions mask_x = (x \u0026lt; -1.0f0 || x \u0026gt; 1.0f0) ? 1 : 0 mask_v = (v \u0026lt; -1.0f0 || v \u0026gt; 1.0f0) ? 1 : 0 mask_exit = mask_x | mask_v # Combine masks (exit if either condition is true) continue_mask = 1 - mask_exit # 1 = active, 0 = exited # Generate two uniform distributed random numbers seed1 = xorshift32(seed1) seed2 = xorshift32(seed2) random_number1 = xorshift32_float(seed1) random_number2 = xorshift32_float(seed2) # Generate a normal distributed noise noise = box_muller(random_number1, random_number2) # Perturb the seeds to avoid deterministic patterns seed1 += UInt32(i) seed2 += UInt32(i) # Update position and velocity and store previous state if not exit x_prev, v_prev = continue_mask * x + mask_exit * x_prev, continue_mask * v + mask_exit * v_prev x += continue_mask * (v * time_step) v += continue_mask * (sqrt(time_step) * noise) end The mask_exit variable is used to check if the particle has exited the box. If it has, we set the continue_mask to 0, which effectively stops the simulation for that particle. The x_prev and v_prev variables are used to store the previous state of the particle before it exited.\nExample plots Consider the following Dirichlet boundary condition: Our codes can simulate the solution efficiently. The following plot shows the full solution and also a zoomed-in view of the solution near the singular boundary: In addition, we can plot the exit points distribution on the boundary for a starting point. The following is an example in the annulus: ","permalink":"http://localhost:1313/projects/monte-carlo-kfp-metal/","summary":"\u003cp\u003eThis project implements a Monte Carlo simulation for the Fokker-Planck equation using \u003ccode\u003eMetal\u003c/code\u003e, a framework for GPU programming on Apple devices. The simulation is designed to model the diffusion of particles in a potential field, which is common in statistical physics and machine learning.\u003c/p\u003e\n\u003cp\u003eThe codes are written in \u003ccode\u003eJulia\u003c/code\u003e and utilize the \u003ccode\u003eMetal.jl\u003c/code\u003e package to leverage the GPU for parallel computation. You can find the repository for this project \u003ca href=\"https://github.com/mingyi-ai/Monte_Carlo_KFP\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"Monte Carlo Simulation for Fokker-Planck Equations using Metal"}]